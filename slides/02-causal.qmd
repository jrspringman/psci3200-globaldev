---
title: "Correlation and Causation"
subtitle: "What are they good for?"
author: "Jeremy Springman"
institute: "University of Pennsylvania"

format:
  revealjs:
    toc: false
    theme: [custom_iea.scss]
    width: 1050
    margin: 0.1
    logo: DevLab_Logo_29Mar2023.png
    footer: "jrspringman.github.io/psci3200-globaldev/"
    embed-resources: true
    template-partials:
      - title-slide.html
    gfm:
    mermaid-format: png
    highlight-style: atom-one-dark
    code-overflow: wrap
editor: source
---


## Logistics

- Did everyone find the readings and slides for today?
- For next week:
  + I'll scan the chapter and upload tonight
  + Remember you have a quasi-assignment


# Correlation

## Which of the following statements describe a correlation?

1. Most professional data analysis took a statistics course in college.

::: {.fragment .highlight-red}
2. The longer a person runs the more calories they burn.
:::
3. People who live to be 100 years old typically take vitamins.

::: {.fragment .highlight-red}
4. Older people vote more than younger people.
:::


## Correlations: Quantitative Comparison

\

- Avoid 'selecting on the dependent variable'
  + Applies to qualitative work as well
- Lots of bad analysis and evaluation *implies* comparisons
  + Ex. 70% of participants felt they had improved
  + Ex. 60% of Americans live paycheck-to-paycheck

## Correlations: Necessary Components

\

**What do we need to calculate correlations?**

- Measures of central tendency
  + Mean 
- Measures of spread
  + Variance
  + Standard deviation


## Central Tendency: Mean


$$
\mu_X = \frac{1}{n} \sum_{i}^{n} X_i
$$

```{r}
#| echo: true

my_vector = rnorm(10, mean = 10, sd = 5)
# Step 1: Sum the values
sum_values <- sum(my_vector)
# Step 2: Count the number of elements
count_elements <- length(my_vector)
# Step 3: Calculate the mean
mean_value <- sum_values / count_elements

print(mean_value)
mean(my_vector)
```


## Spread: Variance

$$
\sigma^2_X = \frac{1}{N} \sum_{i}^{N} (X_i - \mu_X)^2
$$

::: {.incremental}
- What does the square in $\sigma^2$ accomplish?
- What are the implications for interpretation?
  + Units
  + Distribution
- Even with these basic measures, we're already thinking about the distribution!

:::

## Spread: Variance

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Create a vector"

## Create vector, sort by size, and store var
set.seed(123)
dat = rnorm(10, mean = 10, sd = 5)
dat = sort(dat)
o_var = var(dat)
print(dat)

```

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Add a constant to a big number"

## Create new dataframe for big addition and store vector length
b_dat = dat
ind = length(b_dat)

## Add four to the largest number in the vector and calculate size of var increase
b_dat[ind] = b_dat[ind] + 4
b_var = var(b_dat)
val = b_var - o_var
cat("Variance increases by", val )
```



```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Add a constant to a smaller number"

## Create new dataframe for small addition
s_dat = dat

## Add four to the smallest number in the vector and calculate size of var increase
s_dat[ind-2] = s_dat[ind-2] + 4
s_var = var(s_dat)
val = s_var - o_var
cat("Variance increases by", val )

```

## Spread: Standard Deviation

$$
\sigma_X = \sqrt{\frac{1}{N} \sum_{i}^{N} (X_i - \mu_X)^2}
$$

::: {.incremental}
- What does the $\sqrt{}$ accomplish?
- What are the implications for interpretation?
  + Expressed in the same units as the observations
  + How far we expect each observation to be from the mean, on average
- This means we can report effect sizes as SDs
:::


## Measures of Correlation

::: {.incremental}
- <span style="color:red;">Covariance</span> $\text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})$
  + Product of the deviations
  + Range: unbounded
- <span style="color:red;">Correlation coefficient</span> $\text{Cor}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$
  + Covariance normalized by product of SDs
  + Range: -1 to 1
- <span style="color:red;">Slope</span> $\beta_X = \frac{\text{Cov}(X, Y)}{\sigma^2_X}$
  + Covariance normalized by variance
  + Expected change in $Y$ with 1-unit change in $X$
:::


## Measures of Correlation


::: {.incremental}
- What does the correlation coefficient tell you that slope doesn't?
  + Consistency of the relationship on bounded (0-1) scale
- What does slope tell you that the correlation coefficient doesn't?
  + Substantive importance (magnitude)
:::


## Correlation

::: columns
::: {.column width="60%"}

\
**What can with do with them?**

- <span style="color:red;">Description</span> = comparison 
:::

::: {.column width="35%"}

::: {.fragment}

::: {.callout-note appearance="simple" icon=false} 
-   <span style="color:red;">*sample matters alot*</span>
-   <span style="color:blue;">*sample matters less*</span>
:::

:::

:::
:::
- <span style="color:red;">Forecasting</span> = sample population -> out-of-sample
- <span style="color:blue;">Causal inference</span> = correlation + research design

::: {.fragment}

**Simple, but powerful**

- Non-linearities, interactions, machine learning

:::


# Causation

## Schools of Thought

- Potential outcomes and counterfactuals (Econ)
- DAGs and do-calculus (CS)
- Manipulability (Philosophy)


“We think of a cause as something that makes a difference, and the difference it makes must be a difference from what would have happened without it.” (Lewis, 1973)

## Causality: Why bother?

- Understanding cause and effect is how we change things in the real world

## Causality: What makes it hard?

Fundamental Problem of Causal Inference

We only observe any given unit in one treatment status at any one time so we can never directly observe the causal effect of a treatment on a unit. 
We don’t know the counterfactual. 
The actual outcome for unit 𝑖 which was assigned to treatment 𝑇=1  is  𝑌_𝑖 (1) and we cannot observe the value of 𝑌_𝑖 (0).
Solution: 
We must infer counterfactual outcomes
We can estimate average treatment effects


## Potential Outcomes and Counterfactuals

$$
Y_i = 
\begin{cases} 
Y_i(1) & \text{if } D_i = 1 \text{ (treatment group)} \\
Y_i(0) & \text{if } D_i = 0 \text{ (control group)}
\end{cases}
$$

Treatment Effect for individual $i$

- $TE_i = Y_i(1) - Y_i(0)$

Average Treatment Effect (ATE)

- $ATE = \frac{1}{N} \sum_{i=1}^{N} TE_i$



## DAGs and Confounding

```{r}
library(ggdag)

dagify(
  x ~ a,
  y ~ x,
  y ~ a

) %>%
  ggdag()

```

## Flawed Evaluations


<!-- **The correlation between two variables is “the extent to which they tend to occur together” (p. 14). -->
<!-- A relation between two variables (or “features of the world”). -->
<!-- Two variables/features are: -->
<!-- Positively correlated if the two tend to occur together -->
<!-- Negatively correlated if one feature occurs when the other does not -->
<!-- Uncorrelated if the occurrence of one does not affect the occurrence of the other. -->

<!-- ## Politicians facing scandals tend to win re-election -->

<!-- - what data does this rely on? -->
<!-- - why is this not a correlation: we don't learn if these things tend to happen together -->
<!-- - what would make it a correlation  -->
<!-- - what data would we need to assess: rate of re-election for both scandal and non-scandal politicians -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- ::: -->
<!-- ::: -->
