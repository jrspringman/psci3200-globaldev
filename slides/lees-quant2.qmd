---
title: "Quantitative Analysis"
author: "Jeremy Springman"
institute: "University of Pennsylvania"

format:
  pptx:
    toc: false
    theme: [custom_iea.scss]
    width: 1050
    margin: 0.1
    logo: DevLab_Logo_29Mar2023.png
    footer: "jrspringman.github.io/psci3200-globaldev/"
    embed-resources: true
    template-partials:
    - title-slide.html
    gfm:
    mermaid-format: png
    code-line-numbers: false


highlight-style: github
---

# Statistical Review

## Resources

This activity draws on Chapter 2 in [Data Analysis for Social Science: A Friendly and Practical Introduction (DSS)](https://press.princeton.edu/books/paperback/9780691199436/data-analysis-for-social-science)

- [interactive visualization: mean and sd](https://ellaudet.iq.harvard.edu/mean_sd)
- [interactive visualization: correlation](https://ellaudet.iq.harvard.edu/correlation)


## Quantitative Analysis and Comparisons

- Comparisons are at the heart of quantitative analysis
- Lots of bad analysis *implies* comparisons, but doesn't actually make them
  + Ex. 10 things that extremely successful people do to be productive
  + Ex. 70% of participants reported an improvement
- Correlation is the most basic tool for making comparisons with data

:::{.notes}
Making comparisons is one of the most essential parts in research, underpinning everything from simple descriptive studies to machine learning and impact evaluations. It is difficult to learn without comparison. For example, an article that identifies 10 habits of successful people might seek to provide guidance on what behaviors you can adopt to become more successful. However, we should also ask how common these behaviors are among people who are not successful. Similarly, an evaluation might point out that 70% of organizations that participated in a capacity-building intervention reported an improvement in their capacity over the course of the intervention. But we should also ask whether these improvements were larger than improvements experienced by similar organizations that didn't participate. Correlation is the fundamental tool that allows us to answer these important questions.  
:::


## Correlations: Necessary Components

**What do we need to calculate correlations?**

- Measures of central tendency
  + Mean
  + Proportion
- Measures of spread
  + Variance
  + Standard deviation

:::{.notes}
In order to calculate correlations, we need to begin with quantities that we want to compare across groups. For example, we might want to calculate the mean, or the average, of a certain variable. We also need to measure how much variation, or spread, there is in these measures. Are the units in our sample very similar, or are their values spread our across a wide range?
:::


## Central Tendency: Mean

```{.default}
\mu_X = \frac{1}{n} \sum_{i}^{n} X_i
```

```{r}
#| echo: true

my_vector = rnorm(10, mean = 10, sd = 5)
# Step 1: Sum the values
sum_values <- sum(my_vector)
# Step 2: Count the number of elements
count_elements <- length(my_vector)
# Step 3: Calculate the mean
mean_value <- sum_values / count_elements

print(mean_value)
mean(my_vector)
```

:::{.notes}
We begin by reviewing some basic statistical concepts and illustrating those concepts using R code. This will build your familiarity with R while providing a quick review of basic statistical concepts. Everyone knows how to calculate the mean of a variable. In R, there are multiple ways to do it. Here, I created a random variable with 10 values and an average value of 10. You can calculate the mean by hand or using the function ‘mean()’. Both approaches should give you the same value.
:::


## Spread: Variance

```{.default}
\sigma^2_X = \frac{1}{N} \sum_{i}^{N} (X_i - \mu_X)^2
```

::: {.incremental}
- What does the square in `\sigma^2` accomplish?
- What are the implications for interpretation?
  + Units
  + Distribution
- Even with these basic measures, we're already thinking about the distribution!
:::

:::{.notes}
You’re also probably familiar with variance. Variance measures how spread out the values of a variable are around the mean. For the variance of X, we take the sum of the squared differences between each value of X and the mean value of X, and then divide by the number of observations. While squaring the differences is necessary to avoid positive and negative values cancelling out one another, it poses challenges for interpretation. Most importantly, the variance is not on the scale of the variable, so there is no intuitive way to interpret its value. The squaring of the differences also means that more weight is put on observations that are very far from the mean value. The next few slides illustrates this with simulated data.
:::

## Spread: Variance

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Create a vector"

## Create vector, sort by size, and store var
set.seed(123)
dat = rnorm(10, mean = 10, sd = 5)
dat = sort(dat)
o_var = var(dat)
print(dat)

```

:::{.notes}
First, we create a vector of simulated data with a known mean and standard deviation. In statistics and computer science, a vector is a one-dimensional array, which is a data structure that stores elements of the same type in an ordered sequence. 
:::

## Spread: Variance

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Add a constant to a big number"

## Create new dataframe for big addition and store vector length
b_dat = dat
ind = length(b_dat)

## Add four to the largest number in the vector and calculate size of var increase
b_dat[ind] = b_dat[ind] + 4
b_var = var(b_dat)
val = b_var - o_var
cat("Variance increases by", val )
```

:::{.notes}
Second, we add 4 to the largest value of X and observe the change in the standard deviation 
:::

## Spread: Variance

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Add a constant to a smaller number"

## Create new dataframe for small addition
s_dat = dat

## Add four to the smallest number in the vector and calculate size of var increase
s_dat[ind-2] = s_dat[ind-2] + 4
s_var = var(s_dat)
val = s_var - o_var
cat("Variance increases by", val )

```

:::{.notes}
Finally, we add 4 to the smallest value of X and observe that the change in the standard deviation is much smaller. Again, this is because squaring the differences places more weight on extreme values very far from the mean
:::

## Spread: Standard Deviation

```{.default}
\sigma_X = \sqrt{\frac{1}{N} \sum_{i}^{N} (X_i - \mu_X)^2}
```

::: {.incremental}
- What does the `\sqrt{}` accomplish?
- What are the implications for interpretation?
  + Expressed in the same units as the observations
  + How far we expect each observation to be from the mean, on average
- It is often desirable to report effect sizes as SDs
:::

:::{.notes}
Another measure of spread is the standard deviation. The standard deviation is just the square-root of the variance, and tells us the average absolute difference between values of X fall and the mean of X. By applying the square-root, this measure of spread is now measured in the same units as the variable. When calculating correlations between two variables, it is often desirable to translate the size of the relationship into standard deviations.
:::


## Spread: Standard Deviation

- [Check out this interactive visualization](https://ellaudet.iq.harvard.edu/mean_sd)
  + Observe how changes in the standard deviation affect the shape of the distribution 

:::{.notes}
In a new tab, open this interactive visualization of how changes in the standard deviation affect the shape of the distribution. Follow the instructions to see what increases in the spead of a variable looks like.
:::

## Measures of Correlation

::: {.incremental}
- <span style="color:red;">Covariance</span> `\text{Cov}(X, Y) = \frac{1}{n} \sum_{i}^{N} (X_i - \bar{X})(Y_i - \bar{Y})`
  + Product of the deviations
  + Range: unbounded
- <span style="color:red;">Correlation coefficient</span> `\text{Cor}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}`
  + Covariance normalized by product of SDs
  + Range: -1 to 1
:::

:::{.notes}
Now that we have reviewed the fundamental components of correlation, we can turn to the three primary ways of measuring correlation. Correlations rely on two variables, typically denoted as X and Y.  The covariance is the product of of the deviations between values of our two variables and the mean of the variables. However, this measure can tell us about the direction but not the strength of the correlation. Next, we turn to the correlation coefficient, which normalizes the covariance by the product of the variables' standard deviations, forcing them to take a bounded range that can tell us the strength and direction of correlation. 
:::

## Measures of Correlation

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Calculate covariance"

set.seed(1234)
# Generate two vectors of 10 random numbers each from a normal distribution
x <- rnorm(10, mean = 10, sd = 5)
y <- rnorm(10, mean = 10, sd = 5)
# Calculate the means of both vectors
mean_x <- mean(x)
mean_y <- mean(y)
# Calculate the covariance between the two vectors
covariance <- sum((x - mean_x) * (y - mean_y)) / (length(x) - 1)
cov(x,y)

```

## Measures of Correlation

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Calculate correlation"

# Calculate the standard deviations of both vectors
sd_x <- sd(x)
sd_y <- sd(y)

# Calculate the correlation coefficient between the two vectors
correlation <- covariance / (sd_x * sd_y)
cor(x,y)

```

## Measures of Correlation

::: {.incremental}
- <span style="color:red;">Slope</span> `\beta_X = \frac{\text{Cov}(X, Y)}{\sigma^2_X}`
  + Covariance normalized by variance
  + Expected change in `Y` with 1-unit change in `X`
:::

:::{.notes}
Finally, we can calculate the slope of a regression line, which normalized the covariance by the variance and tells us the expected change in Y when we observe a 1-unit increase in X. Here, “normalized” refers to the process of dividing the covariance of X and Y by the variance of X. Normalization refers to the process of scaling a variable so that it fits a specific scale. In this case, normalizing the covariance by the variance of X allows us to interpret the size of the slope in terms of the expect change in Y associated with a 1-unit increase in X.
:::

## Measures of Correlation

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Calculate slope"

# Calculate the variance of X
# Variance = sum((X_i - mean_X)^2) / (n - 1)
variance_x <- sum((x - mean_x)^2) / (length(x) - 1)

# Calculate the slope (beta) of Y on X
# Beta_X = Cov(X, Y) / Var(X)
beta_x <- covariance / variance_x
lm(y ~ x)

```

## Measures of Correlation

::: {.incremental}
- What does the correlation coefficient tell you that slope doesn't?
  + Consistency of the relationship on bounded scale (-1 to 1)
- What does slope tell you that the correlation coefficient doesn't?
  + Substantive importance (magnitude)
- Give an example of when you'd prefer each
  + Correlation: When comparing relationships on different scales
  + Slope: When thinking about ROI
:::

:::{.notes}
Here's a quick review of the concepts we just covered.
:::


## Correlation

**What can with do with them?**

- <span style="color:red;">Description</span>: quantitative comparisons 
- <span style="color:red;">Forecasting</span>: sample population `\rightarrow` out-of-sample 
- <span style="color:blue;">Causal inference</span>: correlation + research design

**Simple, but powerful**

- Non-linearities, interactions, machine learning

:::{.notes}
These simple but powerful statistical tools perform much of the work in contemporary quantitative social science research, ranging from simple description, to forecasting, to causal inference. These simple linear correlation can be combined with squared terms to estimate non-linear relationships, iterations to estimate how a relationship between two variables is affected by values of other variables, or powerful research designs to make inferences about causal relationships.
:::

## Linear Regression Model

```{.default}
Y_i = \alpha + \beta X_i + \epsilon_i
```

::: {.fragment}
::: {.incremental}
- What is `\alpha`?
- What is `\beta`?
- What is `X_i`?
- What is `\epsilon_i`?
:::
:::

:::{.notes}
Linear regression models are often referred to as the "workhorse of the social sciences". Whether it's estimating difference in political opinions across demographic groups or calculating the causal effect of a randomized intervention, most analyses rely on linear regression. The most frequently used model is called "Ordinary Least Squares". The most simple models consisten of an intercept (alpha), which estimates the average value of Y when the independent variable is at 0, the slope or coefficient (beta) for X, which captures the relationship between X and Y (the independent and dependent variables), and the error term (epsion) which captures the variance in Y that is not explained by the intercept and beta.
:::

## Linear Regression Model

**Estimating model parameters**

```{.default}
\hat{Y_i} = \hat{\alpha} + \hat{\beta} X_i
```

**Coefficient**

```{.default}
\hat{\beta} = \Delta{\hat{Y}} / \Delta{X}
```

:::{.notes}
Linear regression provides you with parameter estimates with which you can calculate the expected value of Y by adding together the terms on the right-hand-side of the equals sign in the model.
:::

## Minimizing the Residuals

**What are residuals**

```{.default}
\hat{\epsilon_i} = Y_i - \hat{Y_i}
```

**How do we minimize them?**

```{.default}
SSR = \sum_{i}^{N} \hat{\epsilon}_i^2
```

:::{.notes}
Ordinary least squares works by estimating parameter values the minimize the residuals of the model, which are the difference between each value of Y and the value predicted by the model based on the model parameters. As with our measures of variance, this is done using the sum of squared residuals, to avoid positive and negative differences from cancelling out.
:::

## Linear Regression Model

- [Check out this interactive visualization](https://ellaudet.iq.harvard.edu/least_squares)
  + See how the least squares method is used to calculate the slope of the line by drawing a line that minimizes squared errors 
- [Check out this interactive visualization](https://ellaudet.iq.harvard.edu/linear_model)
  + See how changes in the intercept and slope represents negative and positive relationships across different distributions of Y

## Multiple Regression

```{.default}
Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i
```

::: {.incremental}
- How does our interpretation of `\alpha` change?
- How does our interpretation of `\beta_1` change?
:::

:::{.notes}
Multiple regression works very similarly to simple linear regression, although interpretation becomes slightly more complicated. The intercept (alpha) is not the mean of Y when both independent variables (X1 and X2) are held at 0, while the value of B1 now captures the relationship between X1 and Y while X2 is held constant.
:::

## Multiple Regression

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Multiple Regression"

# Load required libraries
library(dplyr)
library(modelsummary)

# Set seed for reproducibility of random numbers
set.seed(123)

# Create a data frame with 100 observations
data <- data.frame(
  X1 = rnorm(100, mean = 10, sd = 2),    # Generate 100 random numbers for the first independent variable
  X2 = rnorm(100, mean = 5, sd = 2),     # Generate 100 random numbers for the second independent variable
  Y = rnorm(100, mean = 10, sd = 2)      # Generate 100 random numbers for the dependent variable
)

# Introduce correlation between X1 and Y
data$Y <- 0.5 * data$X1 + rnorm(100, mean = 10, sd = 1)

```


## Multiple Regression

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Multiple Regression"

# lm() function fits linear models
simple_model <- lm(Y ~ X1, data = data)

# Run multiple regression model (Y ~ X1 + X2)
multiple_model <- lm(Y ~ X1 + X2, data = data)

# modelsummary() function provides a summary of the regression models
modelsummary(
  list(
    "Simple Linear Regression" = simple_model,
    "Multiple Regression" = multiple_model
  ),
  estimate = "{estimate}{stars} ({std.error})",
  statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.',
  output = "dataframe"
)

```

## Quiz Questions

**Which of the following statements describe a correlation?**

1. Most professional data analysis took a statistics course in college.
2. The longer a person runs the more calories they burn.
3. People who live to be 100 years old typically take vitamins.
4. Older people vote more than younger people.


# Causation

## Schools of Thought

“We think of a cause as something that makes a difference, and the difference it makes must be a difference from what would have happened without it.” (Lewis, 1973)

- Potential outcomes and counterfactuals (Econ)
- DAGs and do-calculus (CS)
- Manipulability (Philosophy)

:::{.notes}
While simple correlations can answer many important questions about the world, on their own, they cannot tell us anything about whether the relationship between X and Y is causal. We define causal relationships as a dependency whereby intervening on or manipulating the value of X while holding all other variables constant will result in a change in the value of Y. Causality is a central topic in philosophy of science, and there are many schools of thought around it. In social science research, most researchers focus on the Potential Outcomes framework.
:::

## Causality: Why bother?

- Understanding cause and effect is how we change things in the real world
- Causal inference separates good evaluations from bad
  + Policy change
  + Development intervention
- Causal identification is not binary
  + It's harder for some policies and interventions than others
  + Variety of tools that can help us rule out different threats to inference

:::{.notes}
As we'll learn, it can be extremely difficult to estimate causal relationships in the social world. However, the ability to make positive change in the world requires an understanding cause and effect. However, most program evaluations cannot provide evidence of a causal impact and sometimes reliable causal inference is difficult or impossible.
:::


## Causality: What makes it hard?

**Fundamental Problem of Causal Inference**

```{.default}
Y_i = 
\begin{cases} 
Y_i(1) & \text{if } D_i = 1 \text{ (treatment group)} \\
Y_i(0) & \text{if } D_i = 0 \text{ (control group)}
\end{cases}
```

- We only observe any given unit in one treatment status at any one time so we can never directly observe the causal effect of a treatment on a unit. 

:::{.notes}
While estimating the correlation between two variables is straightforward, understanding whether this correlation captures a causal relationship is much more difficult. This is due to the “fundamental problem of causal inference”, which arises from our inability to observe counterfactuals. In causal inference, a counterfactual refers to the hypothetical outcome that would have occurred if a subject, who was exposed to a particular treatment or condition, had instead been exposed to a different treatment or no treatment at all. Counterfactuals are a fundamental concept in establishing causal relationships because they help to model what would have happened in an alternate scenario. We always think of causal relationships in terms of a counterfactual. Thinking about a causal relationship between a binary treatment variable D and an outcome Y, for any unit in our sample, we want to observe the value of Y in simultaneous worlds in which the unit does and does not receive the treatment D. For example, if we wanted to measure the effect of a malaria vaccine on the number of days a person misses work in a month, we want to observe simultaneous counterfactual worlds in which that person both does and does not receive the vaccine. Obviously, this is impossible. However, rigorous research designs allow us to estimate this relationship under certain conditions.
:::


## Potential Outcomes and Counterfactuals

**Treatment Effect for individual `i`**

- `TE_i = Y_i(1) - Y_i(0)`

**Average Treatment Effect (ATE)**

- `ATE = \frac{1}{N} \sum_{i=1}^{N} TE_i`

:::{.notes}
Observing these simultaneous counterfactual worlds for each unit in our sample would allow us to easily estimate the average treatment effect for units in our sample. However, this is impossible.
:::

## What's the solution?

**Estimating counterfactuals**

- Units in the control group serve as a stand-in for the counter-factual of the treatment group

```{.default}
\widehat{ATE} = \overline{Y}_{treatment\_group} - \overline{Y}_{control\_group}
```


:::{.notes}
While we can never directly observe a counterfactual, we can try to estimate it by using a credible research design. Causal inference methods attempt to do this by minimizing the probability of systematic differences between units that do and do not receive a treatment. 
:::


## What's complicated about this?

::: {.incremental}
- "Only valid when when the treatment and control group are comparable with respect to *all the variables that might affect the outcome* other than the treatment variable itself."
- "We must *find or create a situation in which the treated observations and the untreated observations are similar* with respect to all the variables that might affect the outcome"
- "By randomly assigning treatment, we ensure that treatment and control groups are, *on average*, identical to each other in all *observed and unobserved* pre-treatment characteristics"
:::

:::{.notes}
Units that have been exposed to the treatment can only serve as a valid counterfactual under very specific conditions. Under random assignment, the statistically most likely outcome is that treatment and control groups will not have any systematic differences that can confound our ability to estimate the treatment effect.
:::


## Can't we just observe and compare?


![](img/nyt-dag2.png){.fragment}

:::{.notes}
In the real world, differences in units’ exposure to X is often driven by important differences that also affect those units’ values of Y. This is called “confounding”. When confounding is present, our estimates of the causal effect of X on Y will be biased, potentially creating the illusion of a causal relationship that doesn’t actually exist. Consider this example, where the NYT reported on a study that found a correlation between opera attendance and longevity. Here, the journalists inferred that this correlation was causal, meaning that by changing whether or not someone goes to the opera, you can change how long they live. However, there are many factors that confound this relationship, including income, which is correlated with both opera attendance and lifespan. 
:::

## Can't we just observe and compare?

![](img/largenstweet.png)

:::{.notes}
Many very smart people do not understand the fundamental problem of causal inference, leading them to underestimate the difficulty of estimating causal relationships without randomization. This can lead to fundamentally wrong conclusions about the impact that certain policies or interventions will have. For example, this Tweet captures an individual arguing that large sample sizes (erroneously referred to as “numbers of n’s”) allow you to interpret correlations as causal. However, this is extremely incorrect. Larger samples are subject to the exact same sources of confounding as small samples. Thinking back to the previous slide, there are still systematic differences between people that go to the opera and those who don’t. No matter how large the sample size, it would be wrong to assume that giving out opera tickets will prolong people's lives. Understanding the challenges of causal inference and being able to communicate these challenges to others can be a huge advantage for researchers that want to help others learn about the world.
:::

## Can't we just observe and compare?

**Example: What is the effect of class size on test scores**

```{r edu-ex-simple, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}
library(ggplot2)
library(ggdag)

edu_ex_coords <- list(x = c(Small = 2, Score = 4, Year = 2, Wealth = 4, 
                              Location = 3, Attention = 3, Effort = 1, U1 = 3),
                        y = c(Small = 2, Score = 2, Year = 3, Wealth = 3, 
                              Location = 3, Attention = 1, Effort = 2, U1 = 4))

dagify(
  Score ~ Small,
  exposure = "Small",
  outcome = "Score",
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges(start_cap = ggraph::circle(3, "lines"),
                 end_cap = ggraph::circle(3, "lines"),
                 edge_width = 1.5, 
                 arrow_directed = grid::arrow(length = grid::unit(0.75, "lines"), type = "closed")) +
  geom_dag_point(aes(color = status), size = 30) +
  geom_dag_text(color = "black", size = 7) +
  scale_color_manual(values = c("#FF4136", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()
```

:::{.notes}
Consider the challenge of estimating the effect of smaller class sizes on test scores. We want to know whether decreasing class sizes will cause an increase in the average test scores of students. We might start by looking at observational data on class size and test scores across a given district or country.
:::

## Can't we just observe and compare?

```{r edu-ex-simplec, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ U1,
  Small ~ U1,
  exposure = "Small",
  outcome = "Score",
  latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "grey60", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
However, even if we find a negative correlation between larger class sizes and higher test scores, we cannot assume that this relationship is causal. If there are any unobserved factors that are correlated with both class size and score, this will confound our estimates.
:::

## Can't we just observe and compare?

```{r edu-ex-simpleb, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ Small  + U1,
  Small ~ U1,
  exposure = "Small",
  outcome = "Score",
  latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "grey60", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
This would create the appearance of a causal relationship, even if one does not actually exist.
:::


## Can't we just observe and compare?

```{r edu-ex-simpled, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ Small  + U1,
  Small ~ U1,
  exposure = "Small",
  outcome = "Score",
  #latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
We might decide to collect data on additional characteristics of students and try to "control" for these potential confounders by including them in a multiple regression model. For example, it might be that wealthier students tend to be in schools with smaller class sizes. If you can control for parental wealth, we can remove this potential bias in your estimates.
:::

## Can't we just observe and compare?

```{r edu-ex-full, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  #Score ~ Small ,
  U1 ~ Small,
  U1 ~ Score,
  exposure = "Small",
  outcome = "Score",
  #latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
However, adding the wrong control variables to your model can also create bias. When your dependent and independent variables have an effect on another variable in your model, this creates collider bias, which can cause the illusion of a causal relationship between X and Y by adding a control. For example, smaller class sizes might cause teachers to pay more attention to students. Higher test scores also might cause teachers to pay more attention to students. Controlling for the amount of attention students get from teachers would create the illusion of a relationship between class size and test scores even if one does not exist.
:::


## Can't we just observe and compare?

```{r edu-ex-fullb, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ Small + Year + Wealth + Location + Attention,
  Small ~ Effort + Location + Wealth + Year,
  Attention ~ Small,
  Wealth ~ U1,
  Location ~ U1,
  exposure = "Small",
  outcome = "Score",
  latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "grey60", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
In reality, we can never control for all potential confounders, and we can never be certain that potential confounders we can control for are not colliders. If this figure captures every potential confounder, controlling for Year, Location, and Wealth would allow us to estimate the true causal effect of class sizes on test scores.
:::


## Can't we just observe and compare?

```{r edu-ex-collider, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ Small + Year + Wealth + Location ,
  Small ~ Effort + Location + Wealth + Year,
  Attention ~ Small + Score,
  Wealth ~ U1,
  Location ~ U1,
  exposure = "Small",
  outcome = "Score",
  latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "grey60", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
However, in this figure, note the change in the direction of the arrow connecting Score and Attention. Now, Score has a causal effect on Attention. if we added a control for Attention, that would introduce collider bias.
:::

## Can't we just observe and compare?

```{r edu-ex-fullc, echo=FALSE, fig.width=9, fig.height=4.5, out.width="100%"}

dagify(
  Score ~ Small + Year + Wealth + Location + Attention + U1,
  Small ~ Effort + Location + Wealth + Year+ U1,
  Attention ~ Small,
  exposure = "Small",
  outcome = "Score",
  latent = "U1",
  labels = c("Small" = "Small Class", "Score" = "Test Scores",
             "Year" = "Year", "Wealth" = "Parents' Wealth",
             "Location" = "Location", "Attention" = "Teacher Attentioon",
             "U1" = "Unobserved stuff"),
  coords = edu_ex_coords) |> 
  tidy_dagitty() |> 
  node_status() |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges() +
  geom_dag_point(aes(color = status), size = 14) +
  geom_dag_text(color = "black", size = 5) +
  scale_color_manual(values = c("#FF4136", "grey60", "#0074D9"),
                     na.value = "#7FDBFF") +
  guides(color = "none") +
  theme_dag()

```

:::{.notes}
And in this figure, no strategy will allow us to estimate a causal relationship because we don't have data on U1 (hence it's grey color). 
:::


## Why can't we just observe how units change over time?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"
library(ggplot2)

Year = c(0,1,2,3)
Outcome = c(NA, 1.3, 1.7,NA)
Treatment = c("Control", "Control","Control","Control")

dat = data.frame(Year, Outcome, Treatment)

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  scale_linetype_manual(values=c("solid")) +
  xlim(0,3) + 
  scale_y_continuous(limits = c(1,1.85), breaks = seq(1, 1.85, by = .1)) + 
  scale_color_manual(values = c("blue") ) +
  theme(legend.position = "none", text = element_text(size=20)) 
 
```


:::{.notes}
A natural response is to wonder about whether we can allow units to serve as their own control group over time. Imagine that we collect data on units in Year 1 before a treatment is administered and again in Year 2  after a treatment is administered. 
:::

## Why can't we just observe how units change over time?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(0.9, 1.3, 1.7, 2.1)
Treatment = c("Control", "Control","Control","Control")

dat = data.frame(Year, Outcome, Treatment)

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  scale_y_continuous(breaks = seq(1, 1.85, by = .1)) + 
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("blue") ) +
  coord_cartesian(ylim = c(1, 1.85), clip = "on") +
  theme(legend.position = "none", text = element_text(size=20))

```

:::{.notes}
Here, we cannot know how the units in our sample would have changed in the absence of the intervention. In this figure, we see that the units were increasing at the same rate in the two years before they received the intervention, and likely would have increased the same amount without the intervention.
:::

## Why can't we just compare units without randomization?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(NA, 1.2, 1.4, NA, 
            NA, 1.3, 1.7, NA)
Treatment = c("Non-treated", "Non-treated","Non-treated","Non-treated", 
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  scale_y_continuous(limits = c(1,1.85), breaks = seq(1, 1.85, by = .1)) + 
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("red", "blue") ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20))

```

:::{.notes}
Imagine we also collect data on a group of units that were not exposed to the treatment. While treatment assignment was not random, we might think that we can compare how treated units and non-treated units change in order to estimate the effect of the treatment.
:::

## Why can't we just compare units without randomization?


```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(1, 1.2, 1.4, 1.6, 
            0.9, 1.3, 1.7, 2.1)
Treatment = c("Non-treated", "Non-treated","Non-treated","Non-treated", 
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)


ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  scale_y_continuous(breaks = seq(1, 1.85, by = .1)) + 
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("red", "blue") ) +
  coord_cartesian(ylim = c(1, 1.85), clip = "on") +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20))

```

:::{.notes}
However, because the treatment was not randomly assigned, treated and non-treated units may be systematically different, leading them to have different trajectors before the treatment was administered. When we see a longer period of time, it's clear that both groups are changing at the same rate that they were prior to the treatment group being exposed to the treatment.
:::


## Causal Inference Tools

- Randomized experiments
  + Gold-standard
  + Field and survey
- Observational data
  + Natural experiments
  + Difference-in-Differences
  + Matching, Synthetic Control

:::{.notes}
There are several ways that we can estimate causal relationships in the real world. Randomized experiments provide the most reliable way to accomplish this. Under random assignment, the statistically most likely outcome is that treatment and control groups will not have any systematic differences that can confound our ability to estimate the treatment effect. 
:::

# Randomized Experimentss

## Estimating Causal Effects with Randomized Experiments 

- Randomly assigning exposure to the treatment ensures that treatment and control groups are, on average, identical on observed and unobserved pre-treatment characteristics
- [Check out this interactive visualization](https://ellaudet.iq.harvard.edu/random_treatment)
  + Observe how random assignment produces treatment and control groups with a similar composition, especially at larger sample sizes

:::{.notes}
As a reminder, randomly assigning exposure to the treatment ensures that treatment and control groups are, on average, identical on observed and unobserved pre-treatment characteristics. Check out this interactive visualization to see how random assignment produces treatment and control groups with a similar composition, especially at larger sample sizes.
:::


## Estimating Causal Effects with Randomized Experiments 

- Best way to estimate causal effect of one variable on another variable
- Randomized experiments are used across a wide variety of disciplines, from medicine and pharmacology to marketing and political science
- We can randomize different types of treatments
  + Impact evaluations randomly assign an intervention
  + Survey experiments randomize exposure to images and text
  + Clinical trials randomly assign randomly assign medical treatments

:::{.notes}
The most straightforward and reliable method for identifying the causal effect of one variable on another is through randomized experiments. Randomized experiments offer us a powerful tool because they allow us to rule-out bias confounding variables, providing a clearer picture of cause and effect relationships. This method is applied across a broad range of fields, and is the foundation for inference in many social and physical science disciplines.
For instance, when conducting an impact evaluation, interventions are randomly assigned to different groups to assess their effects. This could be educational programs, economic policies, or health-related initiatives.
In survey research, we often randomize exposure to specific images or text to understand how different types of content influence opinions or behaviors. This approach is particularly valuable in areas like marketing or political science, where the impact of media and messaging is critical.
Clinical trials are also a form of randomized experiment that's essential in pharmacology and medical research. Here, medical treatments are assigned randomly to participants to rigorously test their efficacy and safety.
By employing randomized experiments, we can draw accurate conclusions about causality, significantly improving the ability of research to inform the design policies and interventions.
:::

## Limitations and Ethical Considerations

- IEs are extremely resource-intensive with high opportunity costs
  + Large financial costs and long-evaluation periods 
  + Require deep technical expertise and implementation experience
- Findings may not be generalizable to other contexts or scales
  + Results may not be applicable to different contexts or populations
  + Challenges in scaling up successful interventions
- Ethical Concerns
  + Control group should receive the status quo
  + Possible resistance from communities or organizations
  + Impact on vulnerable populations must be considered explicitly and early

:::{.notes}
While IEs are extremely powerful, they have some important limitations and require serious engagement with potential ethical concerns.
Firstly, impact evaluations can be extremely resource-intensive. They require substantial financial investment, extensive data collection, and deep technical expertise and implementation experience. This means that resources might be diverted away from other interventions and research. This can also make them impractical for smaller organizations or interventions with limited budgets.
Another limitation is the generalizability of the results. What works well in one specific context or population might not necessarily be effective elsewhere. For this reason, it is necessary to include partners with deep contextual knowledge that can tailor interventions to the context. Relatedly, interventions that are extremely effective with a small group of people might be completely ineffective at a larger scale. Many evaluations have found that wildly successful interventions have no impact when moving from treating a very small proportion of the population to a larger proportion of the population or when moving from or from implementation by an NGO to implementation by a national government agency.
Understanding these limitations and ethical considerations is crucial for conducting responsible and effective impact evaluations and for making responsible policy recommendations based on findings.
From an ethical standpoint, impact evaluations have several unique features that must be considered. First, the status quo should be maintained for control groups. In other words, it is not acceptable to deny the control group access to something that they had before the evaluation period as part of the research. It is OK to withhold new benefits, but it is never OK to revoke or withdrawal existing benefits. Furthermore, there might be resistance from communities or organizations that feel threatened by the changes proposed through the intervention or are upset about how the treatment is being assigned. Finally, when using random assignment, it is important to consider the potential impact on vulnerable populations. Extra effort must often be made to ensure that random assignment does not accidentally exclude specific populations by virtue of their geographic location or differences in social integration. For example, when an intervention is randomly assigned at the village level, minority groups may be less likely to benefit if they are less socially integrated or actively excluded by local institutions. For more general discussion of ethics in research, including the role of ethical review boards, see the Overview for this module.
:::


## Dealing with Small Sample Sizes

- Budget limitations often lead to small sample sizes
- Small samples reduce statistical precision and may result in groups that are less comparable 
- There are tools to ensure balance on observed characteristics
  + Re-randomization
  + Blocking
  + Non-bipartite matching


:::{.notes}
One common challenge for IEs is having a small sample size. Because IEs are expensive, researchers often need to limit the number of subjects (whether they’re individuals, communities, or organizations) involved in an evaluation. Small sample sizes can reduce the statistical precision of our evaluation, and may result in an evaluation that is unable to distinguish a small impact from no impact. This severely limits our ability to learn from the research. Similarly, randomizing assignment among a small sample increases the risk that our treatment and control groups will be systematically different on pre-treatment characteristics, which can produce misleading results. Think back to the interactive visualization on the first slide of this tutorial. 

However, there are some advanced methods to reduce the risks associated with small sample sizes in randomized evaluations. However, all of these methods require researchers to have pre-treatment baseline data about the subjects among which assignment is being randomized.

First, we have Re-randomization. This technique involves randomizing the treatment assignment multiple times and selecting the assignment that returns the best balance across assignment, as measured by baseline data on subject characteristics. It's particularly useful when you're working with a limited number of participants but still want to ensure a fair distribution of characteristics.

Next is Blocking. In blocking, we group participants with similar characteristics together before randomly assigning treatments within these blocks. This method reduces variability between the groups, improving the efficiency and balance of our comparisons, which is crucial in studies with small sample sizes.

We also have Non-bipartite Matching. This approach matches participants in treatment and control groups based on similar characteristics, allowing for more flexibility and potentially better balance across a large range of pre-treatment characteristics. However, this method can be complex and computationally intensive.

It’s important to note that these methods—despite being quite sophisticated—primarily ensure balance on observed characteristics. This means they help us control for known variables, but there may still be unobserved variables that could influence the outcomes.

In summary, while these methods enhance the reliability of our results within the constraints of small sample sizes, they do not completely eliminate the limitations associated with unobserved factors. It’s essential to interpret results with an understanding of both the strengths and the limitations of these statistical techniques."
:::

## Validity

::: {.incremental}
- Internal validity
- External validity
- What are the trade-offs between experiments and observational studies?
  + Experiments have more internal validity
  + But... they often have synthetic treatments, convenience samples
- Where are these studies used in the real-world?
:::

## Additional Resources

- For a deeper-dive on impact evaluations, see these teaching resources created by J-PAL: https://www.povertyactionlab.org/resource/teaching-resources-randomized-evaluations

# Causal Inference with Observational Data

## Causality without Randomization

::: {.incremental}
- You must control for...
  + everything (observed and unobserved) that affects both the treatment variable and the outcome variable
- You *must not* control for...
  + anything that is affected by both the treatment variable and the outcome variable
- You *need to think carefully before* controlling for...
  + anything that is affected by the treatment variable that also affects the outcome variable
::: 

:::{.notes}
There are many contexts in which randomized controlled trials are not feasible. Fortunately, causal inference is also possible without random assignment. However, these tools require very precise conditions and can be difficult to implement without significant background in causal inference methods. For this reason, we provide a high-level overview so that researchers can seek-out more information on methods that seem relevant to their work.

Causal inference with observational data (situations where researchers did not actively manipulate how the treatment was assigned) has been a rapidly growing area of research in the social sciences since the 1990s. There are several specific tools used in this field, each of which rely on specific conditions in the world, many of which are fare. For example, many researchers look for situations in the world that resemble experiments due to naturally occuring random assignment, such as lotteries or discontinuities in exposure, variation in exposure to treatment over time, or creating balance on observable characteristics through matching and weighting procedures. While these methods all have weaknesses, they are essential causal inference tools for social science research.
:::


## Identification strategy

**In the real world, there are always threats to inference that we can't measure/observe or understand well enough to adjust for**

- A *research design* that allows us to isolate a causal effect from observational data
- Approximates an experiment by ensuring that the treatment and control group are similar at baseline
- These strategies rely on assumptions that we can *attempt* to validate

:::{.notes}
In the real world, the main challenge with observational data is the multitude of threats to valid inference—factors we can't measure, observe, or perhaps even understand sufficiently to adjust for. These threats can bias our results and lead to incorrect conclusions.
To tackle this, researchers employ specific research designs that allow us to approximate the conditions of an experiment. These designs ensure that the treatment and control groups are as similar as possible at baseline. By doing this, we aim to isolate the variable of interest and measure its true effect on the outcome.
However, it's important to recognize that these strategies are not foolproof. They rely heavily on assumptions that we need to critically evaluate and attempt to validate. 
:::


## Holy Trinity of Causal Inference

\

1. **Difference-in-Differences**
2. Regression Discontinuity
3. Instrumental Variables

:::{.notes}
The 'Holy Trinity of Causal Inference' is a term we use to describe three powerful research designs that help us establish causality in economics and social sciences when randomized controlled trials are not possible. These methods are Difference-in-Differences, Regression Discontinuity, and Instrumental Variables.
Each of these methods relies on specific assumptions and conditions to validly infer causality. Their strength lies in their ability to provide insights into causal relationships even when experiments are impractical or unethical. It's crucial, however, to ensure that the assumptions behind each method are met in the context of your specific study."
In this tutorial, we will focus on DiD because it is the most commonly used tool and the easiest to implement. We briefly discuss regression-discontinuity. We do not discuss Instrumental variables, because they are rarely applicable in applied research.
:::

## Identification strategy

**In the real world, there are always threats to inference that we can't measure/observe or understand well enough to adjust for**

- A *research design* that allows us to isolate a causal effect from observational data
- Approximates an experiment by ensuring that the treatment and control group are similar at baseline
- These strategies rely on assumptions that we can *attempt* to validate


## Difference-in-Differences

\

```{.default}
Y_{it} = \alpha + \beta_1 \text{Treatment}_i + \beta_2 \text{Post}_t + \gamma (\text{Treatment}_i \times \text{Post}_t) + \epsilon_{it}
```

- `\gamma (\text{Treatment}_i \times \text{Post}_t)`
- Assumes measurement at two points in time

:::{.notes}
We begin with Difference-in-Differences (DiD). This method is used when we have data from both before and after an intervention, for both a treatment group and a control group. By comparing the changes in outcomes over time between these groups, DiD helps us isolate the effect of the intervention from other factors that are changing over time.
The Difference-in-Differences (DiD) approach is a quasi-experimental technique used in econometrics and statistics to estimate the effect of a specific intervention or treatment by comparing the changes in outcomes over time between a group that's exposed to the intervention (treatment group) and a group that is not (control group). The equation on this slide represents a basic DiD model: Yit=α+β1Treatmenti+β2Postt+γ(Treatmenti×Postt)+ϵitYit=α+β1Treatmenti+β2Postt+γ(Treatmenti×Postt)+ϵit
Here's what each term represents:
- Yit is the outcome variable for unit i at time t.
- α is the constant term or the baseline level of the outcome.
- Treatmenti is an indicator variable that equals 1 if the observation ii belongs to the treatment group, and 0 otherwise.
- Postt is an indicator for the post-treatment period, equaling 1 if the observation is after the intervention has been implemented, and 0 if before.
- γ is the coefficient of interest, which represents the interaction between the treatment and the post period. This interaction term captures the differential effect of the treatment over time compared to the control group.
- ϵit is the error term.
:::


## Simulation Example

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

# Load required libraries
library(dplyr)
library(modelsummary)

# Generate example data
set.seed(123)
data <- data.frame(
  treatment = rep(c(1, 0), each = 100),
  post = rep(c(1, 0), each = 50, times = 2),
  outcome = c(rnorm(50, mean = 10, sd = 2), # control: pre-treatment
              rnorm(50, mean = 10, sd = 2), # control: post-treatment
              rnorm(50, mean = 10, sd = 2), # treatment: pre-treatment
              rnorm(50, mean = 12, sd = 2)) # treatment: post-treatment
)

head(data)

```

:::{.notes}
We begin by illustrating the Difference-in-Differences (DiD) method using a simulation example in R. This approach will help us understand how to implement DiD and interpret its output practically. By simulating data that mimics a controlled experiment, we can visually comprehend how DiD estimates the effect of an intervention
First, we load two essential R libraries. dplyr is used for data manipulation, and modelsummary will help us to summarize models easily and produce nice tables for our results.
Then we set the seed of R’s random number generator, which helps in ensuring that the results are reproducible. The same set of random numbers can be generated again, which is crucial for simulations.
Next, we generate a dataset with three variables:
- treatment: A binary variable where 1 represents the treatment group and 0 represents the control group. It's repeated 100 times to match the number of observations.
- post: A binary variable indicating whether the observation is post-treatment (1) or pre-treatment (0). It is repeated 50 times and cycled twice to align with the treatment periods.
- outcome: The outcome variable, generated using a normal distribution. The means and standard deviations are set to simulate the effect of the treatment over time. The treatment group in the post-treatment period has a higher mean (12) compared to the pre-treatment period (10), which illustrates the impact of the intervention.
This simulation setup allows us to apply the DiD analysis on a dataset where we know the true impact of the intervention, thus providing a clear way to evaluate the effectiveness of the DiD method. By comparing the outcomes before and after the treatment in both the treated and control groups, we can observe how the DiD estimator works in practice.
:::

## Simulation Example

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

# Run difference-in-differences model
did_model <- lm(outcome ~ treatment * post, data = data)

# Summarize the output
modelsummary(
  list(lm(outcome ~ treatment + post, data = data), lm(outcome ~ treatment * post, data = data)),
  estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')

```

:::{.notes}
Finally, we estimate the difference in difference model and interpret the results. The first line of code fits a linear regression model (lm) where the outcome variable is regressed on the interactions between the treatment and post variables. The treatment * post syntax includes both the main effects of the treatment and post variables and their interaction term. This interaction term is the key to the DiD analysis, as it estimates the additional effect of being in the treatment group post-intervention relative to pre-intervention and control group changes.
The next block uses the modelsummary function to create a summarized table of the model outputs:
- List of Models: Two models are summarized here. The first is a simpler model without the interaction term, and the second is the full DiD model.
- Estimate Formatting: The output format includes the estimated coefficients ({estimate}), significance stars ({stars}), and standard errors ({std.error}) in parentheses.
- Goodness of Fit (gof) Omissions: The gof_omit parameter specifies which model fit indicators to omit from the summary, such as - - Information Criteria (IC), Root Mean Square Error (RMSE), etc.
- Output: The summary is set to be outputted as a PowerPoint table (table.pptx), which is useful for presentation purposes.

Finally, we interpret the results

Coefficients: Look for the coefficient of the interaction term (treatment:post). This coefficient tells you the estimated effect of the treatment, adjusted for baseline differences and trends over time.
Significance: Significance levels (indicated by stars) give an idea of the statistical significance of the estimates. Commonly, * p<0.05, ** p<0.01, *** p<0.001.
Standard Errors: These provide a measure of the variability of the coefficient estimates. Smaller standard errors suggest more precise estimates.

If the interaction term is statistically significant and positive, it suggests that the treatment had a positive effect on the outcome, over and above any baseline or time-related effects.
The size of the coefficient on the interaction term quantifies the magnitude of this effect. Comparing the two models in the summary (with and without the interaction term) can also provide insights into the importance of considering the interaction between treatment and time for capturing the causal effect accurately.

Here, we focus on Model 2, which is the differences-in-differences model:

The intercept (12.078)*  represents the adjusted average baseline outcome for the control group before the intervention, slightly higher here due to adjustments in the model.
The treatment (-1.785)* indicator coefficient is significantly larger and still negative, which reinforces the observation that the treatment group was initially performing worse than the control group.
The post (-2.585)* coefficient indicates a substantial decline in the outcome for the control group in the post-treatment period, even more pronounced than in Model 1.

The crucial insight from Model 2 is the interaction term (Treatment × Post), which indicates that the treatment effectively improved outcomes by about 2.361 units in the post period, relative to what would have been expected based on the pre-period trends and the initial negative impact of the treatment. This suggests the treatment's effectiveness becomes apparent only when considering its temporal implementation.
:::

## DiD: Assumptions

- Assumption
  + Parallel Trends: Treatment and control units would have changed in similar ways without exposure to the treatment
- Limitations
  + Requires at least two pre-treatment observations and one post-treatment observation (observations at 3 time points) 
  + Cannot rule-out confounders that only affect the treatment group and are simultaneous with the treatment
  + Estimation is more complicated when conditions change

:::{.notes}
While DiD is widely used, it comes with some important assumptions and limitations.
The Parallel Trends Assumption is critical and assumes that in the absence of treatment, the average outcomes for the treated and control groups would have followed parallel paths over time. This means any difference between these groups over time can be attributed to the treatment and not to other factors.
To validate the parallel trends assumption, we need data for at least two time points before the intervention. More time points can increase the robustness of the model by allowing for more complex specifications and validation of the parallel trends assumption.
The simplicity of DiD can also be a limitation. For instance, if other events affect the treated and control groups differently at the same time as the treatment (confounding events), this can bias the results. Moreover, the method only ensures balance on observed characteristics; unobserved biases that vary over time can still affect the outcome.

Finally, estimation can get much more complicated when the treatment affects different treatment units at different times or when the treatment “turns on and off”.
:::

## Why can't we just observe how units change over time?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

library(ggplot2)

Year = c(0,1,2,3)
Outcome = c(NA, 1.3, 1.7, NA)
Treatment = c("Treatment", "Treatment","Treatment","Treatment")

dat = data.frame(Year, Outcome, Treatment)

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("blue") ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
In the following slides, we visualize the logic of difference-in-difference designs. Imagine we have a treatment that was administered in between data collection in Year 1 and Year 2. Traditional evaluations often measure treatment units before and after an intervention
:::

## Why can't we just observe how units change over time?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(0.9, 1.3, 1.7, 2.1)
Treatment = c("Treatment", "Treatment","Treatment","Treatment")

dat = data.frame(Year, Outcome, Treatment)

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("blue") ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
However, this method can’t rule-out the possibility that the treatment group was already improving at a steady pace, and would have improved by the same amount in the absence of the intervention. Here, we see that the blue line followed a consistent trend throughout the entire period, we it doesn’t look like the intervention had anything to do with the improvement between Y1 and Y2.
:::

## Why can't we just compare units without randomization?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(NA, 1.2, 1.4, NA, 
            NA, 1.3, 1.7, NA)
Treatment = c("Control", "Control","Control","Control", 
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)
dat$Treatment = factor(dat$Treatment, levels = c("Treatment", "Control"))

ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("blue", "red") ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
One solution is to have a comparison group that did not receive the treatment. Here, we see that the improvement between Y1 and Y2 was larger for the treatment group, suggesting there may have been an effect of the treatment.
:::

## Why can't we just compare units without randomization?


```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(1, 1.2, 1.4, 1.6, 
            0.9, 1.3, 1.7, 2.1)
Treatment = c("Control", "Control","Control","Control", 
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)
dat$Treatment = factor(dat$Treatment, levels = c("Treatment", "Control"))


ggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  xlim(0,3) + 
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "solid")) +
  scale_color_manual(values = c("blue", "red") ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
However, if the treatment was not randomly assigned, we can’t be confident that these groups would have changed in similar ways without the intervention. In this example, the control group was improving more slowly before the intervention, suggesting that the treatment did not have any effect on the improvement of the treatment group.
:::


## Why can't we just compare units without randomization?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(NA, 1.3, 1.5, NA,
            1, 1.2, 1.4, NA,
            1.1, 1.3, 1.7, NA)
Treatment = c("Comparison","Comparison","Comparison","Comparison",
              "Control", "Control","Control","Control",
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)
dat$Treatment = factor(dat$Treatment, levels = c("Treatment", "Comparison", "Control"))

ggplot(data = dat, aes(x = Year, y = Outcome,  color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "dotted", "solid")) +
  scale_color_manual(values = c("blue", "black", "red"  ) ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
Difference-in-differences allows us to confirm whether or not the treatment and control group were changing at a similar rate before the treatment was administered (known as parallel trends). If the size of this change increases more for the treatment group than it does for the control group, we can consider this strong evidence for a causal effect of the treatment.
:::

## Why can't we just compare units without randomization?

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

Year = c(0,1,2,3)
Outcome = c(NA, 1.3, 1.5, 1.7,
            1, 1.2, 1.4,1.6,
            1.1, 1.3, 1.7, 1.9)
Treatment = c("Comparison","Comparison","Comparison","Comparison",
              "Control", "Control","Control","Control",
              "Treatment", "Treatment", "Treatment", "Treatment")

dat = data.frame(Year, Outcome, Treatment)
dat$Treatment = factor(dat$Treatment, levels = c("Treatment", "Comparison", "Control"))

ggplot(data = dat, aes(x = Year, y = Outcome,  color = Treatment)) +
  geom_line(aes(linetype=Treatment),size=2) +
  geom_point(size = 6) +
  ylim(0.8, 2.2) +
  scale_linetype_manual(values=c("solid", "dotted", "solid")) +
  scale_color_manual(values = c("blue", "black", "red"  ) ) +
  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),
        legend.title=element_blank())

```

:::{.notes}
Observations for more time periods allow us to be more confident in these findings.
:::

## Regression Discontinuity Designs

- Estimating causal effects using a naturally occurring cutoff or threshold that determines which subjects are exposed to a treatment
- Assumes that subjects just below and just above are similar on all characteristics aside from receiving the treatment
- Allows researchers to estimate a Local Average Treatment Effect (LATE)

:::{.notes}
The main idea of RDD is to estimate causal effects by taking advantage of a naturally occurring cutoff or threshold. This could be something like a test score determining eligibility for a scholarship, where the cutoff score is the threshold.
In RDD, we make a critical assumption: individuals who are just below and just above this cutoff are similar in all respects except for the exposure to the treatment. This assumption is crucial because it implies that any differences observed between these two groups can be attributed to the treatment effect and not to other underlying differences. It must be the case that subjects cannot change their behavior to ensure that they fall on one side of the cutoff instead of the other, which we refer to as “sorting”.
Using RDD allows researchers to estimate what we call the Local Average Treatment Effect, or LATE. This measure tells us the effect of the treatment on those who are precisely at the threshold—those who are affected by being just above or just below the cutoff. It is important to note that this effect may not generalize to individuals further from the threshold.
This method is particularly powerful in settings where randomized control trials are impractical or unethical. It provides a robust way to measure the impact of treatments or interventions in a natural setting, making it a valuable tool in our statistical toolbox.
:::

## Example

- Exploring the effect of a merit-based scholarship to attend college on future earnings
- We are interested in this relation:
  + A[Merit-based Scholarship] --> B[Earnings after college]

- Suppose we run a regression such as:

`Earnings_i = \alpha + \beta \times Scholarship_i + \epsilon_i`

Problems?
- **Confounding**: People who earn merit-based scholarships are different than those who do not in important ways: in grades, in abilities, in drive...

:::{.notes}
Let’s use a practical example to illustrate the concept of causality in statistical research. We are focusing on the effect of a merit-based scholarship on future earnings after college.

Imagine we want to analyze whether receiving a merit-based scholarship to attend college influences a person's earnings after they graduate. The relationship we're interested in is straightforward: does the scholarship lead to higher earnings? To explore this, one might think about running a simple regression model where earnings after college depend on whether a person received a scholarship. The model might look something like this:

Earningsi=α+β×Scholarshipi+ϵiEarningsi=α+β×Scholarshipi+ϵi

Here, αα represents the baseline earnings, ββ represents the effect of the scholarship, and ϵiϵi is the error term.

However, there's a significant problem with this approach: confounding variables. People who earn merit-based scholarships are likely different from those who do not in several important ways. They might have higher grades, better test scores, or more drive and ambition. These differences can influence their future earnings independently of the scholarship. Thus, any analysis that doesn't account for these factors might mistakenly attribute the effect of these underlying characteristics to the scholarship itself. This is a classic example of the confounding problem in causal inference. 
:::

## Example

- Example: effect of a merit-based scholarship to attend college on future earnings
- Suppose we know that scholarships are assigned based on a score.
  + A[Score] --> B[Merit-based Scholarship] --> C[Earnings after college]
- ...and there is a strict rule to allocate scholarships:
  + If score > \(c_0\) → Scholarship
  + If score ≤ \(c_0\) → No Scholarship
- Students just below \(c_0\) and just above \(c_0\) are likely to be very similar.
- Therefore, for people within small bandwidth around the \(c_0\) threshold we can estimate a **Local Average Treatment Effect**.

:::{.notes}
Imagine scholarships are assigned based on a score, and there's a strict rule for allocation: if a student's score is above a certain threshold, c0, they receive a scholarship; if it's below, they do not.

The key here is that students just below and just above the threshold, c0, are very similar in all respects except for the receipt of the scholarship. This similarity allows us to make a clear comparison, focusing on the effect of the scholarship alone, without the confounding factors we discussed earlier.

Given this setup, we can apply a Regression Discontinuity Design. We focus on students within a small bandwidth around the threshold score. This allows us to estimate what's known as the Local Average Treatment Effect, or LATE. The LATE gives us insights into the impact of the scholarship on those students who are right around the cutoff—those who just qualify and those who just miss out.

This method is powerful because it closely mimics a randomized experiment, the gold standard in causal inference, by comparing very similar individuals who differ only in the treatment received—here, the scholarship based on their score relative to c0c0.
:::

## Example

![](img/rdd3.png){.fragment}


## Adjusting on Observables

- Focuses analysis on units with characteristics that are common in both the treatment and control group
  + Assumes that units similar on observed characteristics are also similar on unobserved characteristics (Ignorability)
  + Benefits from large samples
- Matching
  + Genetic matching, Propensity score matching
- Weighting
  + Entropy Balancing, Inverse probability weighting, Synthetic control

:::{.notes}
Adjusting on observable characteristic can be useful where randomization is not possible and we do not have a cutoff in the assignment of a treatment.
Matching or weighting involves pairing or grouping units—like individuals, schools, or communities—based on similar observable characteristics. The goal here is to create equivalent groups that mimic the conditions of a randomized experiment by focusing our analysis on units with characteristics that are common in both the treatment and control group. This approach is particularly useful in observational studies. For example, in healthcare research, patients receiving different treatments can be matched based on variables such as age, pre-existing conditions, and lifestyle choices to assess the effectiveness of a new drug or treatment protocol.
Matching and weighting techniques rely on the assumption of "ignorability" or "conditional independence assumption" (CIA). This assumption holds that once you control for the observed covariates, the assignment to the treatment group is as good as random. In practical terms, this means all confounders must be observed and correctly measured so that any two units with the same values on observed covariates have the same potential outcomes irrespective of treatment status. In other words, the key assumption underpinning this method is that units matched on observables are comparable. However, it is impossible to validate this assumption, posing a fundamental limitation of this method.
Many matching and weighting methods, particularly those that rely on propensity scores, perform better with large samples. In smaller samples, these methods may struggle to find appropriate matches, reducing the statistical power of the study and increasing the variance of the estimate.
Importantly, these methods can be paired with DiD designs to make the parallel trends assumption more realistic.
:::

# Multiple Hypothesis Testing

## Statistical Power

- **Statistical Power**: The probability that a statistical test will correctly detect a treatment effect when there actually is one
- **Factors Affecting Power**:
  - **Sample Size**: Larger samples generally increase power.
  - **Effect Size**: Larger effects are easier to detect.
  - **Significance Level**: Lower $\alpha$ levels reduce power, though they decrease the risk of false positives.
  - **Variability**: Less variability (i.e., lower standard deviation) in data increases power.

:::{.notes}
Statistical power is fundamental to the design and interpretation of any research study. Statistical power is defined as the probability that our test will correctly reject a false null hypothesis. In simpler terms, it's the likelihood that our study will actually detect an effect if there is one to be found.
Why is power important? Without sufficient power, our study might miss detecting real effects, leading to what we call a Type II error, or a false negative. This means we might conclude there is no effect when, in fact, there is one. If a treatment has a positive impact, but we conclude that it doesn’t, this can waste valuable resources and deprive people of welfare-enhancing benefits.
Several key factors influence the power of a statistical test. First, the sample size: larger sample sizes increase the power of a test, improving our chances of detecting an effect. Second, the effect size: if the effect we are looking for is larger, it will be easier to detect. Third, the significance level, typically set at 0.05 for many tests. Lowering this level makes it harder to achieve statistical significance, which can reduce power, though it also makes it less likely to get a false positive. Lastly, variability in our data: less variability means our effects are more discernible, which increases power.
Therefore, it's crucial to conduct a power analysis during the design phase of a study. This analysis will help us determine the necessary sample size and other parameters to ensure our study has adequate power to detect meaningful effects. By planning for power, we maximize the reliability and validity of our research findings
:::

## Statistical Power

- **Planning for Power**: Conduct power analyses before study design to determine the necessary sample size to achieve adequate power.
- **Many tools exist to estimate the power of your research design**:
  - This tool from EGAP is a good starting point: [https://egap.shinyapps.io/power-app/](https://egap.shinyapps.io/power-app/)
  - Researchers with more sophisticated designs should check out the `declaredesign` package for R.

:::{.notes}
Planning for statistical power is critical because it allows us to determine the necessary sample size and other parameters needed to achieve adequate power in our study. By conducting power analyses before the study design, we ensure that our study is capable of detecting the effects we are interested in observing, thus reducing the risk of Type II errors—failing to detect an effect when there actually is one.
To aid in these analyses, there are numerous tools available that can estimate the power of your research design. One accessible tool that I recommend for those starting out is the EGAP Power Analysis Tool, which you can find online. This tool is user-friendly and can be very helpful in preliminary power calculations.
For those involved in more sophisticated study designs, I suggest exploring the 'DeclareDesign' package for R. This package allows for a more detailed and nuanced approach to planning your study, taking into account complex design elements and providing a robust framework for understanding the interplay between design decisions and statistical power.
:::

## Multiple Hypothesis Tests

- Researchers usually want to measure important outcomes in multiple ways
- Testing the same hypothesis with multiple separate measures will increase the probability of mistakenly finding a positive effect even
- Traditional correction methods can reduce statistical power
- Index measures can help to avoid these problems

:::{.notes}
Researchers usually want to measure important outcomes in multiple ways. This can be extremely important. Any individual approach to measuring an outcome might be subject to unexpected measurement error. For example, if we want to measure the effect of an intervention on civic engagement, survey-based measures asking about recent behavior are vulnerable to subjects over-stating their engagement or mis-remembering the number of times they’ve participated in politics. For this reason, we might also want to consult administrative records whether a person voted or volunteered with community organizations.
However, when we test the same hypotheses using multiple different measures, we encounter a significant statistical challenge known as multiple hypothesis testing. This issue arises because the more tests we perform, the higher the likelihood of encountering a false positive. Essentially, if we conduct, say, 20 tests, each at a significance level of 5%, we might expect one of those tests to show significant results purely by chance, even if all our null hypotheses are true.
Traditionally, statisticians have used methods such as the Bonferroni correction to adjust the significance levels when multiple comparisons are made. Essentially, this increases the level of statistical significance required to conclude that an effect exists. However, these methods reduce the power of statistical tests, increasing the risk of falsely concluding that a treatment was not effective. Especially in the context of expensive IEs where sample sizes are often small, such corrections should be avoided when possible.
This is where index measures come into play. By combining several indicators into a single index, we reduce the total number of hypotheses being tested. This consolidation not only simplifies our analysis but also inherently controls for the risk of multiple hypothesis testing errors. Consequently, our statistical analysis becomes not just simpler, but more accurate and meaningful, focusing on robust, interpretable results rather than being clouded by potential statistical anomalies.
:::

## Creating Index Measures

**When to create an index measure**

- When you have many ways of measuring a single concept
  + This is true for outcome measures, treatment measures, and covariates

**Benefits of index measures**

- Simplifies analysis (fewer graphs, tables, etc.)
- Reduces number of hypotheses being tested

:::{.notes}
This is a crucial technique, especially when you have numerous ways to measure a single concept. This applies to various types of measures including outcome measures, treatment measures, and covariates.
Now, why do we create index measures? There are two main benefits: First, it simplifies our analysis. By combining multiple indicators into a single index, we reduce the complexity of our data. This means fewer graphs and tables, and a more streamlined presentation of our results. Second, creating an index measure helps in reducing the number of hypotheses being tested. This is important because it minimizes the risk of statistical errors that can occur from multiple testing.
:::

## Additive Scale

**What is an additive scale?**

- Simple sum across columns (index = column_1 + column_2)

**When to use an additive scale**

- When variables are measured on a common scale
- When you are interested in a cumulative amount of something
  + Number of times someone engaged in a specific behavior
  + Amount of money from several different sources

:::{.notes}
An additive scale is a straightforward yet powerful statistical tool used to combine multiple variables into a single index. This is achieved by simply summing up the values across columns. For instance, if you have two variables, the index would be calculated as the sum of column_1 and column_2.
Now, let's talk about when it's appropriate to use an additive scale. This method is particularly useful when the variables you're combining are measured on a common scale. This ensures that each variable contributes equally to the final index score.
An additive scale is also beneficial when you're interested in a cumulative quantity of something. For example, if you want to measure the total number of times an individual has engaged in a specific behavior, or if you're looking to calculate the total amount of money received from several different sources.
By using an additive scale, you can create a single measure that represents a comprehensive view of these cumulative or combined behaviors or contributions, making your data analysis both simpler and more meaningful.
:::

## Additive Scale

**Benefits of additive scales**

- Interpretability: number on the original scale
- Simplicity: Just plain addition

:::{.notes}
Let's dive into some of the key benefits of using additive scales in our data analysis.
First, we have interpretability. One of the major advantages of an additive scale is that the resultant index remains on the original scale of the variables. This means that the numbers produced are easy to understand and interpret because they directly relate to the familiar measures of the variables involved.
Second is simplicity. The process involves plain addition, which makes it not only easy to calculate but also straightforward to explain to stakeholders or audiences who may not be familiar with complex statistical techniques. This simplicity also reduces errors in computation and data handling, making additive scales a reliable choice for many research scenarios.
:::

## Additive Scale

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

set.seed(123)
# Create a data frame with 200 observations
data <- data.frame(
  variable1 = rnorm(200, mean = 50, sd = 10),  # Generate 200 random numbers for the first variable
  variable2 = rnorm(200, mean = 30, sd = 5)   # Generate 200 random numbers for the second variable
)

# Create an additive index variable
# This index is simply the sum of variable1 and variable2
data <- data %>%
  mutate(additive_index = variable1 + variable2)

# Print the first few rows of the generated data
# head() function shows the first six rows of the data frame by default
# The sum of variable1 and variable2 is equivalent to the additive_index value
head(data)
```

## Averaged Z-Scores

**What is a z-score?**

- `Z = (X - \mu) / \sigma`
- Standardized: Mean of 0 and standard deviation of 1

**When to use averaged z-scores**

- When variables are measured on different scales 
- When variables cannot be summed

:::{.notes}
Another useful method is the 'Averaged Z-Scores.' Let's start by understanding what a z-score is. A z-score, or standard score, is a way to describe a value's relationship to the mean of a group of values, measured in terms of standard deviations from the mean. Mathematically, it's expressed as Z=(X−μ)/σZ=(X−μ)/σ, where X is the variable, μ is the mean of that variable, and σ is the variable’s standard deviation. This standardization process transforms the data to have a mean of 0 and a standard deviation of 1.
Now, when should we use averaged z-scores? This approach is particularly useful in two scenarios:
- When variables are measured on different scales: If you're working with variables that have different units or scales, converting them into z-scores allows them to be compared and combined meaningfully. This is because z-scores normalize the data, removing the units and putting everything on a common scale.
- When variables cannot be summed directly: There are cases where it's not appropriate or possible to sum variables directly, perhaps due to their differing natures or units. Averaged z-scores provide a way to aggregate these variables into a single index while retaining the statistical integrity of the data.
By averaging these standardized scores, we can create composite indices that allow us to conduct a single hypotheses that combines multiple measures of the same outcome. 

:::

## Averaged Z-Scores

\

**Benefits of averaged z-scores**

- Interpretability: Standard deviations from the mean
- Outlier detection: abs(3)

:::{.notes}
Continuing our discussion on averaged z-scores, let's focus on the key benefits this statistical method brings to our data analysis.
Firstly, interpretability is a major advantage. By transforming our data into z-scores, we represent each value as its distance from the mean in terms of standard deviations. This means that the transformed data points tell us how many standard deviations they lie from the average. A z-score of +1.0 indicates a value one standard deviation above the mean, while -1.0 indicates one standard deviation below. This standardization makes it easier to understand where each data point stands in relation to the group average, enhancing our ability to interpret complex data sets. For this reason, z-scores are often used to report the results of IEs. A general rule of thumb is that effects of less than 0.2 standard deviations is a small effect, between .2 and .4 is a medium effect, and anything above .4 is a very large effect.
Another significant benefit is in outlier detection. By using the criterion of z-scores exceeding an absolute value of 3, we can easily identify outliers. Values that fall beyond three standard deviations from the mean are typically considered extreme. It is often important to drop extreme values from analyses to ensure that these outliers are not driving your findings.
:::

## Averaged Z-Scores

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

set.seed(123)
# Create a data frame with 200 observations
data <- data.frame(
  variable1 = rnorm(200, mean = 50, sd = 10),  # Generate 200 random numbers for the first variable
  variable2 = rnorm(200, mean = 30, sd = 5),   # Generate 200 random numbers for the second variable
  variable3 = rnorm(200, mean = 20, sd = 3)    # Generate 200 random numbers for the third variable
)

# Standardize each variable to have a mean of 0 and a standard deviation of 1
# Here, we create three new variables (z_variable1, z_variable2, z_variable3)
# which are the standardized (z-score) versions of variable1, variable2, and variable3.
data <- data %>%
  mutate(
    z_variable1 = (variable1 - mean(variable1)) / sd(variable1),  # Z-score for variable1
    z_variable2 = (variable2 - mean(variable2)) / sd(variable2),  # Z-score for variable2
    z_variable3 = (variable3 - mean(variable3)) / sd(variable3)   # Z-score for variable3
  )

```


## Averaged Z-Scores

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: "Show code"

# Create an index by averaging the z-scores of the variables
# In this case, select(., starts_with("z_variable")) selects all columns whose names start with "z_variable"
# rowMeans(select(., starts_with("z_variable"))) calculates the average of these selected columns for each row
data <- data %>%
  mutate(average_z_score = rowMeans(select(., starts_with("z_variable"))))

# Print the first few rows of the generated data
head(data)

```

## Fancier index techniques

\

- Principal Component Analysis
- Factor Analysis
- Inverse Covariance Weighting

:::{.notes}
There are also more sophisticated techniques available to create index variables. These methods not only help us create robust indices but also enrich our analytical capabilities.
First, we have Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This technique is especially useful when you want to reduce the dimensionality of your dataset while retaining as much variance as possible. It helps in creating a new underlying variable that explains most of the variability in your data.
Next, Factor Analysis is another technique that is similar to PCA but focuses more on latent variables that cannot be measured directly. This method examines the interrelationships among a large number of variables and condenses them into a few factors for easier interpretation and analysis. Factor analysis is particularly useful when your variables are related and you suspect that they are measuring underlying factors.
Lastly, Inverse Covariance Weighting. This method weights variables inversely to their covariance with other variables, aiming to minimize the impact of highly correlated variables in the analysis. It’s a way to adjust the contribution of each variable to the index based on how much unique information it provides.
:::

# Testing Heterogeneous Effects

## Interaction Terms

**What is an interaction term?**

- Simple linear models assume that the effect of predictors is independent of other factors 
- Interaction terms allow us to estimate the difference in the slope of a predictor across unit characteristics

**What are interaction terms used for?**

- Heterogeneous effects
- Difference-in-differences

:::{.notes}
Interaction terms are crucial role in understanding heterogeneous effects in regression models. Let’s start by understanding what an interaction term is. In simple linear regression models, we typically assume that the effect of one predictor on the outcome variable is independent of other factors. However, this assumption often oversimplifies reality, as the impact of a predictor can vary depending on the levels or presence of another predictor. This is crucial in fields such as medicine, where the effectiveness of a treatment might depend on patients' age or genetic background, and in economics, where the impact of a policy might differ across regions or demographic groups.
For example, we might want to test the impact of a new teaching method on students test scores. However, we might expect that the effectiveness of this new teaching method in improving students' test scores differs between students from low and high socioeconomic backgrounds.
You might also recognize interaction terms from our difference-in-differences tutorial. In policy evaluation, interaction terms are used to implement the difference-in-differences estimation. This technique compares the before-and-after differences in outcomes between a treatment group and a control group, thus controlling for common trends that might affect both groups.
Interaction terms are incorporated into regression models to allow us to estimate how the effect of one variable changes as another variable changes. Essentially, these terms help us identify and quantify synergies or interdependencies between variables.
:::

## Interaction Terms

```{.default}
Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1}*X_{i2} + \epsilon_i
```

**Example:** Continuous outcome with two binary predictors

- `\alpha`: Intercept when `X_{i1}` and `X_{i2}` are 0
- `\beta_1` Slope when `X_{i2} = 0`
- `\beta_2` Difference in `\alpha` between `X_{i2}=0` and `X_{i2}=1`
- `\beta_3` Difference in `\beta_1` between `X_{i2}=0` and `X_{i2}=1`

:::{.notes}
The formula on the slide illustrates how to include an interaction term in a regression model:
Yi=α+β1Xi1+β2Xi2+β3Xi1Xi2+ϵiYi=α+β1Xi1+β2Xi2+β3Xi1Xi2+ϵi
In this equation:
α represents the intercept, the expected value of Y when both Xi1 and Xi2 are 0.
β1 shows the change in Y when Xi1 changes from 0 to 1, assuming Xi2 is 0. It reflects the isolated effect of Xi1.
β2 indicates the difference in intercept (α) when Xi2 changes from 0 to 1, assuming Xi1 remains 0
β3 measures the difference in the slope of Xi1 between the scenarios when Xi2 is 0 and when Xi2 is 1. This coefficient is crucial for understanding how the relationship between Y and Xi1 changes in the presence of Xi2.
By adding β3 to β1, we are able to observe how the slope of the predictor Xi1Xi1 varies across different levels of Xi2, providing a richer understanding of the data dynamics. This is particularly useful in policy analysis, medical research, and economic studies, where the effect of an intervention may depend on individual or contextual characteristics.
Imagine Xi1 captures whether or not at student was exposed to the new teaching method and Xi2 captures whether or not the student is high-socioeconomic status. If the interaction term β3 is significantly positive, it suggests that high SES students benefit more from the new method compared to low SES students. This could lead to insights that help tailor educational strategies to maximize their effectiveness across different demographic groups.
:::

## Interaction Terms

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Interactions"

set.seed(123)
# Create a data frame with 200 observations
data <- data.frame(
  treatment = rep(c(1, 0), each = 100),      # Create a binary treatment variable: 1 for treatment, 0 for control
  group = rep(c(1, 0), each = 50, times = 2), # Create a binary group variable: 1 for Group 1, 0 for Group 2
  outcome = c(rnorm(50, mean = 10, sd = 2),  # Group 1: treatment
              rnorm(50, mean = 10, sd = 2),  # Group 2: treatment
              rnorm(50, mean = 10, sd = 2),  # Group 1: control
              rnorm(50, mean = 15, sd = 2))  # Group 2: control
)

# Run linear regression model with interaction term (outcome ~ treatment * group)
interaction_model <- lm(outcome ~ treatment * group, data = data)

# modelsummary() function provides a summary of the regression models
modelsummary(
  list(
    lm(outcome ~ treatment + group, data = data),    # Model without interaction term
    lm(outcome ~ treatment * group, data = data)     # Model with interaction term
  ),
  estimate = "{estimate}{stars} ({std.error})",
  statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.',
  output = "dataframe"
)

```

