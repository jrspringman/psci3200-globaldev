[
  {
    "objectID": "materials/08_final_project.html",
    "href": "materials/08_final_project.html",
    "title": "Final Project Overview",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "materials/08_final_project.html#required-readings",
    "href": "materials/08_final_project.html#required-readings",
    "title": "Final Project Overview",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "materials/08_final_project.html#slides",
    "href": "materials/08_final_project.html#slides",
    "title": "Final Project Overview",
    "section": "Slides",
    "text": "Slides\n\nSlides"
  },
  {
    "objectID": "materials/08_final_project.html#final-project-touchpoints",
    "href": "materials/08_final_project.html#final-project-touchpoints",
    "title": "Final Project Overview",
    "section": "Final Project Touchpoints",
    "text": "Final Project Touchpoints\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "assignments/03_fprqd.html",
    "href": "assignments/03_fprqd.html",
    "title": "Final Project Research Question and Dataset",
    "section": "",
    "text": "Your first assignment for the final project is sketching a research question that you’d like to investigate and identifying data that could be used to answer that question.\nSend me a quarto html file that:\n\nBriefly describes your idea for a research question\n\nThis should be at least 3-4 sentences describing some relationship in the world that you want to investigate. This should involve at least two things in the world that can be measured with existing quantitative data.\nYou are welcome to submit more than 1 idea.\n\nProposes a dataset and measures that will help you answer it\n\nThis should include a specific, existing dataset that you can access\nThis should also include mention of the specific variables within that dataset that will be used to answer the research question\n\n\nThis project should be submitted via Slack by 11:59pm EST on Monday, February 19. Your submission must include:\nAfter you submit this assignment, I will provide feedback on the viability of the questions, the suitability of the data, and the extent to which your general idea will meet my expectations for the final project."
  },
  {
    "objectID": "assignments/03_fprqd.html#assignment",
    "href": "assignments/03_fprqd.html#assignment",
    "title": "Final Project Research Question and Dataset",
    "section": "",
    "text": "Your first assignment for the final project is sketching a research question that you’d like to investigate and identifying data that could be used to answer that question.\nSend me a quarto html file that:\n\nBriefly describes your idea for a research question\n\nThis should be at least 3-4 sentences describing some relationship in the world that you want to investigate. This should involve at least two things in the world that can be measured with existing quantitative data.\nYou are welcome to submit more than 1 idea.\n\nProposes a dataset and measures that will help you answer it\n\nThis should include a specific, existing dataset that you can access\nThis should also include mention of the specific variables within that dataset that will be used to answer the research question\n\n\nThis project should be submitted via Slack by 11:59pm EST on Monday, February 19. Your submission must include:\nAfter you submit this assignment, I will provide feedback on the viability of the questions, the suitability of the data, and the extent to which your general idea will meet my expectations for the final project."
  },
  {
    "objectID": "assignments/03_fprqd.html#types-of-data",
    "href": "assignments/03_fprqd.html#types-of-data",
    "title": "Final Project Research Question and Dataset",
    "section": "Types of data",
    "text": "Types of data\nThere are many types of data in the world. Below is a brief discussion of the most common sources of data in the social sciences.\n\nElection returns\n\nThere are various compilations of election data from around the world, such as the Constituency-Level Elections Archive (CLEA)\nReturns for specific elections are often available from a country’s electoral commission website\n\nReplication data\n\nAny published research from the last 5-10 years should make the data and analysis files publicly available. You can almost always find where these replication materials are hosted on the article’s webpage at whichever journal puslished the article. Oftentimes, these data are hosted on Harvard’s Dataverse.\n\nSurvey data\n\nSurvey data is used extremely heavily on the social sciences. Most prominently in political science are the various ‘barometer’ surveys (Afrobarometer, Latinobarometer, etc.).\n\nAdministrative data\n\nData on government (or organization) programs or\n\nExpert-coded data\n\nData where experts code the characteristics of countries or political entities (such as parties)"
  },
  {
    "objectID": "assignments/03_fprqd.html#popular-public-datasets",
    "href": "assignments/03_fprqd.html#popular-public-datasets",
    "title": "Final Project Research Question and Dataset",
    "section": "Popular Public Datasets",
    "text": "Popular Public Datasets\nThere are many publicly available datasets. This Dataset of datasets may be useful in identifying datasets that are relevant for your research question. Below, I list several of the most widely used, high-quality datasets used by development scholars:\n\nVarieties of Democracy\nWorld Bank Development/Governance Indicators\nArmed Conflict Location & Event Data Project (ACLED)\nAidData\nDemographic and Health Survey\nInernational Organization for Migration"
  },
  {
    "objectID": "assignments/03_fprqd.html#devlab-datasets",
    "href": "assignments/03_fprqd.html#devlab-datasets",
    "title": "Final Project Research Question and Dataset",
    "section": "DevLab datasets",
    "text": "DevLab datasets\nThere are also many datasets collected by DevLab researchers. Below, I list several of the datasets that I was directly involved in. If you have a specific interest in a topic, I can also ask around DevLab to see if anyone has data available. In particular, I know there are several datasets on both migration and land-use that might be avaiable.\n\nMachine Learning for Peace\n\nThis data captures the volume of reporting from high-quality, local news sources on 42 distinct political events from 2012-2023 for 60 aid-receiving countries.\nFocuses on events that constitute changes in civic space, such as censorship, legal changes, and other forms of repression\n\nCambodian NGOs (n \\(\\approx\\) 100)\n\nConvenience sample\nPanel survey, financial data, networks, open-ended responses\nMeasures organization activities, spending, revenues, and management practices\n\nEthiopian University Students (n \\(\\approx\\) 900)\n\nRepresentative sample\nPanel survey, networks, open-ended responses\nMeasures political attitudes and behavior, with a focus on conflict and national/regional/ethnic identification\n\nGhanaian Radio Stations (n \\(\\approx\\) 400)\n\nConvenience sample\nQuestions on organization activities, spending, revenues, and management practices, with a focus on conflict and misinformation"
  },
  {
    "objectID": "assignments/04_fpdesign.html",
    "href": "assignments/04_fpdesign.html",
    "title": "Final Project Research Design",
    "section": "",
    "text": "Your second assignment for the final project is specifying a research plan that will guide your investigation of the research question you have identified. This assignment is designed to get you thinking more clearly about a specific hypothesis related to your research question and how you can produce and present evidence for or against it.\nLike a Pre-Analysis Plan, you will need to state testable hypotheses, describe your measurement of the variables necessary to test the hypotheses, and specify a statistical model to conduct the test. Unlike a Pre-Analysis Plan, this is just a starting point.\nThis project should be submitted via Slack by 11:59pm EST on Monday, March 5. Your submission must include:\n\nAn html file presenting your written design and figures. It is not necessary to show printed code.\nA .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using."
  },
  {
    "objectID": "assignments/04_fpdesign.html#overview",
    "href": "assignments/04_fpdesign.html#overview",
    "title": "Final Project Research Design",
    "section": "",
    "text": "Your second assignment for the final project is specifying a research plan that will guide your investigation of the research question you have identified. This assignment is designed to get you thinking more clearly about a specific hypothesis related to your research question and how you can produce and present evidence for or against it.\nLike a Pre-Analysis Plan, you will need to state testable hypotheses, describe your measurement of the variables necessary to test the hypotheses, and specify a statistical model to conduct the test. Unlike a Pre-Analysis Plan, this is just a starting point.\nThis project should be submitted via Slack by 11:59pm EST on Monday, March 5. Your submission must include:\n\nAn html file presenting your written design and figures. It is not necessary to show printed code.\nA .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using."
  },
  {
    "objectID": "assignments/04_fpdesign.html#requirements",
    "href": "assignments/04_fpdesign.html#requirements",
    "title": "Final Project Research Design",
    "section": "Requirements",
    "text": "Requirements\nThis assignment has four components. The components are presented below, along with their importance for the grading of the overall assignment.\n\nDescribe your research question and provide some background on why you find it interesting or important. Be sure to incorporate the feedback provided on the first assignment. Include references to at least two pieces of existing research that illustrate what has already been discovered about the relationship you are investigating. This could be articles published in academic journals, policy reports and white papers, articles published by think tanks and other credible organizations (ex. Brookings, the United Nations High Commissioner for Refugees, etc.), or pieces of data journalism. A full-credit response will probably be around 150-200 words. (40%)\nState at least one testable hypothesis. Distill your research question into a statement about a specific relationship you expect to see in the world. In most cases, this hypothesis will describe a causal relationship between two variables (i.e. changes in x cause changes in y). Make an argument for why you expect to see this relationship. This should be based on related findings from existing research and/or your own theoretical/logical reasoning. A full-credit response will probably be around 150-200 words. (25%)\nBriefly discuss the specific variables you will use to test your hypothesis and the dataset they are drawn from. Be sure to mention the source of the data (the organization or individuals that produced it), the unit of analysis, and the sample. Create a ggplot to visualize the relationship between the variables being used to test your hypothesis. The best method of visualizing this relationship will depend on the scale of your variables. Good methods to consider are scatter plots (for continuous variables), grouped bar charts (for ordinal or binary variables), and line graphs (for continuous variables with time-series). (25%)\nSpecify the main regression model you will use to test your hypothesis. This regression model should provide a preliminary test for or against the validity of your hypothesis. Use markdown to render the equation neatly to the html file. (10%)\n\n\n\n\n\n\n\nNote\n\n\n\nA hypothesis should look like a prediction. Often, a hypothesis states that you expect a decrease or increase in one variable to cause a decrease or increase in another variable. A hypothesis must also be falsifiable. In other words, it must be possible to prove that the hypothesis is false by showing that the relationship you are predicting to see in your data is not actually present.\n\n\nIn the next assignment, you will be asked to discuss the assumptions that your research design is making. For example, you will need to describe the assumptions that are necessary to interpret your regression model as evidence for your hypothesis. You will also be asked to propose additional tests that can help justify these assumptions, using variables that are available in your dataset."
  },
  {
    "objectID": "assignments/05_fptest.html",
    "href": "assignments/05_fptest.html",
    "title": "Final Project Expanded Research Design",
    "section": "",
    "text": "Your third assignment for the final project is refining the research plan that will guide your investigation of your research question. This assignment is designed to get you thinking more clearly about the limitations of your research design and how you can strengthen your inferences.\nThis assignment will help you plan the remaining components of your final project analysis. This should look like an expanded, cleaned-up version of your previous assignment.\nThis project should be submitted via Slack by 11:59pm EST on Wednesday, April 2. Your submission must include:\n\nAn html file presenting your written design and figures. It is not necessary to show printed code.\nA .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using."
  },
  {
    "objectID": "assignments/05_fptest.html#overview",
    "href": "assignments/05_fptest.html#overview",
    "title": "Final Project Expanded Research Design",
    "section": "",
    "text": "Your third assignment for the final project is refining the research plan that will guide your investigation of your research question. This assignment is designed to get you thinking more clearly about the limitations of your research design and how you can strengthen your inferences.\nThis assignment will help you plan the remaining components of your final project analysis. This should look like an expanded, cleaned-up version of your previous assignment.\nThis project should be submitted via Slack by 11:59pm EST on Wednesday, April 2. Your submission must include:\n\nAn html file presenting your written design and figures. It is not necessary to show printed code.\nA .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using."
  },
  {
    "objectID": "assignments/05_fptest.html#requirements",
    "href": "assignments/05_fptest.html#requirements",
    "title": "Final Project Expanded Research Design",
    "section": "Requirements",
    "text": "Requirements\nThis assignment has four components. The components are presented below, along with their importance for the grading of the overall assignment.\n\nIntroduction: Create an introduction section that discusses your research question. Incorporate previous feedback into your statement of the research question. Expand your discussion of previous work by other scholars and how your analysis will build-on earlier contributions. The best introductions will concisely explain the state of scholarly knowledge on the topic and the gap in this knowledge that your analysis aims to address. Your introduction should probably be around 400-600 words. (10%)\nHypothesis: Incorporate previous feedback into the statement of your hypothesis or hypotheses. Expand your discussion of the logic behind the relationship that your hypothesis expects. (10%)\nData: Describe the dataset(s) you are using to conduct your analysis. Discuss the specific variables you will use. Create a table or figures that communicate the mean and the range of your key variables. Add an informative caption to the ggplot figure that you created in the previous assignment. For guidance, see the captions in the academic papers we have been reading this semester. Interpret the plot; what does it tell us about your hypothesis? (30%)\nResearch Design:\n\nSpecify the main regression model you will use to test your hypothesis. Include any covariates that you will use to control for potential confounders, and justify your decision to include these covariates. If you hypothesis states a causal relationship, discuss the threats to interpreting the coefficient on your primary independent variable as an estimate of a causal effect on the outcome. Describe potential unobserved confounders. (10%)\nIdentify one empirical extension that you will conduct that will add credibility to your inference. Think back to the examples I provided regarding the effect of moving to a new city for college on student civic engagement. For example, students that already lived in Addis Ababa are likely to be different from students that moved to Addis in many ways that may affect their engagement. One extension I proposed was to restrict my analysis to only compare students that moved from an urban area with students already living in Addis. If I were still to see a negative relationship between moving and engagement among these students, this would allow me to rule-out differences between students from urban vs rural homes as a confounder. Your empirical extension should allow you to rule out at least one potential confounder. (30%)\n\nClean-up the document. If you haven’t, incorporate a Bibliography using a references.bib file and Quarto citations. Remove the printed warnings and code from your document. Create clear sections for each stage of the analysis. (20%)"
  },
  {
    "objectID": "assignments/final_project.html",
    "href": "assignments/final_project.html",
    "title": "Final Project Assignment",
    "section": "",
    "text": "The final project is the culmination of this course. It should reflect the substantive knowledge and technical skills that you developed and the work from the three touch-point assignments that you submitted and received feedback on. The objective of this assignment is to give you experience generating a viable research question based on a topic that you are interested in, refining that broad research question into a testable hypothesis, identifying data that can be used to provide evidence for or against your hypothesis, and communicating the results of your analysis to others.\nUnderstanding this process can not only help you answer research questions in the future, but can also help you assess the strength of evidence presented by others. As data becomes an increasingly important part of our lives, these skills will be useful in your career, especially if you conduct your own research or incorporate research by others into your decision-making.\nThis project should be submitted via Slack by 11:59pm EST on Friday, May 10. The grade on your submission will constitute 44% of your final grade for this course. Your submission must include:\n\nA url linking me to a clean, professional webpage on your personal website that presents your research to potential employers\nThe .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using.\n\n\n\n\n\n\n\nNote\n\n\n\nImportantly, the grade for this assignment will not be affected by whether your analysis uncovers a statistically significant relationship. If you provide a clear, plausible argument for why you expect to find a relationship between your dependent and independent variables, a null finding provides interesting information about the world! All good (and honest) researchers find that their theoretical expectations were wrong just as often as they find that they were right."
  },
  {
    "objectID": "assignments/final_project.html#overview",
    "href": "assignments/final_project.html#overview",
    "title": "Final Project Assignment",
    "section": "",
    "text": "The final project is the culmination of this course. It should reflect the substantive knowledge and technical skills that you developed and the work from the three touch-point assignments that you submitted and received feedback on. The objective of this assignment is to give you experience generating a viable research question based on a topic that you are interested in, refining that broad research question into a testable hypothesis, identifying data that can be used to provide evidence for or against your hypothesis, and communicating the results of your analysis to others.\nUnderstanding this process can not only help you answer research questions in the future, but can also help you assess the strength of evidence presented by others. As data becomes an increasingly important part of our lives, these skills will be useful in your career, especially if you conduct your own research or incorporate research by others into your decision-making.\nThis project should be submitted via Slack by 11:59pm EST on Friday, May 10. The grade on your submission will constitute 44% of your final grade for this course. Your submission must include:\n\nA url linking me to a clean, professional webpage on your personal website that presents your research to potential employers\nThe .qmd file that you used to generate the html file\nAll code should be thoroughly commented to explain the choices you are making and the techniques you are using.\n\n\n\n\n\n\n\nNote\n\n\n\nImportantly, the grade for this assignment will not be affected by whether your analysis uncovers a statistically significant relationship. If you provide a clear, plausible argument for why you expect to find a relationship between your dependent and independent variables, a null finding provides interesting information about the world! All good (and honest) researchers find that their theoretical expectations were wrong just as often as they find that they were right."
  },
  {
    "objectID": "assignments/final_project.html#structure",
    "href": "assignments/final_project.html#structure",
    "title": "Final Project Assignment",
    "section": "Structure",
    "text": "Structure\nYour final project submission should be organized into the following sections. This format roughly follows the organization of an academic research paper or a policy report.\n\nIntroduction to the Research Question (~600 words)\n\nIn the introduction, you should introduce your research question to readers. Explain why you find it interesting or important. Describe existing research that is related to this topic and include references to relevant studies. This could be articles published in academic journals, policy reports and white papers, articles published by think tanks and other credible organizations (ex. Brookings, the United Nations High Commissioner for Refugees, etc.), or pieces of data journalism. For most topics, you should be able to identify and cite at least 3-4 relevant studies.\nBriefly discuss what has already been discovered about the relationship you are investigating and how these findings shaped your research question. The best introductions will concisely explain the state of scholarly knowledge on the topic and the gap in this knowledge that your analysis aims to fill.\n\nTheory and Hypotheses (~600 words)\n\nIn the theory and hypothesis section, you should refine your broad research question into a specific, testable hypothesis. State at least one testable hypothesis. Distill your research question into a statement about a specific relationship you expect to see in the world. In most cases, this hypothesis will describe a causal relationship between two variables (i.e. changes in x cause changes in y). Make an argument for why you expect to see this relationship. This should be based on related findings from existing research (cited in the introduction) and your own theoretical/logical reasoning. Readers should have a clear understanding of why you expect to see this relationship in the world. They might not agree, but they should understand your reasoning.\n\n\n\n\n\n\nNote\n\n\n\nA hypothesis should look like a prediction. Often, a hypothesis states that you expect a decrease or increase in one variable to cause a decrease or increase in another variable. A hypothesis must also be falsifiable. In other words, it must be possible to prove that the hypothesis is false by showing that the relationship you are predicting to see in your data is not actually present.\n\n\n\nResearch Design (~600 words; 1 table)\n\nDescribe the dataset(s) you are using to conduct your analysis. Be sure to mention the source of the data (the organization or individuals that produced it), the unit of analysis, and the sample. Are there any limitations that readers should be aware of?\nDiscuss the specific variables you will use to test your hypothesis. Explain how these variables map onto the theoretical concepts that underpin your hypothesis. Create a table that communicates the mean, range, and standard deviation of these variables. You can use packages like gt() or flextable.\nSpecify the main regression model you will use to test your hypothesis. Include any covariates that you will use to control for potential confounders, and justify your decision to include these covariates. If your hypothesis states a causal relationship, discuss the threats to interpreting the coefficient on your primary independent variable as an estimate of a causal effect on the outcome. Describe potential unobserved confounders.\nEvery empirical test has shortcomings that prevent us from being fully confident in the interpretation. What are the limitations of your test? Identify one empirical extension that you will conduct that will add credibility to the inference you are trying to make with your hypothesis test. Think back to the examples I provided regarding the effect of moving to a new city for college on student civic engagement. For example, students that already lived in Addis Ababa are likely to be different from students that moved to Addis in many ways that may affect their engagement. One extension I proposed was to restrict my analysis to only compare students that moved from an urban area with students already living in Addis. If I were still to see a negative relationship between moving and engagement among these students, this would allow me to rule-out differences between students from urban vs rural homes as a confounder.\nYour empirical extension should allow you to rule out at least one potential confounder.Make sure to clarify What purpose will it serve. Be specific about how it makes us more confident in the ability of your test to answer the research question.\n\nFindings (~300 words; 1 figure; at least 1 regression table)\n\nCreate at least one ggplot to visualize the relationship between the variables being used to test your hypothesis. The best method of visualizing this relationship will depend on the scale of your variables. Good methods to consider are scatter plots (for continuous variables), grouped bar charts (for ordinal or binary variables), and line graphs (for continuous variables with time-series). Your figure should have an informative caption that allows readers to understand what is being shown. For guidance, see the captions in the academic papers we have been reading this semester. Interpret the plot; does it tell us anything about the veracity your hypothesis?\nUse the modelsummary package to present the results of the regression model testing your hypothesis. Be sure to interpret the magnitudes of the main variables in your regression model. Substantively, is this a big effect or a small effect? Once you have discussed the magnitude of the key independent variable(s), tell us whether the results are statistically significant. If you have a large substantive magnitude on your independent variable’s coefficient but no statistical significance, discuss why this might be the case. Do you think it is related to the amount of statistical power of your hypothesis test?\n\nEmpirical Extension (~300 words; at least 1 regression table)\n\nRepeat the steps above for your empirical extension.\n\nDiscussion and Policy Implications (~300 words)\n\nDiscuss what we learned from your final project. How conclusive are your results? Is this strong evidence for/against your hypothesis? If so, are there any implications for policy? If your analysis does not provide strong evidence for/against the hypothesis, what future research could provide stronger evidence?"
  },
  {
    "objectID": "assignments/final_project.html#formatting-requirements",
    "href": "assignments/final_project.html#formatting-requirements",
    "title": "Final Project Assignment",
    "section": "Formatting Requirements",
    "text": "Formatting Requirements\n\nCreate clear sections for each stage of the analysis described above\nRemove the printed warnings and code from your document\nIncorporate a Bibliography using a references.bib file and Quarto citations\nPost the resulting .html document to your webpage"
  },
  {
    "objectID": "slides/08_final_project.html",
    "href": "slides/08_final_project.html",
    "title": "Final Project Overview",
    "section": "",
    "text": "The final project is the culmination of this course\n\nReflect the substantive knowledge and technical skills that you developed\nProvide experience\n\nGenerating a research question\nRefining into a testable hypothesis\nIdentifying data to provide evidence for or against\nCommunicating the results of your analysis to others\n\n\n\n\n\n\nHelp you answer research questions in the future\nAssess the strength of evidence presented by others\nIncorporate research into your professional decision-making\n\n\n\n\n\nResearch project with data of your choosing\n\nFormulate a research question\nFind data that can help you answer that question\nApply the tools and methods from this course\nWrite-up analysis\n\nProduce a webpage to present your results for public consumption\n\n\n\n\n\nIntroduction to research question and data\nDiscussion of research design, assumptions, and threats to inference\nVisualization describing your data\nPresentation of results from a regression model and discussion of implications for research question\nRobustness test to probe assumptions or empirical extension\nDiscussion of policy implications\nAppendix including supplementary materials\n\n\n\n\n\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "slides/08_final_project.html#objectives",
    "href": "slides/08_final_project.html#objectives",
    "title": "Final Project Overview",
    "section": "",
    "text": "The final project is the culmination of this course\n\nReflect the substantive knowledge and technical skills that you developed\nProvide experience\n\nGenerating a research question\nRefining into a testable hypothesis\nIdentifying data to provide evidence for or against\nCommunicating the results of your analysis to others"
  },
  {
    "objectID": "slides/08_final_project.html#objectives-1",
    "href": "slides/08_final_project.html#objectives-1",
    "title": "Final Project Overview",
    "section": "",
    "text": "Help you answer research questions in the future\nAssess the strength of evidence presented by others\nIncorporate research into your professional decision-making"
  },
  {
    "objectID": "slides/08_final_project.html#requirements",
    "href": "slides/08_final_project.html#requirements",
    "title": "Final Project Overview",
    "section": "",
    "text": "Research project with data of your choosing\n\nFormulate a research question\nFind data that can help you answer that question\nApply the tools and methods from this course\nWrite-up analysis\n\nProduce a webpage to present your results for public consumption"
  },
  {
    "objectID": "slides/08_final_project.html#write-up",
    "href": "slides/08_final_project.html#write-up",
    "title": "Final Project Overview",
    "section": "",
    "text": "Introduction to research question and data\nDiscussion of research design, assumptions, and threats to inference\nVisualization describing your data\nPresentation of results from a regression model and discussion of implications for research question\nRobustness test to probe assumptions or empirical extension\nDiscussion of policy implications\nAppendix including supplementary materials"
  },
  {
    "objectID": "slides/08_final_project.html#final-project-touchpoints",
    "href": "slides/08_final_project.html#final-project-touchpoints",
    "title": "Final Project Overview",
    "section": "",
    "text": "Milestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "slides/08_final_project.html#types-of-data",
    "href": "slides/08_final_project.html#types-of-data",
    "title": "Final Project Overview",
    "section": "Types of data",
    "text": "Types of data\nThere are many types of data in the world. Below is a brief discussion of the most common sources of data in the social sciences.\n\nElection returns\n\nCompilations of election data, such as Constituency-Level Elections Archive (CLEA)\nReturns for specific elections are often available from a country’s electoral commission website"
  },
  {
    "objectID": "slides/08_final_project.html#types-of-data-1",
    "href": "slides/08_final_project.html#types-of-data-1",
    "title": "Final Project Overview",
    "section": "Types of data",
    "text": "Types of data\n\nReplication data\n\nAny published research from the last 5-10 years should make the data and analysis files publicly available. You can almost always find where these replication materials are hosted on the article’s webpage at whichever journal puslished the article. Oftentimes, these data are hosted on Harvard’s Dataverse."
  },
  {
    "objectID": "slides/08_final_project.html#popular-public-datasets",
    "href": "slides/08_final_project.html#popular-public-datasets",
    "title": "Final Project Overview",
    "section": "Popular Public Datasets",
    "text": "Popular Public Datasets\nDataset of datasets\n\nVarieties of Democracy\nWorld Bank Development/Governance Indicators\nArmed Conflict Location & Event Data Project (ACLED)\nAidData\nDemographic and Health Survey"
  },
  {
    "objectID": "slides/08_final_project.html#my-datasets",
    "href": "slides/08_final_project.html#my-datasets",
    "title": "Final Project Overview",
    "section": "My Datasets",
    "text": "My Datasets\n\nMachine Learning for Peace\n\nThis data captures the volume of reporting from high-quality, local news sources on 42 distinct political events from 2012-2023 for 60 aid-receiving countries.\nFocuses on events that constitute changes in civic space, such as censorship, legal changes, and other forms of repression\n\nCambodian NGOs (n \\(\\approx\\) 100)\n\nConvenience sample\nPanel survey, financial data, networks, open-ended responses"
  },
  {
    "objectID": "slides/08_final_project.html#assignment-1-research-question-and-data",
    "href": "slides/08_final_project.html#assignment-1-research-question-and-data",
    "title": "Final Project Overview",
    "section": "Assignment 1: Research Question and Data",
    "text": "Assignment 1: Research Question and Data\nMust be submitted via Slack by 11:59pm EST on Monday, February 19\n\nSketch a research question that you’d like to investigate\nIdentify data that can answer that question\nI will provide feedback on the viability of the questions, the suitability of the data, and the extent to which your general idea will meet my expectations for the final project"
  },
  {
    "objectID": "slides/08_final_project.html#assignment-1-research-question-and-data-1",
    "href": "slides/08_final_project.html#assignment-1-research-question-and-data-1",
    "title": "Final Project Overview",
    "section": "Assignment 1: Research Question and Data",
    "text": "Assignment 1: Research Question and Data\nSend me a quarto html file that:\n\nBriefly describes your idea for a research question\n\n3-4 sentences describing some relationship in the world that you want to investigate\nThis should involve at least two things in the world that can be measured with existing data\nYou may submit more than 1 idea.\n\nProposes a dataset and measures that will help you answer it\n\nSpecific, existing dataset that you can access\nSpecific variables that will be used to answer the research question"
  },
  {
    "objectID": "slides/08_final_project.html#research-question-and-data",
    "href": "slides/08_final_project.html#research-question-and-data",
    "title": "Final Project Overview",
    "section": "Research Question and Data",
    "text": "Research Question and Data\nMust be submitted via Slack by 11:59pm EST on Monday, February 19\n\nSketch a research question that you’d like to investigate\nIdentify data that can answer that question\nI will provide feedback on the viability of the questions, the suitability of the data, and the extent to which your general idea will meet my expectations for the final project"
  },
  {
    "objectID": "slides/08_final_project.html#research-question-and-data-1",
    "href": "slides/08_final_project.html#research-question-and-data-1",
    "title": "Final Project Overview",
    "section": "Research Question and Data",
    "text": "Research Question and Data\nSend me a quarto html file that:\n\nBriefly describes your idea for a research question\n\n3-4 sentences describing some relationship in the world that you want to investigate\nThis should involve at least two things in the world that can be measured with existing data\nYou may submit more than 1 idea.\n\nProposes data and measures that will help you answer it\n\nSpecific, existing dataset that you can access\nSpecific variables that will be used to answer the research question"
  },
  {
    "objectID": "slides/08_final_project.html#types-of-data-2",
    "href": "slides/08_final_project.html#types-of-data-2",
    "title": "Final Project Overview",
    "section": "Types of data",
    "text": "Types of data\n\nSurvey data\n\nSurvey data is used extremely heavily on the social sciences. Most prominently in political science are the various ‘barometer’ surveys (Afrobarometer, Latinobarometer, etc.).\n\nAdministrative data\n\nData on government (or organization) programs or\n\nExpert-coded data\n\nData where experts code the characteristics of countries or political entities (such as parties)"
  },
  {
    "objectID": "slides/08_final_project.html#my-datasets-1",
    "href": "slides/08_final_project.html#my-datasets-1",
    "title": "Final Project Overview",
    "section": "My Datasets",
    "text": "My Datasets\n\nEthiopian University Students (n \\(\\approx\\) 900)\n\nRepresentative sample\nPanel survey, networks, open-ended responses\n\nGhanaian Radio Stations (n \\(\\approx\\) 400)\n\nConvenience sample"
  },
  {
    "objectID": "materials/06_regression.html",
    "href": "materials/06_regression.html",
    "title": "Regression",
    "section": "",
    "text": "DSS Ch 3\nDSS Ch 4"
  },
  {
    "objectID": "materials/06_regression.html#required-readings",
    "href": "materials/06_regression.html#required-readings",
    "title": "Regression",
    "section": "",
    "text": "DSS Ch 3\nDSS Ch 4"
  },
  {
    "objectID": "materials/06_regression.html#assignment",
    "href": "materials/06_regression.html#assignment",
    "title": "Regression",
    "section": "Assignment",
    "text": "Assignment\n\nCreate Github repo for the course\n\nInstructions\nGit with R\nBlackwell tutorial"
  },
  {
    "objectID": "materials/06_regression.html#slides",
    "href": "materials/06_regression.html#slides",
    "title": "Regression",
    "section": "Slides",
    "text": "Slides\n\nSlides"
  },
  {
    "objectID": "slides/lees-quant.html",
    "href": "slides/lees-quant.html",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "This activity draws on Chapter 2 in Data Analysis for Social Science: A Friendly and Practical Introduction (DSS)\n\ninteractive visualization: mean and sd\ninteractive visualization: correlation\n\n\n\n\n\nComparisons are at the heart of quantitative analysis\nLots of bad analysis implies comparisons, but doesn’t actually make them\n\nEx. 10 things that extremely successful people do to be productive\nEx. 70% of participants reported an improvement\n\nCorrelation is the most basic tool for making comparisons with data\n\n\nMaking comparisons is one of the most essential parts in research, underpinning everything from simple descriptive studies to machine learning and impact evaluations. It is difficult to learn without comparison. For example, an article that identifies 10 habits of successful people might seek to provide guidance on what behaviors you can adopt to become more successful. However, we should also ask how common these behaviors are among people who are not successful. Similarly, an evaluation might point out that 70% of organizations that participated in a capacity-building intervention reported an improvement in their capacity over the course of the intervention. But we should also ask whether these improvements were larger than improvements experienced by similar organizations that didn’t participate. Correlation is the fundamental tool that allows us to answer these important questions.\n\n\n\n\nWhat do we need to calculate correlations?\n\nMeasures of central tendency\n\nMean\nProportion\n\nMeasures of spread\n\nVariance\nStandard deviation\n\n\n\nIn order to calculate correlations, we need to begin with quantities that we want to compare across groups. For example, we might want to calculate the mean, or the average, of a certain variable. We also need to measure how much variation, or spread, there is in these measures. Are the units in our sample very similar, or are their values spread our across a wide range?\n\n\n\n\n\\mu_X = \\frac{1}{n} \\sum_{i}^{n} X_i\n\nmy_vector = rnorm(10, mean = 10, sd = 5)\n# Step 1: Sum the values\nsum_values &lt;- sum(my_vector)\n# Step 2: Count the number of elements\ncount_elements &lt;- length(my_vector)\n# Step 3: Calculate the mean\nmean_value &lt;- sum_values / count_elements\n\nprint(mean_value)\n\n[1] 9.499699\n\nmean(my_vector)\n\n[1] 9.499699\n\n\n\nWe begin by reviewing some basic statistical concepts and illustrating those concepts using R code. This will build your familiarity with R while providing a quick review of basic statistical concepts. Everyone knows how to calculate the mean of a variable. In R, there are multiple ways to do it. Here, I created a random variable with 10 values and an average value of 10. You can calculate the mean by hand or using the function ‘mean()’. Both approaches should give you the same value.\n\n\n\n\n\\sigma^2_X = \\frac{1}{N} \\sum_{i}^{N} (X_i - \\mu_X)^2\n\n\nWhat does the square in \\sigma^2 accomplish?\nWhat are the implications for interpretation?\n\nUnits\nDistribution\n\nEven with these basic measures, we’re already thinking about the distribution!\n\n\n\nYou’re also probably familiar with variance. Variance measures how spread out the values of a variable are around the mean. For the variance of X, we take the sum of the squared differences between each value of X and the mean value of X, and then divide by the number of observations. While squaring the differences is necessary to avoid positive and negative values cancelling out one another, it poses challenges for interpretation. Most importantly, the variance is not on the scale of the variable, so there is no intuitive way to interpret its value. The squaring of the differences also means that more weight is put on observations that are very far from the mean value. The next few slides illustrates this with simulated data.\n\n\n\n\n\n\nCreate a vector\n## Create vector, sort by size, and store var\nset.seed(123)\ndat = rnorm(10, mean = 10, sd = 5)\ndat = sort(dat)\no_var = var(dat)\nprint(dat)\n\n\n [1]  3.674694  6.565736  7.197622  7.771690  8.849113 10.352542 10.646439\n [8] 12.304581 17.793542 18.575325\n\n\n\nFirst, we create a vector of simulated data with a known mean and standard deviation. In statistics and computer science, a vector is a one-dimensional array, which is a data structure that stores elements of the same type in an ordered sequence.\n\n\n\n\n\n\nAdd a constant to a big number\n## Create new dataframe for big addition and store vector length\nb_dat = dat\nind = length(b_dat)\n\n## Add four to the largest number in the vector and calculate size of var increase\nb_dat[ind] = b_dat[ind] + 4\nb_var = var(b_dat)\nval = b_var - o_var\ncat(\"Variance increases by\", val )\n\n\nVariance increases by 8.890842\n\n\n\nSecond, we add 4 to the largest value of X and observe the change in the standard deviation\n\n\n\n\n\n\nAdd a constant to a smaller number\n## Create new dataframe for small addition\ns_dat = dat\n\n## Add four to the smallest number in the vector and calculate size of var increase\ns_dat[ind-2] = s_dat[ind-2] + 4\ns_var = var(s_dat)\nval = s_var - o_var\ncat(\"Variance increases by\", val )\n\n\nVariance increases by 3.316847\n\n\n\nFinally, we add 4 to the smallest value of X and observe that the change in the standard deviation is much smaller. Again, this is because squaring the differences places more weight on extreme values very far from the mean\n\n\n\n\n\\sigma_X = \\sqrt{\\frac{1}{N} \\sum_{i}^{N} (X_i - \\mu_X)^2}\n\n\nWhat does the \\sqrt{} accomplish?\nWhat are the implications for interpretation?\n\nExpressed in the same units as the observations\nHow far we expect each observation to be from the mean, on average\n\nIt is often desirable to report effect sizes as SDs\n\n\n\nAnother measure of spread is the standard deviation. The standard deviation is just the square-root of the variance, and tells us the average absolute difference between values of X and the mean of X. By taking the square root of the variance, we convert it back to the original units of the data, making it a more interpretable measure of dispersion. For example, if the data represents weights in kilograms, the standard deviation will also be in kilograms. A larger standard deviation indicates that the data points are more spread out from the mean, and a smaller standard deviation indicates that they are closer to the mean. In many research contexts, especially in the social sciences and medicine, it’s common to report effect sizes relative to the standard deviation. This is because it provides a scale of variability that can be used to compare the magnitude of an effect across different studies or different units of measurement. For example, an effect size of 0.2 standard deviations is often considered a small effect, regardless of the actual units of the data.\n\n\n\n\n\nCheck out this interactive visualization\n\nObserve how changes in the standard deviation affect the shape of the distribution\n\n\n\nIn a new tab, open this interactive visualization of how changes in the standard deviation affect the shape of the distribution. Follow the instructions to see what increases in the spead of a variable looks like.\n\n\n\n\n\n\nCovariance \\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y})\n\nProduct of the deviations\nRange: unbounded\n\nCorrelation coefficient \\text{Cor}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\nCovariance normalized by product of SDs\nRange: -1 to 1\n\n\n\n\nNow that we have reviewed the fundamental components of correlation, we can turn to the three primary ways of measuring correlation. Correlations rely on two variables, typically denoted as X and Y.\nCovariance measures the joint variability of two variables. As you can see from the equation, the covariance is calculated by summing the product of the deviations of X and Y and dividing this value by the number of observations. Deviations are calculated by taking each value of X and Y and subtracting the variable’s mean value. Covariance can tell us if increases in one variable generally correspond to increases in another (positive covariance), or if increases in one correspond to decreases in the other (negative covariance). However, the scale of the covariance is not standardized, making it difficult to interpret. , In fact, the results are unbounded, meaning that there is no minimum or maximum value for covariance.\nNext, we turn to the correlation coefficient, which normalizes the covariance by the product of the variables’ standard deviations, forcing them to take a bounded range that can tell us the strength and direction of correlation. The Correlation Coefficient tells us the strength and direction of a linear relationship between two variables. A correlation of 1 means there is a perfect positive linear relationship, -1 means there is a perfect negative linear relationship, and 0 implies no linear relationship at all.\nIn summary, while covariance provides a first glance at the direction of a relationship, the correlation coefficient helps us understand both the strength and direction in a standardized format, making it easier to interpret and compare across different studies or datasets.\n\n\n\n\n\n\nCalculate covariance\nset.seed(1234)\n# Generate two vectors of 10 random numbers each from a normal distribution\nx &lt;- rnorm(10, mean = 10, sd = 5)\ny &lt;- rnorm(10, mean = 10, sd = 5)\n# Calculate the means of both vectors\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\n# Calculate the covariance between the two vectors\ncovariance &lt;- sum((x - mean_x) * (y - mean_y)) / (length(x) - 1)\ncov(x,y)\n\n\n[1] -4.657457\n\n\n\n\n\n\n\nCalculate correlation\n# Calculate the standard deviations of both vectors\nsd_x &lt;- sd(x)\nsd_y &lt;- sd(y)\n\n# Calculate the correlation coefficient between the two vectors\ncorrelation &lt;- covariance / (sd_x * sd_y)\ncor(x,y)\n\n\n[1] -0.1752832\n\n\n\n\n\n\n\nSlope \\beta_X = \\frac{\\text{Cov}(X, Y)}{\\sigma^2_X}\n\nCovariance normalized by variance\nExpected change in Y with 1-unit change in X\n\n\n\n\nFinally, we can calculate the slope of a regression line, which normalized the covariance by the variance and tells us the expected change in Y when we observe a 1-unit increase in X. Here, “normalized” refers to the process of dividing the covariance of X and Y by the variance of X. Normalization refers to the process of scaling a variable so that it fits a specific scale. In this case, normalizing the covariance by the variance of X allows us to interpret the size of the slope in terms of the expect change in Y associated with a 1-unit increase in X.\n\n\n\n\n\n\nCalculate slope\n# Calculate the variance of X\n# Variance = sum((X_i - mean_X)^2) / (n - 1)\nvariance_x &lt;- sum((x - mean_x)^2) / (length(x) - 1)\n\n# Calculate the slope (beta) of Y on X\n# Beta_X = Cov(X, Y) / Var(X)\nbeta_x &lt;- covariance / variance_x\nlm(y ~ x)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    10.9280      -0.1879  \n\n\n\n\n\n\n\nWhat does the correlation coefficient tell you that slope doesn’t?\n\nConsistency of the relationship on bounded scale (-1 to 1)\n\nWhat does slope tell you that the correlation coefficient doesn’t?\n\nSubstantive importance (magnitude)\n\nGive an example of when you’d prefer each\n\nCorrelation: When comparing relationships on different scales\nSlope: When thinking about ROI\n\n\n\n\nHere’s a quick review of the concepts we just covered.\n\n\n\n\nWhat can with do with them?\n\nDescription: quantitative comparisons\nForecasting: sample population \\rightarrow out-of-sample\nCausal inference: correlation + research design\n\nSimple, but powerful\n\nNon-linearities, interactions, machine learning\n\n\nThese simple but powerful statistical tools perform much of the work in contemporary quantitative social science research, ranging from simple description, to forecasting, to causal inference. These simple linear correlation can be combined with squared terms to estimate non-linear relationships, iterations to estimate how a relationship between two variables is affected by values of other variables, or powerful research designs to make inferences about causal relationships.\n\n\n\n\nY_i = \\alpha + \\beta X_i + \\epsilon_i\n\n\n\nWhat is \\alpha?\nWhat is \\beta?\nWhat is X_i?\nWhat is \\epsilon_i?\n\n\n\n\nLinear regression models are often referred to as the “workhorse of the social sciences”. Whether it’s estimating difference in political opinions across demographic groups or calculating the causal effect of a randomized intervention, most analyses rely on linear regression. The most frequently used model is called “Ordinary Least Squares”. The most simple models consisten of an intercept (alpha), which estimates the average value of Y when the independent variable is at 0, the slope or coefficient (beta) for X, which captures the relationship between X and Y (the independent and dependent variables), and the error term (epsion) which captures the variance in Y that is not explained by the intercept and beta.\n\n\n\n\nEstimating model parameters\n\\hat{Y_i} = \\hat{\\alpha} + \\hat{\\beta} X_i\nCoefficient\n\\hat{\\beta} = \\Delta{\\hat{Y}} / \\Delta{X}\n\nLinear regression provides you with parameter estimates with which you can calculate the expected value of Y by adding together the terms on the right-hand-side of the equals sign in the model.\n\n\n\n\nWhat are residuals\n\\hat{\\epsilon_i} = Y_i - \\hat{Y_i}\nHow do we minimize them?\nSSR = \\sum_{i}^{N} \\hat{\\epsilon}_i^2\n\nOrdinary least squares works by estimating parameter values the minimize the residuals of the model, which are the difference between each value of Y and the value predicted by the model based on the model parameters. As with our measures of variance, this is done using the sum of squared residuals, to avoid positive and negative differences from cancelling out.\n\n\n\n\n\nCheck out this interactive visualization\n\nSee how the least squares method is used to calculate the slope of the line by drawing a line that minimizes squared errors\n\nCheck out this interactive visualization\n\nSee how changes in the intercept and slope represents negative and positive relationships across different distributions of Y\n\n\n\n\n\nY_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n\n\nHow does our interpretation of \\alpha change?\nHow does our interpretation of \\beta_1 change?\n\n\n\nMultiple regression works very similarly to simple linear regression, although interpretation becomes slightly more complicated. The intercept (alpha) is not the mean of Y when both independent variables (X1 and X2) are held at 0, while the value of B1 now captures the relationship between X1 and Y while X2 is held constant.\n\n\n\n\n\n\nMultiple Regression\n# Load required libraries\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nMultiple Regression\nlibrary(modelsummary)\n\n# Set seed for reproducibility of random numbers\nset.seed(123)\n\n# Create a data frame with 100 observations\ndata &lt;- data.frame(\n  X1 = rnorm(100, mean = 10, sd = 2),    # Generate 100 random numbers for the first independent variable\n  X2 = rnorm(100, mean = 5, sd = 2),     # Generate 100 random numbers for the second independent variable\n  Y = rnorm(100, mean = 10, sd = 2)      # Generate 100 random numbers for the dependent variable\n)\n\n# Introduce correlation between X1 and Y\ndata$Y &lt;- 0.5 * data$X1 + rnorm(100, mean = 10, sd = 1)\n\n\n\n\n\n\n\nMultiple Regression\n# lm() function fits linear models\nsimple_model &lt;- lm(Y ~ X1, data = data)\n\n# Run multiple regression model (Y ~ X1 + X2)\nmultiple_model &lt;- lm(Y ~ X1 + X2, data = data)\n\n# modelsummary() function provides a summary of the regression models\nmodelsummary(\n  list(\n    \"Simple Linear Regression\" = simple_model,\n    \"Multiple Regression\" = multiple_model\n  ),\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.',\n  output = \"dataframe\"\n)\n\n\n       part        term         statistic Simple Linear Regression\n1 estimates (Intercept) modelsummary_tmp1        10.219*** (0.594)\n2 estimates          X1 modelsummary_tmp1         0.475*** (0.057)\n3 estimates          X2 modelsummary_tmp1                         \n4       gof    Num.Obs.                                        100\n5       gof     R2 Adj.                                      0.405\n  Multiple Regression\n1   10.100*** (0.663)\n2    0.476*** (0.058)\n3       0.022 (0.055)\n4                 100\n5               0.400\n\n\n\n\n\nWhich of the following statements describe a correlation?\n\nMost professional data analysis took a statistics course in college.\nThe longer a person runs the more calories they burn.\nPeople who live to be 100 years old typically take vitamins.\nOlder people vote more than younger people."
  },
  {
    "objectID": "slides/lees-quant.html#resources",
    "href": "slides/lees-quant.html#resources",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "This activity draws on Chapter 2 in Data Analysis for Social Science: A Friendly and Practical Introduction (DSS)\n\ninteractive visualization: mean and sd\ninteractive visualization: correlation"
  },
  {
    "objectID": "slides/lees-quant.html#quantitative-analysis-and-comparisons",
    "href": "slides/lees-quant.html#quantitative-analysis-and-comparisons",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Comparisons are at the heart of quantitative analysis\nLots of bad analysis implies comparisons, but doesn’t actually make them\n\nEx. 10 things that extremely successful people do to be productive\nEx. 70% of participants reported an improvement\n\nCorrelation is the most basic tool for making comparisons with data\n\n\nMaking comparisons is one of the most essential parts in research, underpinning everything from simple descriptive studies to machine learning and impact evaluations. It is difficult to learn without comparison. For example, an article that identifies 10 habits of successful people might seek to provide guidance on what behaviors you can adopt to become more successful. However, we should also ask how common these behaviors are among people who are not successful. Similarly, an evaluation might point out that 70% of organizations that participated in a capacity-building intervention reported an improvement in their capacity over the course of the intervention. But we should also ask whether these improvements were larger than improvements experienced by similar organizations that didn’t participate. Correlation is the fundamental tool that allows us to answer these important questions."
  },
  {
    "objectID": "slides/lees-quant.html#correlations-necessary-components",
    "href": "slides/lees-quant.html#correlations-necessary-components",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "What do we need to calculate correlations?\n\nMeasures of central tendency\n\nMean\nProportion\n\nMeasures of spread\n\nVariance\nStandard deviation\n\n\n\nIn order to calculate correlations, we need to begin with quantities that we want to compare across groups. For example, we might want to calculate the mean, or the average, of a certain variable. We also need to measure how much variation, or spread, there is in these measures. Are the units in our sample very similar, or are their values spread our across a wide range?"
  },
  {
    "objectID": "slides/lees-quant.html#central-tendency-mean",
    "href": "slides/lees-quant.html#central-tendency-mean",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "\\mu_X = \\frac{1}{n} \\sum_{i}^{n} X_i\n\nmy_vector = rnorm(10, mean = 10, sd = 5)\n# Step 1: Sum the values\nsum_values &lt;- sum(my_vector)\n# Step 2: Count the number of elements\ncount_elements &lt;- length(my_vector)\n# Step 3: Calculate the mean\nmean_value &lt;- sum_values / count_elements\n\nprint(mean_value)\n\n[1] 9.499699\n\nmean(my_vector)\n\n[1] 9.499699\n\n\n\nWe begin by reviewing some basic statistical concepts and illustrating those concepts using R code. This will build your familiarity with R while providing a quick review of basic statistical concepts. Everyone knows how to calculate the mean of a variable. In R, there are multiple ways to do it. Here, I created a random variable with 10 values and an average value of 10. You can calculate the mean by hand or using the function ‘mean()’. Both approaches should give you the same value."
  },
  {
    "objectID": "slides/lees-quant.html#spread-variance",
    "href": "slides/lees-quant.html#spread-variance",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "\\sigma^2_X = \\frac{1}{N} \\sum_{i}^{N} (X_i - \\mu_X)^2\n\n\nWhat does the square in \\sigma^2 accomplish?\nWhat are the implications for interpretation?\n\nUnits\nDistribution\n\nEven with these basic measures, we’re already thinking about the distribution!\n\n\n\nYou’re also probably familiar with variance. Variance measures how spread out the values of a variable are around the mean. For the variance of X, we take the sum of the squared differences between each value of X and the mean value of X, and then divide by the number of observations. While squaring the differences is necessary to avoid positive and negative values cancelling out one another, it poses challenges for interpretation. Most importantly, the variance is not on the scale of the variable, so there is no intuitive way to interpret its value. The squaring of the differences also means that more weight is put on observations that are very far from the mean value. The next few slides illustrates this with simulated data."
  },
  {
    "objectID": "slides/lees-quant.html#spread-variance-1",
    "href": "slides/lees-quant.html#spread-variance-1",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Create a vector\n## Create vector, sort by size, and store var\nset.seed(123)\ndat = rnorm(10, mean = 10, sd = 5)\ndat = sort(dat)\no_var = var(dat)\nprint(dat)\n\n\n [1]  3.674694  6.565736  7.197622  7.771690  8.849113 10.352542 10.646439\n [8] 12.304581 17.793542 18.575325\n\n\n\nFirst, we create a vector of simulated data with a known mean and standard deviation. In statistics and computer science, a vector is a one-dimensional array, which is a data structure that stores elements of the same type in an ordered sequence."
  },
  {
    "objectID": "slides/lees-quant.html#spread-variance-2",
    "href": "slides/lees-quant.html#spread-variance-2",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Add a constant to a big number\n## Create new dataframe for big addition and store vector length\nb_dat = dat\nind = length(b_dat)\n\n## Add four to the largest number in the vector and calculate size of var increase\nb_dat[ind] = b_dat[ind] + 4\nb_var = var(b_dat)\nval = b_var - o_var\ncat(\"Variance increases by\", val )\n\n\nVariance increases by 8.890842\n\n\n\nSecond, we add 4 to the largest value of X and observe the change in the standard deviation"
  },
  {
    "objectID": "slides/lees-quant.html#spread-variance-3",
    "href": "slides/lees-quant.html#spread-variance-3",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Add a constant to a smaller number\n## Create new dataframe for small addition\ns_dat = dat\n\n## Add four to the smallest number in the vector and calculate size of var increase\ns_dat[ind-2] = s_dat[ind-2] + 4\ns_var = var(s_dat)\nval = s_var - o_var\ncat(\"Variance increases by\", val )\n\n\nVariance increases by 3.316847\n\n\n\nFinally, we add 4 to the smallest value of X and observe that the change in the standard deviation is much smaller. Again, this is because squaring the differences places more weight on extreme values very far from the mean"
  },
  {
    "objectID": "slides/lees-quant.html#spread-standard-deviation",
    "href": "slides/lees-quant.html#spread-standard-deviation",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "\\sigma_X = \\sqrt{\\frac{1}{N} \\sum_{i}^{N} (X_i - \\mu_X)^2}\n\n\nWhat does the \\sqrt{} accomplish?\nWhat are the implications for interpretation?\n\nExpressed in the same units as the observations\nHow far we expect each observation to be from the mean, on average\n\nIt is often desirable to report effect sizes as SDs\n\n\n\nAnother measure of spread is the standard deviation. The standard deviation is just the square-root of the variance, and tells us the average absolute difference between values of X and the mean of X. By taking the square root of the variance, we convert it back to the original units of the data, making it a more interpretable measure of dispersion. For example, if the data represents weights in kilograms, the standard deviation will also be in kilograms. A larger standard deviation indicates that the data points are more spread out from the mean, and a smaller standard deviation indicates that they are closer to the mean. In many research contexts, especially in the social sciences and medicine, it’s common to report effect sizes relative to the standard deviation. This is because it provides a scale of variability that can be used to compare the magnitude of an effect across different studies or different units of measurement. For example, an effect size of 0.2 standard deviations is often considered a small effect, regardless of the actual units of the data."
  },
  {
    "objectID": "slides/lees-quant.html#spread-standard-deviation-1",
    "href": "slides/lees-quant.html#spread-standard-deviation-1",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Check out this interactive visualization\n\nObserve how changes in the standard deviation affect the shape of the distribution\n\n\n\nIn a new tab, open this interactive visualization of how changes in the standard deviation affect the shape of the distribution. Follow the instructions to see what increases in the spead of a variable looks like."
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation",
    "href": "slides/lees-quant.html#measures-of-correlation",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Covariance \\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y})\n\nProduct of the deviations\nRange: unbounded\n\nCorrelation coefficient \\text{Cor}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\nCovariance normalized by product of SDs\nRange: -1 to 1\n\n\n\n\nNow that we have reviewed the fundamental components of correlation, we can turn to the three primary ways of measuring correlation. Correlations rely on two variables, typically denoted as X and Y.\nCovariance measures the joint variability of two variables. As you can see from the equation, the covariance is calculated by summing the product of the deviations of X and Y and dividing this value by the number of observations. Deviations are calculated by taking each value of X and Y and subtracting the variable’s mean value. Covariance can tell us if increases in one variable generally correspond to increases in another (positive covariance), or if increases in one correspond to decreases in the other (negative covariance). However, the scale of the covariance is not standardized, making it difficult to interpret. , In fact, the results are unbounded, meaning that there is no minimum or maximum value for covariance.\nNext, we turn to the correlation coefficient, which normalizes the covariance by the product of the variables’ standard deviations, forcing them to take a bounded range that can tell us the strength and direction of correlation. The Correlation Coefficient tells us the strength and direction of a linear relationship between two variables. A correlation of 1 means there is a perfect positive linear relationship, -1 means there is a perfect negative linear relationship, and 0 implies no linear relationship at all.\nIn summary, while covariance provides a first glance at the direction of a relationship, the correlation coefficient helps us understand both the strength and direction in a standardized format, making it easier to interpret and compare across different studies or datasets."
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation-1",
    "href": "slides/lees-quant.html#measures-of-correlation-1",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Calculate covariance\nset.seed(1234)\n# Generate two vectors of 10 random numbers each from a normal distribution\nx &lt;- rnorm(10, mean = 10, sd = 5)\ny &lt;- rnorm(10, mean = 10, sd = 5)\n# Calculate the means of both vectors\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\n# Calculate the covariance between the two vectors\ncovariance &lt;- sum((x - mean_x) * (y - mean_y)) / (length(x) - 1)\ncov(x,y)\n\n\n[1] -4.657457"
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation-2",
    "href": "slides/lees-quant.html#measures-of-correlation-2",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Calculate correlation\n# Calculate the standard deviations of both vectors\nsd_x &lt;- sd(x)\nsd_y &lt;- sd(y)\n\n# Calculate the correlation coefficient between the two vectors\ncorrelation &lt;- covariance / (sd_x * sd_y)\ncor(x,y)\n\n\n[1] -0.1752832"
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation-3",
    "href": "slides/lees-quant.html#measures-of-correlation-3",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Slope \\beta_X = \\frac{\\text{Cov}(X, Y)}{\\sigma^2_X}\n\nCovariance normalized by variance\nExpected change in Y with 1-unit change in X\n\n\n\n\nFinally, we can calculate the slope of a regression line, which normalized the covariance by the variance and tells us the expected change in Y when we observe a 1-unit increase in X. Here, “normalized” refers to the process of dividing the covariance of X and Y by the variance of X. Normalization refers to the process of scaling a variable so that it fits a specific scale. In this case, normalizing the covariance by the variance of X allows us to interpret the size of the slope in terms of the expect change in Y associated with a 1-unit increase in X."
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation-4",
    "href": "slides/lees-quant.html#measures-of-correlation-4",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Calculate slope\n# Calculate the variance of X\n# Variance = sum((X_i - mean_X)^2) / (n - 1)\nvariance_x &lt;- sum((x - mean_x)^2) / (length(x) - 1)\n\n# Calculate the slope (beta) of Y on X\n# Beta_X = Cov(X, Y) / Var(X)\nbeta_x &lt;- covariance / variance_x\nlm(y ~ x)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    10.9280      -0.1879"
  },
  {
    "objectID": "slides/lees-quant.html#measures-of-correlation-5",
    "href": "slides/lees-quant.html#measures-of-correlation-5",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "What does the correlation coefficient tell you that slope doesn’t?\n\nConsistency of the relationship on bounded scale (-1 to 1)\n\nWhat does slope tell you that the correlation coefficient doesn’t?\n\nSubstantive importance (magnitude)\n\nGive an example of when you’d prefer each\n\nCorrelation: When comparing relationships on different scales\nSlope: When thinking about ROI\n\n\n\n\nHere’s a quick review of the concepts we just covered."
  },
  {
    "objectID": "slides/lees-quant.html#correlation",
    "href": "slides/lees-quant.html#correlation",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "What can with do with them?\n\nDescription: quantitative comparisons\nForecasting: sample population \\rightarrow out-of-sample\nCausal inference: correlation + research design\n\nSimple, but powerful\n\nNon-linearities, interactions, machine learning\n\n\nThese simple but powerful statistical tools perform much of the work in contemporary quantitative social science research, ranging from simple description, to forecasting, to causal inference. These simple linear correlation can be combined with squared terms to estimate non-linear relationships, iterations to estimate how a relationship between two variables is affected by values of other variables, or powerful research designs to make inferences about causal relationships."
  },
  {
    "objectID": "slides/lees-quant.html#linear-regression-model",
    "href": "slides/lees-quant.html#linear-regression-model",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Y_i = \\alpha + \\beta X_i + \\epsilon_i\n\n\n\nWhat is \\alpha?\nWhat is \\beta?\nWhat is X_i?\nWhat is \\epsilon_i?\n\n\n\n\nLinear regression models are often referred to as the “workhorse of the social sciences”. Whether it’s estimating difference in political opinions across demographic groups or calculating the causal effect of a randomized intervention, most analyses rely on linear regression. The most frequently used model is called “Ordinary Least Squares”. The most simple models consisten of an intercept (alpha), which estimates the average value of Y when the independent variable is at 0, the slope or coefficient (beta) for X, which captures the relationship between X and Y (the independent and dependent variables), and the error term (epsion) which captures the variance in Y that is not explained by the intercept and beta."
  },
  {
    "objectID": "slides/lees-quant.html#linear-regression-model-1",
    "href": "slides/lees-quant.html#linear-regression-model-1",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Estimating model parameters\n\\hat{Y_i} = \\hat{\\alpha} + \\hat{\\beta} X_i\nCoefficient\n\\hat{\\beta} = \\Delta{\\hat{Y}} / \\Delta{X}\n\nLinear regression provides you with parameter estimates with which you can calculate the expected value of Y by adding together the terms on the right-hand-side of the equals sign in the model."
  },
  {
    "objectID": "slides/lees-quant.html#minimizing-the-residuals",
    "href": "slides/lees-quant.html#minimizing-the-residuals",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "What are residuals\n\\hat{\\epsilon_i} = Y_i - \\hat{Y_i}\nHow do we minimize them?\nSSR = \\sum_{i}^{N} \\hat{\\epsilon}_i^2\n\nOrdinary least squares works by estimating parameter values the minimize the residuals of the model, which are the difference between each value of Y and the value predicted by the model based on the model parameters. As with our measures of variance, this is done using the sum of squared residuals, to avoid positive and negative differences from cancelling out."
  },
  {
    "objectID": "slides/lees-quant.html#linear-regression-model-2",
    "href": "slides/lees-quant.html#linear-regression-model-2",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Check out this interactive visualization\n\nSee how the least squares method is used to calculate the slope of the line by drawing a line that minimizes squared errors\n\nCheck out this interactive visualization\n\nSee how changes in the intercept and slope represents negative and positive relationships across different distributions of Y"
  },
  {
    "objectID": "slides/lees-quant.html#multiple-regression",
    "href": "slides/lees-quant.html#multiple-regression",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Y_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n\n\nHow does our interpretation of \\alpha change?\nHow does our interpretation of \\beta_1 change?\n\n\n\nMultiple regression works very similarly to simple linear regression, although interpretation becomes slightly more complicated. The intercept (alpha) is not the mean of Y when both independent variables (X1 and X2) are held at 0, while the value of B1 now captures the relationship between X1 and Y while X2 is held constant."
  },
  {
    "objectID": "slides/lees-quant.html#multiple-regression-1",
    "href": "slides/lees-quant.html#multiple-regression-1",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Multiple Regression\n# Load required libraries\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nMultiple Regression\nlibrary(modelsummary)\n\n# Set seed for reproducibility of random numbers\nset.seed(123)\n\n# Create a data frame with 100 observations\ndata &lt;- data.frame(\n  X1 = rnorm(100, mean = 10, sd = 2),    # Generate 100 random numbers for the first independent variable\n  X2 = rnorm(100, mean = 5, sd = 2),     # Generate 100 random numbers for the second independent variable\n  Y = rnorm(100, mean = 10, sd = 2)      # Generate 100 random numbers for the dependent variable\n)\n\n# Introduce correlation between X1 and Y\ndata$Y &lt;- 0.5 * data$X1 + rnorm(100, mean = 10, sd = 1)"
  },
  {
    "objectID": "slides/lees-quant.html#multiple-regression-2",
    "href": "slides/lees-quant.html#multiple-regression-2",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Multiple Regression\n# lm() function fits linear models\nsimple_model &lt;- lm(Y ~ X1, data = data)\n\n# Run multiple regression model (Y ~ X1 + X2)\nmultiple_model &lt;- lm(Y ~ X1 + X2, data = data)\n\n# modelsummary() function provides a summary of the regression models\nmodelsummary(\n  list(\n    \"Simple Linear Regression\" = simple_model,\n    \"Multiple Regression\" = multiple_model\n  ),\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.',\n  output = \"dataframe\"\n)\n\n\n       part        term         statistic Simple Linear Regression\n1 estimates (Intercept) modelsummary_tmp1        10.219*** (0.594)\n2 estimates          X1 modelsummary_tmp1         0.475*** (0.057)\n3 estimates          X2 modelsummary_tmp1                         \n4       gof    Num.Obs.                                        100\n5       gof     R2 Adj.                                      0.405\n  Multiple Regression\n1   10.100*** (0.663)\n2    0.476*** (0.058)\n3       0.022 (0.055)\n4                 100\n5               0.400"
  },
  {
    "objectID": "slides/lees-quant.html#quiz-questions",
    "href": "slides/lees-quant.html#quiz-questions",
    "title": "Quantitative Analysis",
    "section": "",
    "text": "Which of the following statements describe a correlation?\n\nMost professional data analysis took a statistics course in college.\nThe longer a person runs the more calories they burn.\nPeople who live to be 100 years old typically take vitamins.\nOlder people vote more than younger people."
  },
  {
    "objectID": "slides/lees-quant.html#schools-of-thought",
    "href": "slides/lees-quant.html#schools-of-thought",
    "title": "Quantitative Analysis",
    "section": "Schools of Thought",
    "text": "Schools of Thought\n“We think of a cause as something that makes a difference, and the difference it makes must be a difference from what would have happened without it.” (Lewis, 1973)\n\nPotential outcomes and counterfactuals (Econ)\nDAGs and do-calculus (CS)\nManipulability (Philosophy)\n\n\nWhile simple correlations can answer many important questions about the world, on their own, they cannot tell us anything about whether the relationship between X and Y is causal. We define causal relationships as a dependency whereby intervening on or manipulating the value of X while holding all other variables constant will result in a change in the value of Y. Causality is a central topic in philosophy of science, and there are many schools of thought around it. In social science research, most researchers focus on the Potential Outcomes framework."
  },
  {
    "objectID": "slides/lees-quant.html#causality-why-bother",
    "href": "slides/lees-quant.html#causality-why-bother",
    "title": "Quantitative Analysis",
    "section": "Causality: Why bother?",
    "text": "Causality: Why bother?\n\nUnderstanding cause and effect is how we change things in the real world\nCausal inference separates good evaluations from bad\n\nPolicy change\nDevelopment intervention\n\nCausal identification is not binary\n\nIt’s harder for some policies and interventions than others\nVariety of tools that can help us rule out different threats to inference\n\n\n\nAs we’ll learn, it can be extremely difficult to estimate causal relationships in the social world. However, the ability to make positive change in the world requires an understanding cause and effect. However, most program evaluations cannot provide evidence of a causal impact and sometimes reliable causal inference is difficult or impossible."
  },
  {
    "objectID": "slides/lees-quant.html#causality-what-makes-it-hard",
    "href": "slides/lees-quant.html#causality-what-makes-it-hard",
    "title": "Quantitative Analysis",
    "section": "Causality: What makes it hard?",
    "text": "Causality: What makes it hard?\nFundamental Problem of Causal Inference\nY_i = \n\\begin{cases} \nY_i(1) & \\text{if } D_i = 1 \\text{ (treatment group)} \\\\\nY_i(0) & \\text{if } D_i = 0 \\text{ (control group)}\n\\end{cases}\n\nWe only observe any given unit in one treatment status at any one time so we can never directly observe the causal effect of a treatment on a unit.\n\n\nWhile estimating the correlation between two variables is straightforward, understanding whether this correlation captures a causal relationship is much more difficult. This is due to the “fundamental problem of causal inference”, which arises from our inability to observe counterfactuals. In causal inference, a counterfactual refers to the hypothetical outcome that would have occurred if a subject, who was exposed to a particular treatment or condition, had instead been exposed to a different treatment or no treatment at all. Counterfactuals are a fundamental concept in establishing causal relationships because they help to model what would have happened in an alternate scenario. We always think of causal relationships in terms of a counterfactual. Thinking about a causal relationship between a binary treatment variable D and an outcome Y, for any unit in our sample, we want to observe the value of Y in simultaneous worlds in which the unit does and does not receive the treatment D. For example, if we wanted to measure the effect of a malaria vaccine on the number of days a person misses work in a month, we want to observe simultaneous counterfactual worlds in which that person both does and does not receive the vaccine. Obviously, this is impossible. However, rigorous research designs allow us to estimate this relationship under certain conditions."
  },
  {
    "objectID": "slides/lees-quant.html#potential-outcomes-and-counterfactuals",
    "href": "slides/lees-quant.html#potential-outcomes-and-counterfactuals",
    "title": "Quantitative Analysis",
    "section": "Potential Outcomes and Counterfactuals",
    "text": "Potential Outcomes and Counterfactuals\nTreatment Effect for individual i\n\nTE_i = Y_i(1) - Y_i(0)\n\nAverage Treatment Effect (ATE)\n\nATE = \\frac{1}{N} \\sum_{i=1}^{N} TE_i\n\n\nObserving these simultaneous counterfactual worlds for each unit in our sample would allow us to easily estimate the average treatment effect for units in our sample. However, this is impossible."
  },
  {
    "objectID": "slides/lees-quant.html#whats-the-solution",
    "href": "slides/lees-quant.html#whats-the-solution",
    "title": "Quantitative Analysis",
    "section": "What’s the solution?",
    "text": "What’s the solution?\nEstimating counterfactuals\n\nUnits in the control group serve as a stand-in for the counter-factual of the treatment group\n\n\\widehat{ATE} = \\overline{Y}_{treatment\\_group} - \\overline{Y}_{control\\_group}\n\nWhile we can never directly observe a counterfactual, we can try to estimate it by using a credible research design. Causal inference methods attempt to do this by minimizing the probability of systematic differences between units that do and do not receive a treatment."
  },
  {
    "objectID": "slides/lees-quant.html#whats-complicated-about-this",
    "href": "slides/lees-quant.html#whats-complicated-about-this",
    "title": "Quantitative Analysis",
    "section": "What’s complicated about this?",
    "text": "What’s complicated about this?\n\n\n“Only valid when when the treatment and control group are comparable with respect to all the variables that might affect the outcome other than the treatment variable itself.”\n“We must find or create a situation in which the treated observations and the untreated observations are similar with respect to all the variables that might affect the outcome”\n“By randomly assigning treatment, we ensure that treatment and control groups are, on average, identical to each other in all observed and unobserved pre-treatment characteristics”\n\n\n\nUnits that have been exposed to the treatment can only serve as a valid counterfactual under very specific conditions. Under random assignment, the statistically most likely outcome is that treatment and control groups will not have any systematic differences that can confound our ability to estimate the treatment effect."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\nIn the real world, differences in units’ exposure to X is often driven by important differences that also affect those units’ values of Y. This is called “confounding”. When confounding is present, our estimates of the causal effect of X on Y will be biased, potentially creating the illusion of a causal relationship that doesn’t actually exist. Consider this example, where the NYT reported on a study that found a correlation between opera attendance and longevity. Here, the journalists inferred that this correlation was causal, meaning that by changing whether or not someone goes to the opera, you can change how long they live. However, there are many factors that confound this relationship, including income, which is correlated with both opera attendance and lifespan."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-1",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-1",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\nMany very smart people do not understand the fundamental problem of causal inference, leading them to underestimate the difficulty of estimating causal relationships without randomization. This can lead to fundamentally wrong conclusions about the impact that certain policies or interventions will have. For example, this Tweet captures an individual arguing that large sample sizes (erroneously referred to as “numbers of n’s”) allow you to interpret correlations as causal. However, this is extremely incorrect. Larger samples are subject to the exact same sources of confounding as small samples. Thinking back to the previous slide, there are still systematic differences between people that go to the opera and those who don’t. No matter how large the sample size, it would be wrong to assume that giving out opera tickets will prolong people’s lives. Understanding the challenges of causal inference and being able to communicate these challenges to others can be a huge advantage for researchers that want to help others learn about the world."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-2",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-2",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\nExample: What is the effect of class size on test scores\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\n\nConsider the challenge of estimating the effect of smaller class sizes on test scores. We want to know whether decreasing class sizes will cause an increase in the average test scores of students. We might start by looking at observational data on class size and test scores across a given district or country."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-3",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-3",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nHowever, even if we find a negative correlation between larger class sizes and higher test scores, we cannot assume that this relationship is causal. If there are any unobserved factors that are correlated with both class size and score, this will confound our estimates."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-4",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-4",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nThis would create the appearance of a causal relationship, even if one does not actually exist."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-5",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-5",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nWe might decide to collect data on additional characteristics of students and try to “control” for these potential confounders by including them in a multiple regression model. For example, it might be that wealthier students tend to be in schools with smaller class sizes. If you can control for parental wealth, we can remove this potential bias in your estimates."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-6",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-6",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nHowever, adding the wrong control variables to your model can also create bias. When your dependent and independent variables have an effect on another variable in your model, this creates collider bias, which can cause the illusion of a causal relationship between X and Y by adding a control. For example, smaller class sizes might cause teachers to pay more attention to students. Higher test scores also might cause teachers to pay more attention to students. Controlling for the amount of attention students get from teachers would create the illusion of a relationship between class size and test scores even if one does not exist."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-7",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-7",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nIn reality, we can never control for all potential confounders, and we can never be certain that potential confounders we can control for are not colliders. If this figure captures every potential confounder, controlling for Year, Location, and Wealth would allow us to estimate the true causal effect of class sizes on test scores."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-8",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-8",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nHowever, in this figure, note the change in the direction of the arrow connecting Score and Attention. Now, Score has a causal effect on Attention. if we added a control for Attention, that would introduce collider bias."
  },
  {
    "objectID": "slides/lees-quant.html#cant-we-just-observe-and-compare-9",
    "href": "slides/lees-quant.html#cant-we-just-observe-and-compare-9",
    "title": "Quantitative Analysis",
    "section": "Can’t we just observe and compare?",
    "text": "Can’t we just observe and compare?\n\n\n\n\n\n\n\n\n\n\nAnd in this figure, no strategy will allow us to estimate a causal relationship because we don’t have data on U1 (hence it’s grey color)."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time",
    "href": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just observe how units change over time?",
    "text": "Why can’t we just observe how units change over time?\n\n\nShow code\nlibrary(ggplot2)\n\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.3, 1.7,NA)\nTreatment = c(\"Control\", \"Control\",\"Control\",\"Control\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  scale_linetype_manual(values=c(\"solid\")) +\n  xlim(0,3) + \n  scale_y_continuous(limits = c(1,1.85), breaks = seq(1, 1.85, by = .1)) + \n  scale_color_manual(values = c(\"blue\") ) +\n  theme(legend.position = \"none\", text = element_text(size=20)) \n\n\n\n\n\n\n\n\n\n\nA natural response is to wonder about whether we can allow units to serve as their own control group over time. Imagine that we collect data on units in Year 1 before a treatment is administered and again in Year 2 after a treatment is administered."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-1",
    "href": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-1",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just observe how units change over time?",
    "text": "Why can’t we just observe how units change over time?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(0.9, 1.3, 1.7, 2.1)\nTreatment = c(\"Control\", \"Control\",\"Control\",\"Control\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  scale_y_continuous(breaks = seq(1, 1.85, by = .1)) + \n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\") ) +\n  coord_cartesian(ylim = c(1, 1.85), clip = \"on\") +\n  theme(legend.position = \"none\", text = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\nHere, we cannot know how the units in our sample would have changed in the absence of the intervention. In this figure, we see that the units were increasing at the same rate in the two years before they received the intervention, and likely would have increased the same amount without the intervention."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.2, 1.4, NA, \n            NA, 1.3, 1.7, NA)\nTreatment = c(\"Non-treated\", \"Non-treated\",\"Non-treated\",\"Non-treated\", \n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  scale_y_continuous(limits = c(1,1.85), breaks = seq(1, 1.85, by = .1)) + \n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"red\", \"blue\") ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\nImagine we also collect data on a group of units that were not exposed to the treatment. While treatment assignment was not random, we might think that we can compare how treated units and non-treated units change in order to estimate the effect of the treatment."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-1",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-1",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(1, 1.2, 1.4, 1.6, \n            0.9, 1.3, 1.7, 2.1)\nTreatment = c(\"Non-treated\", \"Non-treated\",\"Non-treated\",\"Non-treated\", \n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  scale_y_continuous(breaks = seq(1, 1.85, by = .1)) + \n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"red\", \"blue\") ) +\n  coord_cartesian(ylim = c(1, 1.85), clip = \"on\") +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20))\n\n\n\n\n\n\n\n\n\n\nHowever, because the treatment was not randomly assigned, treated and non-treated units may be systematically different, leading them to have different trajectors before the treatment was administered. When we see a longer period of time, it’s clear that both groups are changing at the same rate that they were prior to the treatment group being exposed to the treatment."
  },
  {
    "objectID": "slides/lees-quant.html#causal-inference-tools",
    "href": "slides/lees-quant.html#causal-inference-tools",
    "title": "Quantitative Analysis",
    "section": "Causal Inference Tools",
    "text": "Causal Inference Tools\n\nRandomized experiments\n\nGold-standard\nField and survey\n\nObservational data\n\nNatural experiments\nDifference-in-Differences\nMatching, Synthetic Control\n\n\n\nThere are several ways that we can estimate causal relationships in the real world. Randomized experiments provide the most reliable way to accomplish this. Under random assignment, the statistically most likely outcome is that treatment and control groups will not have any systematic differences that can confound our ability to estimate the treatment effect."
  },
  {
    "objectID": "slides/lees-quant.html#estimating-causal-effects-with-randomized-experiments",
    "href": "slides/lees-quant.html#estimating-causal-effects-with-randomized-experiments",
    "title": "Quantitative Analysis",
    "section": "Estimating Causal Effects with Randomized Experiments",
    "text": "Estimating Causal Effects with Randomized Experiments\n\nRandomly assigning exposure to the treatment ensures that treatment and control groups are, on average, identical on observed and unobserved pre-treatment characteristics\nCheck out this interactive visualization\n\nObserve how random assignment produces treatment and control groups with a similar composition, especially at larger sample sizes\n\n\n\nAs a reminder, randomly assigning exposure to the treatment ensures that treatment and control groups are, on average, identical on observed and unobserved pre-treatment characteristics. Check out this interactive visualization to see how random assignment produces treatment and control groups with a similar composition, especially at larger sample sizes."
  },
  {
    "objectID": "slides/lees-quant.html#estimating-causal-effects-with-randomized-experiments-1",
    "href": "slides/lees-quant.html#estimating-causal-effects-with-randomized-experiments-1",
    "title": "Quantitative Analysis",
    "section": "Estimating Causal Effects with Randomized Experiments",
    "text": "Estimating Causal Effects with Randomized Experiments\n\nBest way to estimate causal effect of one variable on another variable\nRandomized experiments are used across a wide variety of disciplines, from medicine and pharmacology to marketing and political science\nWe can randomize different types of treatments\n\nImpact evaluations randomly assign an intervention\nSurvey experiments randomize exposure to images and text\nClinical trials randomly assign randomly assign medical treatments\n\n\n\nThe most straightforward and reliable method for identifying the causal effect of one variable on another is through randomized experiments. Randomized experiments offer us a powerful tool because they allow us to rule-out bias confounding variables, providing a clearer picture of cause and effect relationships. This method is applied across a broad range of fields, and is the foundation for inference in many social and physical science disciplines. For instance, when conducting an impact evaluation, interventions are randomly assigned to different groups to assess their effects. This could be educational programs, economic policies, or health-related initiatives. In survey research, we often randomize exposure to specific images or text to understand how different types of content influence opinions or behaviors. This approach is particularly valuable in areas like marketing or political science, where the impact of media and messaging is critical. Clinical trials are also a form of randomized experiment that’s essential in pharmacology and medical research. Here, medical treatments are assigned randomly to participants to rigorously test their efficacy and safety. By employing randomized experiments, we can draw accurate conclusions about causality, significantly improving the ability of research to inform the design policies and interventions."
  },
  {
    "objectID": "slides/lees-quant.html#limitations-and-ethical-considerations",
    "href": "slides/lees-quant.html#limitations-and-ethical-considerations",
    "title": "Quantitative Analysis",
    "section": "Limitations and Ethical Considerations",
    "text": "Limitations and Ethical Considerations\n\nIEs are extremely resource-intensive with high opportunity costs\n\nLarge financial costs and long-evaluation periods\nRequire deep technical expertise and implementation experience\n\nFindings may not be generalizable to other contexts or scales\n\nResults may not be applicable to different contexts or populations\nChallenges in scaling up successful interventions\n\nEthical Concerns\n\nControl group should receive the status quo\nPossible resistance from communities or organizations\nImpact on vulnerable populations must be considered explicitly and early\n\n\n\nWhile IEs are extremely powerful, they have some important limitations and require serious engagement with potential ethical concerns. Firstly, impact evaluations can be extremely resource-intensive. They require substantial financial investment, extensive data collection, and deep technical expertise and implementation experience. This means that resources might be diverted away from other interventions and research. This can also make them impractical for smaller organizations or interventions with limited budgets. Another limitation is the generalizability of the results. What works well in one specific context or population might not necessarily be effective elsewhere. For this reason, it is necessary to include partners with deep contextual knowledge that can tailor interventions to the context. Relatedly, interventions that are extremely effective with a small group of people might be completely ineffective at a larger scale. Many evaluations have found that wildly successful interventions have no impact when moving from treating a very small proportion of the population to a larger proportion of the population or when moving from or from implementation by an NGO to implementation by a national government agency. Understanding these limitations and ethical considerations is crucial for conducting responsible and effective impact evaluations and for making responsible policy recommendations based on findings. From an ethical standpoint, impact evaluations have several unique features that must be considered. First, the status quo should be maintained for control groups. In other words, it is not acceptable to deny the control group access to something that they had before the evaluation period as part of the research. It is OK to withhold new benefits, but it is never OK to revoke or withdrawal existing benefits. Furthermore, there might be resistance from communities or organizations that feel threatened by the changes proposed through the intervention or are upset about how the treatment is being assigned. Finally, when using random assignment, it is important to consider the potential impact on vulnerable populations. Extra effort must often be made to ensure that random assignment does not accidentally exclude specific populations by virtue of their geographic location or differences in social integration. For example, when an intervention is randomly assigned at the village level, minority groups may be less likely to benefit if they are less socially integrated or actively excluded by local institutions. For more general discussion of ethics in research, including the role of ethical review boards, see the Overview for this module."
  },
  {
    "objectID": "slides/lees-quant.html#dealing-with-small-sample-sizes",
    "href": "slides/lees-quant.html#dealing-with-small-sample-sizes",
    "title": "Quantitative Analysis",
    "section": "Dealing with Small Sample Sizes",
    "text": "Dealing with Small Sample Sizes\n\nBudget limitations often lead to small sample sizes\nSmall samples reduce statistical precision and may result in groups that are less comparable\nThere are tools to ensure balance on observed characteristics\n\nRe-randomization\nBlocking\nNon-bipartite matching\n\n\n\nOne common challenge for IEs is having a small sample size. Because IEs are expensive, researchers often need to limit the number of subjects (whether they’re individuals, communities, or organizations) involved in an evaluation. Small sample sizes can reduce the statistical precision of our evaluation, and may result in an evaluation that is unable to distinguish a small impact from no impact. This severely limits our ability to learn from the research. Similarly, randomizing assignment among a small sample increases the risk that our treatment and control groups will be systematically different on pre-treatment characteristics, which can produce misleading results. Think back to the interactive visualization on the first slide of this tutorial.\nHowever, there are some advanced methods to reduce the risks associated with small sample sizes in randomized evaluations. However, all of these methods require researchers to have pre-treatment baseline data about the subjects among which assignment is being randomized.\nFirst, we have Re-randomization. This technique involves randomizing the treatment assignment multiple times and selecting the assignment that returns the best balance across assignment, as measured by baseline data on subject characteristics. It’s particularly useful when you’re working with a limited number of participants but still want to ensure a fair distribution of characteristics.\nNext is Blocking. In blocking, we group participants with similar characteristics together before randomly assigning treatments within these blocks. This method reduces variability between the groups, improving the efficiency and balance of our comparisons, which is crucial in studies with small sample sizes.\nWe also have Non-bipartite Matching. This approach matches participants in treatment and control groups based on similar characteristics, allowing for more flexibility and potentially better balance across a large range of pre-treatment characteristics. However, this method can be complex and computationally intensive.\nIt’s important to note that these methods—despite being quite sophisticated—primarily ensure balance on observed characteristics. This means they help us control for known variables, but there may still be unobserved variables that could influence the outcomes.\nIn summary, while these methods enhance the reliability of our results within the constraints of small sample sizes, they do not completely eliminate the limitations associated with unobserved factors. It’s essential to interpret results with an understanding of both the strengths and the limitations of these statistical techniques.”"
  },
  {
    "objectID": "slides/lees-quant.html#validity",
    "href": "slides/lees-quant.html#validity",
    "title": "Quantitative Analysis",
    "section": "Validity",
    "text": "Validity\n\n\nInternal validity\nExternal validity\nWhat are the trade-offs between experiments and observational studies?\n\nExperiments have more internal validity\nBut… they often have synthetic treatments, convenience samples\n\nWhere are these studies used in the real-world?"
  },
  {
    "objectID": "slides/lees-quant.html#additional-resources",
    "href": "slides/lees-quant.html#additional-resources",
    "title": "Quantitative Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nFor a deeper-dive on impact evaluations, see these teaching resources created by J-PAL: https://www.povertyactionlab.org/resource/teaching-resources-randomized-evaluations"
  },
  {
    "objectID": "slides/lees-quant.html#causality-without-randomization",
    "href": "slides/lees-quant.html#causality-without-randomization",
    "title": "Quantitative Analysis",
    "section": "Causality without Randomization",
    "text": "Causality without Randomization\n\n\nYou must control for…\n\neverything (observed and unobserved) that affects both the treatment variable and the outcome variable\n\nYou must not control for…\n\nanything that is affected by both the treatment variable and the outcome variable\n\nYou need to think carefully before controlling for…\n\nanything that is affected by the treatment variable that also affects the outcome variable\n\n\n\n\nThere are many contexts in which randomized controlled trials are not feasible. Fortunately, causal inference is also possible without random assignment. However, these tools require very precise conditions and can be difficult to implement without significant background in causal inference methods. For this reason, we provide a high-level overview so that researchers can seek-out more information on methods that seem relevant to their work.\nCausal inference with observational data (situations where researchers did not actively manipulate how the treatment was assigned) has been a rapidly growing area of research in the social sciences since the 1990s. There are several specific tools used in this field, each of which rely on specific conditions in the world, many of which are fare. For example, many researchers look for situations in the world that resemble experiments due to naturally occuring random assignment, such as lotteries or discontinuities in exposure, variation in exposure to treatment over time, or creating balance on observable characteristics through matching and weighting procedures. While these methods all have weaknesses, they are essential causal inference tools for social science research."
  },
  {
    "objectID": "slides/lees-quant.html#identification-strategy",
    "href": "slides/lees-quant.html#identification-strategy",
    "title": "Quantitative Analysis",
    "section": "Identification strategy",
    "text": "Identification strategy\nIn the real world, there are always threats to inference that we can’t measure/observe or understand well enough to adjust for\n\nA research design that allows us to isolate a causal effect from observational data\nApproximates an experiment by ensuring that the treatment and control group are similar at baseline\nThese strategies rely on assumptions that we can attempt to validate\n\n\nIn the real world, the main challenge with observational data is the multitude of threats to valid inference—factors we can’t measure, observe, or perhaps even understand sufficiently to adjust for. These threats can bias our results and lead to incorrect conclusions. To tackle this, researchers employ specific research designs that allow us to approximate the conditions of an experiment. These designs ensure that the treatment and control groups are as similar as possible at baseline. By doing this, we aim to isolate the variable of interest and measure its true effect on the outcome. However, it’s important to recognize that these strategies are not foolproof. They rely heavily on assumptions that we need to critically evaluate and attempt to validate."
  },
  {
    "objectID": "slides/lees-quant.html#holy-trinity-of-causal-inference",
    "href": "slides/lees-quant.html#holy-trinity-of-causal-inference",
    "title": "Quantitative Analysis",
    "section": "Holy Trinity of Causal Inference",
    "text": "Holy Trinity of Causal Inference\n\n\n\nDifference-in-Differences\nRegression Discontinuity\nInstrumental Variables\n\n\nThe ‘Holy Trinity of Causal Inference’ is a term we use to describe three powerful research designs that help us establish causality in economics and social sciences when randomized controlled trials are not possible. These methods are Difference-in-Differences, Regression Discontinuity, and Instrumental Variables. Each of these methods relies on specific assumptions and conditions to validly infer causality. Their strength lies in their ability to provide insights into causal relationships even when experiments are impractical or unethical. It’s crucial, however, to ensure that the assumptions behind each method are met in the context of your specific study.” In this tutorial, we will focus on DiD because it is the most commonly used tool and the easiest to implement. We briefly discuss regression-discontinuity. We do not discuss Instrumental variables, because they are rarely applicable in applied research."
  },
  {
    "objectID": "slides/lees-quant.html#identification-strategy-1",
    "href": "slides/lees-quant.html#identification-strategy-1",
    "title": "Quantitative Analysis",
    "section": "Identification strategy",
    "text": "Identification strategy\nIn the real world, there are always threats to inference that we can’t measure/observe or understand well enough to adjust for\n\nA research design that allows us to isolate a causal effect from observational data\nApproximates an experiment by ensuring that the treatment and control group are similar at baseline\nThese strategies rely on assumptions that we can attempt to validate"
  },
  {
    "objectID": "slides/lees-quant.html#difference-in-differences",
    "href": "slides/lees-quant.html#difference-in-differences",
    "title": "Quantitative Analysis",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\n\n\nY_{it} = \\alpha + \\beta_1 \\text{Treatment}_i + \\beta_2 \\text{Post}_t + \\gamma (\\text{Treatment}_i \\times \\text{Post}_t) + \\epsilon_{it}\n\n\\gamma (\\text{Treatment}_i \\times \\text{Post}_t)\nAssumes measurement at two points in time\n\n\nWe begin with Difference-in-Differences (DiD). This method is used when we have data from both before and after an intervention, for both a treatment group and a control group. By comparing the changes in outcomes over time between these groups, DiD helps us isolate the effect of the intervention from other factors that are changing over time. The Difference-in-Differences (DiD) approach is a quasi-experimental technique used in econometrics and statistics to estimate the effect of a specific intervention or treatment by comparing the changes in outcomes over time between a group that’s exposed to the intervention (treatment group) and a group that is not (control group). The equation on this slide represents a basic DiD model: Yit=α+β1Treatmenti+β2Postt+γ(Treatmenti×Postt)+ϵitYit=α+β1Treatmenti+β2Postt+γ(Treatmenti×Postt)+ϵit Here’s what each term represents: - Yit is the outcome variable for unit i at time t. - α is the constant term or the baseline level of the outcome. - Treatmenti is an indicator variable that equals 1 if the observation ii belongs to the treatment group, and 0 otherwise. - Postt is an indicator for the post-treatment period, equaling 1 if the observation is after the intervention has been implemented, and 0 if before. - γ is the coefficient of interest, which represents the interaction between the treatment and the post period. This interaction term captures the differential effect of the treatment over time compared to the control group. - ϵit is the error term."
  },
  {
    "objectID": "slides/lees-quant.html#simulation-example",
    "href": "slides/lees-quant.html#simulation-example",
    "title": "Quantitative Analysis",
    "section": "Simulation Example",
    "text": "Simulation Example\n\n\nShow code\n# Load required libraries\nlibrary(dplyr)\nlibrary(modelsummary)\n\n# Generate example data\nset.seed(123)\ndata &lt;- data.frame(\n  treatment = rep(c(1, 0), each = 100),\n  post = rep(c(1, 0), each = 50, times = 2),\n  outcome = c(rnorm(50, mean = 10, sd = 2), # control: pre-treatment\n              rnorm(50, mean = 10, sd = 2), # control: post-treatment\n              rnorm(50, mean = 10, sd = 2), # treatment: pre-treatment\n              rnorm(50, mean = 12, sd = 2)) # treatment: post-treatment\n)\n\nhead(data)\n\n\n  treatment post   outcome\n1         1    1  8.879049\n2         1    1  9.539645\n3         1    1 13.117417\n4         1    1 10.141017\n5         1    1 10.258575\n6         1    1 13.430130\n\n\n\nWe begin by illustrating the Difference-in-Differences (DiD) method using a simulation example in R. This approach will help us understand how to implement DiD and interpret its output practically. By simulating data that mimics a controlled experiment, we can visually comprehend how DiD estimates the effect of an intervention First, we load two essential R libraries. dplyr is used for data manipulation, and modelsummary will help us to summarize models easily and produce nice tables for our results. Then we set the seed of R’s random number generator, which helps in ensuring that the results are reproducible. The same set of random numbers can be generated again, which is crucial for simulations. Next, we generate a dataset with three variables: - treatment: A binary variable where 1 represents the treatment group and 0 represents the control group. It’s repeated 100 times to match the number of observations. - post: A binary variable indicating whether the observation is post-treatment (1) or pre-treatment (0). It is repeated 50 times and cycled twice to align with the treatment periods. - outcome: The outcome variable, generated using a normal distribution. The means and standard deviations are set to simulate the effect of the treatment over time. The treatment group in the post-treatment period has a higher mean (12) compared to the pre-treatment period (10), which illustrates the impact of the intervention. This simulation setup allows us to apply the DiD analysis on a dataset where we know the true impact of the intervention, thus providing a clear way to evaluate the effectiveness of the DiD method. By comparing the outcomes before and after the treatment in both the treated and control groups, we can observe how the DiD estimator works in practice."
  },
  {
    "objectID": "slides/lees-quant.html#simulation-example-1",
    "href": "slides/lees-quant.html#simulation-example-1",
    "title": "Quantitative Analysis",
    "section": "Simulation Example",
    "text": "Simulation Example\n\n\nShow code\n# Run difference-in-differences model\ndid_model &lt;- lm(outcome ~ treatment * post, data = data)\n\n# Summarize the output\nmodelsummary(\n  list(lm(outcome ~ treatment + post, data = data), lm(outcome ~ treatment * post, data = data)),\n  estimate  = \"{estimate}{stars} ({std.error})\",\n             statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\n(Intercept)\n11.487*** (0.241)\n12.078*** (0.265)\n\n\ntreatment\n−0.604* (0.278)\n−1.785*** (0.375)\n\n\npost\n−1.405*** (0.278)\n−2.585*** (0.375)\n\n\ntreatment × post\n\n2.361*** (0.531)\n\n\nNum.Obs.\n200\n200\n\n\nR2 Adj.\n0.124\n0.201\n\n\n\n\n\n\n\n\nFinally, we estimate the difference in difference model and interpret the results. The first line of code fits a linear regression model (lm) where the outcome variable is regressed on the interactions between the treatment and post variables. The treatment * post syntax includes both the main effects of the treatment and post variables and their interaction term. This interaction term is the key to the DiD analysis, as it estimates the additional effect of being in the treatment group post-intervention relative to pre-intervention and control group changes. The next block uses the modelsummary function to create a summarized table of the model outputs: - List of Models: Two models are summarized here. The first is a simpler model without the interaction term, and the second is the full DiD model. - Estimate Formatting: The output format includes the estimated coefficients ({estimate}), significance stars ({stars}), and standard errors ({std.error}) in parentheses. - Goodness of Fit (gof) Omissions: The gof_omit parameter specifies which model fit indicators to omit from the summary, such as - - Information Criteria (IC), Root Mean Square Error (RMSE), etc. - Output: The summary is set to be outputted as a PowerPoint table (table.pptx), which is useful for presentation purposes.\nFinally, we interpret the results\nCoefficients: Look for the coefficient of the interaction term (treatment:post). This coefficient tells you the estimated effect of the treatment, adjusted for baseline differences and trends over time. Significance: Significance levels (indicated by stars) give an idea of the statistical significance of the estimates. Commonly, * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001. Standard Errors: These provide a measure of the variability of the coefficient estimates. Smaller standard errors suggest more precise estimates.\nIf the interaction term is statistically significant and positive, it suggests that the treatment had a positive effect on the outcome, over and above any baseline or time-related effects. The size of the coefficient on the interaction term quantifies the magnitude of this effect. Comparing the two models in the summary (with and without the interaction term) can also provide insights into the importance of considering the interaction between treatment and time for capturing the causal effect accurately.\nHere, we focus on Model 2, which is the differences-in-differences model:\nThe intercept (12.078)* represents the adjusted average baseline outcome for the control group before the intervention, slightly higher here due to adjustments in the model. The treatment (-1.785)* indicator coefficient is significantly larger and still negative, which reinforces the observation that the treatment group was initially performing worse than the control group. The post (-2.585)* coefficient indicates a substantial decline in the outcome for the control group in the post-treatment period, even more pronounced than in Model 1.\nThe crucial insight from Model 2 is the interaction term (Treatment × Post), which indicates that the treatment effectively improved outcomes by about 2.361 units in the post period, relative to what would have been expected based on the pre-period trends and the initial negative impact of the treatment. This suggests the treatment’s effectiveness becomes apparent only when considering its temporal implementation."
  },
  {
    "objectID": "slides/lees-quant.html#did-assumptions",
    "href": "slides/lees-quant.html#did-assumptions",
    "title": "Quantitative Analysis",
    "section": "DiD: Assumptions",
    "text": "DiD: Assumptions\n\nAssumption\n\nParallel Trends: Treatment and control units would have changed in similar ways without exposure to the treatment\n\nLimitations\n\nRequires at least two pre-treatment observations and one post-treatment observation (observations at 3 time points)\nCannot rule-out confounders that only affect the treatment group and are simultaneous with the treatment\nEstimation is more complicated when conditions change\n\n\n\nWhile DiD is widely used, it comes with some important assumptions and limitations. The Parallel Trends Assumption is critical and assumes that in the absence of treatment, the average outcomes for the treated and control groups would have followed parallel paths over time. This means any difference between these groups over time can be attributed to the treatment and not to other factors. To validate the parallel trends assumption, we need data for at least two time points before the intervention. More time points can increase the robustness of the model by allowing for more complex specifications and validation of the parallel trends assumption. The simplicity of DiD can also be a limitation. For instance, if other events affect the treated and control groups differently at the same time as the treatment (confounding events), this can bias the results. Moreover, the method only ensures balance on observed characteristics; unobserved biases that vary over time can still affect the outcome.\nFinally, estimation can get much more complicated when the treatment affects different treatment units at different times or when the treatment “turns on and off”."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-2",
    "href": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-2",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just observe how units change over time?",
    "text": "Why can’t we just observe how units change over time?\n\n\nShow code\nlibrary(ggplot2)\n\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.3, 1.7, NA)\nTreatment = c(\"Treatment\", \"Treatment\",\"Treatment\",\"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\") ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nIn the following slides, we visualize the logic of difference-in-difference designs. Imagine we have a treatment that was administered in between data collection in Year 1 and Year 2. Traditional evaluations often measure treatment units before and after an intervention"
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-3",
    "href": "slides/lees-quant.html#why-cant-we-just-observe-how-units-change-over-time-3",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just observe how units change over time?",
    "text": "Why can’t we just observe how units change over time?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(0.9, 1.3, 1.7, 2.1)\nTreatment = c(\"Treatment\", \"Treatment\",\"Treatment\",\"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\") ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nHowever, this method can’t rule-out the possibility that the treatment group was already improving at a steady pace, and would have improved by the same amount in the absence of the intervention. Here, we see that the blue line followed a consistent trend throughout the entire period, we it doesn’t look like the intervention had anything to do with the improvement between Y1 and Y2."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-2",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-2",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.2, 1.4, NA, \n            NA, 1.3, 1.7, NA)\nTreatment = c(\"Control\", \"Control\",\"Control\",\"Control\", \n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\ndat$Treatment = factor(dat$Treatment, levels = c(\"Treatment\", \"Control\"))\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\", \"red\") ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nOne solution is to have a comparison group that did not receive the treatment. Here, we see that the improvement between Y1 and Y2 was larger for the treatment group, suggesting there may have been an effect of the treatment."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-3",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-3",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(1, 1.2, 1.4, 1.6, \n            0.9, 1.3, 1.7, 2.1)\nTreatment = c(\"Control\", \"Control\",\"Control\",\"Control\", \n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\ndat$Treatment = factor(dat$Treatment, levels = c(\"Treatment\", \"Control\"))\n\n\nggplot(data = dat, aes(x = Year, y = Outcome, group = Treatment, color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  xlim(0,3) + \n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\", \"red\") ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nHowever, if the treatment was not randomly assigned, we can’t be confident that these groups would have changed in similar ways without the intervention. In this example, the control group was improving more slowly before the intervention, suggesting that the treatment did not have any effect on the improvement of the treatment group."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-4",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-4",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.3, 1.5, NA,\n            1, 1.2, 1.4, NA,\n            1.1, 1.3, 1.7, NA)\nTreatment = c(\"Comparison\",\"Comparison\",\"Comparison\",\"Comparison\",\n              \"Control\", \"Control\",\"Control\",\"Control\",\n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\ndat$Treatment = factor(dat$Treatment, levels = c(\"Treatment\", \"Comparison\", \"Control\"))\n\nggplot(data = dat, aes(x = Year, y = Outcome,  color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"dotted\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\", \"black\", \"red\"  ) ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nDifference-in-differences allows us to confirm whether or not the treatment and control group were changing at a similar rate before the treatment was administered (known as parallel trends). If the size of this change increases more for the treatment group than it does for the control group, we can consider this strong evidence for a causal effect of the treatment."
  },
  {
    "objectID": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-5",
    "href": "slides/lees-quant.html#why-cant-we-just-compare-units-without-randomization-5",
    "title": "Quantitative Analysis",
    "section": "Why can’t we just compare units without randomization?",
    "text": "Why can’t we just compare units without randomization?\n\n\nShow code\nYear = c(0,1,2,3)\nOutcome = c(NA, 1.3, 1.5, 1.7,\n            1, 1.2, 1.4,1.6,\n            1.1, 1.3, 1.7, 1.9)\nTreatment = c(\"Comparison\",\"Comparison\",\"Comparison\",\"Comparison\",\n              \"Control\", \"Control\",\"Control\",\"Control\",\n              \"Treatment\", \"Treatment\", \"Treatment\", \"Treatment\")\n\ndat = data.frame(Year, Outcome, Treatment)\ndat$Treatment = factor(dat$Treatment, levels = c(\"Treatment\", \"Comparison\", \"Control\"))\n\nggplot(data = dat, aes(x = Year, y = Outcome,  color = Treatment)) +\n  geom_line(aes(linetype=Treatment),size=2) +\n  geom_point(size = 6) +\n  ylim(0.8, 2.2) +\n  scale_linetype_manual(values=c(\"solid\", \"dotted\", \"solid\")) +\n  scale_color_manual(values = c(\"blue\", \"black\", \"red\"  ) ) +\n  theme(legend.position = c(0.8, 0.2), text = element_text(size=20),\n        legend.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nObservations for more time periods allow us to be more confident in these findings."
  },
  {
    "objectID": "slides/lees-quant.html#regression-discontinuity-designs",
    "href": "slides/lees-quant.html#regression-discontinuity-designs",
    "title": "Quantitative Analysis",
    "section": "Regression Discontinuity Designs",
    "text": "Regression Discontinuity Designs\n\nEstimating causal effects using a naturally occurring cutoff or threshold that determines which subjects are exposed to a treatment\nAssumes that subjects just below and just above are similar on all characteristics aside from receiving the treatment\nAllows researchers to estimate a Local Average Treatment Effect (LATE)\n\n\nThe main idea of RDD is to estimate causal effects by taking advantage of a naturally occurring cutoff or threshold. This could be something like a test score determining eligibility for a scholarship, where the cutoff score is the threshold. In RDD, we make a critical assumption: individuals who are just below and just above this cutoff are similar in all respects except for the exposure to the treatment. This assumption is crucial because it implies that any differences observed between these two groups can be attributed to the treatment effect and not to other underlying differences. It must be the case that subjects cannot change their behavior to ensure that they fall on one side of the cutoff instead of the other, which we refer to as “sorting”. Using RDD allows researchers to estimate what we call the Local Average Treatment Effect, or LATE. This measure tells us the effect of the treatment on those who are precisely at the threshold—those who are affected by being just above or just below the cutoff. It is important to note that this effect may not generalize to individuals further from the threshold. This method is particularly powerful in settings where randomized control trials are impractical or unethical. It provides a robust way to measure the impact of treatments or interventions in a natural setting, making it a valuable tool in our statistical toolbox."
  },
  {
    "objectID": "slides/lees-quant.html#example",
    "href": "slides/lees-quant.html#example",
    "title": "Quantitative Analysis",
    "section": "Example",
    "text": "Example\n\nExploring the effect of a merit-based scholarship to attend college on future earnings\nWe are interested in this relation:\n\nA[Merit-based Scholarship] –&gt; B[Earnings after college]\n\nSuppose we run a regression such as:\n\nEarnings_i = \\alpha + \\beta \\times Scholarship_i + \\epsilon_i\nProblems? - Confounding: People who earn merit-based scholarships are different than those who do not in important ways: in grades, in abilities, in drive…\n\nLet’s use a practical example to illustrate the concept of causality in statistical research. We are focusing on the effect of a merit-based scholarship on future earnings after college.\nImagine we want to analyze whether receiving a merit-based scholarship to attend college influences a person’s earnings after they graduate. The relationship we’re interested in is straightforward: does the scholarship lead to higher earnings? To explore this, one might think about running a simple regression model where earnings after college depend on whether a person received a scholarship. The model might look something like this:\nEarningsi=α+β×Scholarshipi+ϵiEarningsi=α+β×Scholarshipi+ϵi\nHere, αα represents the baseline earnings, ββ represents the effect of the scholarship, and ϵiϵi is the error term.\nHowever, there’s a significant problem with this approach: confounding variables. People who earn merit-based scholarships are likely different from those who do not in several important ways. They might have higher grades, better test scores, or more drive and ambition. These differences can influence their future earnings independently of the scholarship. Thus, any analysis that doesn’t account for these factors might mistakenly attribute the effect of these underlying characteristics to the scholarship itself. This is a classic example of the confounding problem in causal inference."
  },
  {
    "objectID": "slides/lees-quant.html#example-1",
    "href": "slides/lees-quant.html#example-1",
    "title": "Quantitative Analysis",
    "section": "Example",
    "text": "Example\n\nExample: effect of a merit-based scholarship to attend college on future earnings\nSuppose we know that scholarships are assigned based on a score.\n\nA[Score] –&gt; B[Merit-based Scholarship] –&gt; C[Earnings after college]\n\n…and there is a strict rule to allocate scholarships:\n\nIf score &gt; (c_0) → Scholarship\nIf score ≤ (c_0) → No Scholarship\n\nStudents just below (c_0) and just above (c_0) are likely to be very similar.\nTherefore, for people within small bandwidth around the (c_0) threshold we can estimate a Local Average Treatment Effect.\n\n\nImagine scholarships are assigned based on a score, and there’s a strict rule for allocation: if a student’s score is above a certain threshold, c0, they receive a scholarship; if it’s below, they do not.\nThe key here is that students just below and just above the threshold, c0, are very similar in all respects except for the receipt of the scholarship. This similarity allows us to make a clear comparison, focusing on the effect of the scholarship alone, without the confounding factors we discussed earlier.\nGiven this setup, we can apply a Regression Discontinuity Design. We focus on students within a small bandwidth around the threshold score. This allows us to estimate what’s known as the Local Average Treatment Effect, or LATE. The LATE gives us insights into the impact of the scholarship on those students who are right around the cutoff—those who just qualify and those who just miss out.\nThis method is powerful because it closely mimics a randomized experiment, the gold standard in causal inference, by comparing very similar individuals who differ only in the treatment received—here, the scholarship based on their score relative to c0c0."
  },
  {
    "objectID": "slides/lees-quant.html#example-2",
    "href": "slides/lees-quant.html#example-2",
    "title": "Quantitative Analysis",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lees-quant.html#adjusting-on-observables",
    "href": "slides/lees-quant.html#adjusting-on-observables",
    "title": "Quantitative Analysis",
    "section": "Adjusting on Observables",
    "text": "Adjusting on Observables\n\nFocuses analysis on units with characteristics that are common in both the treatment and control group\n\nAssumes that units similar on observed characteristics are also similar on unobserved characteristics (Ignorability)\nBenefits from large samples\n\nMatching\n\nGenetic matching, Propensity score matching\n\nWeighting\n\nEntropy Balancing, Inverse probability weighting, Synthetic control\n\n\n\nAdjusting on observable characteristic can be useful where randomization is not possible and we do not have a cutoff in the assignment of a treatment. Matching or weighting involves pairing or grouping units—like individuals, schools, or communities—based on similar observable characteristics. The goal here is to create equivalent groups that mimic the conditions of a randomized experiment by focusing our analysis on units with characteristics that are common in both the treatment and control group. This approach is particularly useful in observational studies. For example, in healthcare research, patients receiving different treatments can be matched based on variables such as age, pre-existing conditions, and lifestyle choices to assess the effectiveness of a new drug or treatment protocol. Matching and weighting techniques rely on the assumption of “ignorability” or “conditional independence assumption” (CIA). This assumption holds that once you control for the observed covariates, the assignment to the treatment group is as good as random. In practical terms, this means all confounders must be observed and correctly measured so that any two units with the same values on observed covariates have the same potential outcomes irrespective of treatment status. In other words, the key assumption underpinning this method is that units matched on observables are comparable. However, it is impossible to validate this assumption, posing a fundamental limitation of this method. Many matching and weighting methods, particularly those that rely on propensity scores, perform better with large samples. In smaller samples, these methods may struggle to find appropriate matches, reducing the statistical power of the study and increasing the variance of the estimate. Importantly, these methods can be paired with DiD designs to make the parallel trends assumption more realistic."
  },
  {
    "objectID": "slides/lees-quant.html#statistical-power",
    "href": "slides/lees-quant.html#statistical-power",
    "title": "Quantitative Analysis",
    "section": "Statistical Power",
    "text": "Statistical Power\n\nStatistical Power: The probability that a statistical test will correctly detect a treatment effect when there actually is one\nFactors Affecting Power:\n\nSample Size: Larger samples generally increase power.\nEffect Size: Larger effects are easier to detect.\nSignificance Level: Lower \\(\\alpha\\) levels reduce power, though they decrease the risk of false positives.\nVariability: Less variability (i.e., lower standard deviation) in data increases power.\n\n\n\nStatistical power is fundamental to the design and interpretation of any research study. Statistical power is defined as the probability that our test will correctly reject a false null hypothesis. In simpler terms, it’s the likelihood that our study will actually detect an effect if there is one to be found. Why is power important? Without sufficient power, our study might miss detecting real effects, leading to what we call a Type II error, or a false negative. This means we might conclude there is no effect when, in fact, there is one. If a treatment has a positive impact, but we conclude that it doesn’t, this can waste valuable resources and deprive people of welfare-enhancing benefits. Several key factors influence the power of a statistical test. First, the sample size: larger sample sizes increase the power of a test, improving our chances of detecting an effect. Second, the effect size: if the effect we are looking for is larger, it will be easier to detect. Third, the significance level, typically set at 0.05 for many tests. Lowering this level makes it harder to achieve statistical significance, which can reduce power, though it also makes it less likely to get a false positive. Lastly, variability in our data: less variability means our effects are more discernible, which increases power. Therefore, it’s crucial to conduct a power analysis during the design phase of a study. This analysis will help us determine the necessary sample size and other parameters to ensure our study has adequate power to detect meaningful effects. By planning for power, we maximize the reliability and validity of our research findings"
  },
  {
    "objectID": "slides/lees-quant.html#statistical-power-1",
    "href": "slides/lees-quant.html#statistical-power-1",
    "title": "Quantitative Analysis",
    "section": "Statistical Power",
    "text": "Statistical Power\n\nPlanning for Power: Conduct power analyses before study design to determine the necessary sample size to achieve adequate power.\nMany tools exist to estimate the power of your research design:\n\nThis tool from EGAP is a good starting point: https://egap.shinyapps.io/power-app/\nResearchers with more sophisticated designs should check out the declaredesign package for R.\n\n\n\nPlanning for statistical power is critical because it allows us to determine the necessary sample size and other parameters needed to achieve adequate power in our study. By conducting power analyses before the study design, we ensure that our study is capable of detecting the effects we are interested in observing, thus reducing the risk of Type II errors—failing to detect an effect when there actually is one. To aid in these analyses, there are numerous tools available that can estimate the power of your research design. One accessible tool that I recommend for those starting out is the EGAP Power Analysis Tool, which you can find online. This tool is user-friendly and can be very helpful in preliminary power calculations. For those involved in more sophisticated study designs, I suggest exploring the ‘DeclareDesign’ package for R. This package allows for a more detailed and nuanced approach to planning your study, taking into account complex design elements and providing a robust framework for understanding the interplay between design decisions and statistical power."
  },
  {
    "objectID": "slides/lees-quant.html#multiple-hypothesis-tests",
    "href": "slides/lees-quant.html#multiple-hypothesis-tests",
    "title": "Quantitative Analysis",
    "section": "Multiple Hypothesis Tests",
    "text": "Multiple Hypothesis Tests\n\nResearchers usually want to measure important outcomes in multiple ways\nTesting the same hypothesis with multiple separate measures will increase the probability of mistakenly finding a positive effect even\nTraditional correction methods can reduce statistical power\nIndex measures can help to avoid these problems\n\n\nResearchers usually want to measure important outcomes in multiple ways. This can be extremely important. Any individual approach to measuring an outcome might be subject to unexpected measurement error. For example, if we want to measure the effect of an intervention on civic engagement, survey-based measures asking about recent behavior are vulnerable to subjects over-stating their engagement or mis-remembering the number of times they’ve participated in politics. For this reason, we might also want to consult administrative records whether a person voted or volunteered with community organizations. However, when we test the same hypotheses using multiple different measures, we encounter a significant statistical challenge known as multiple hypothesis testing. This issue arises because the more tests we perform, the higher the likelihood of encountering a false positive. Essentially, if we conduct, say, 20 tests, each at a significance level of 5%, we might expect one of those tests to show significant results purely by chance, even if all our null hypotheses are true. Traditionally, statisticians have used methods such as the Bonferroni correction to adjust the significance levels when multiple comparisons are made. Essentially, this increases the level of statistical significance required to conclude that an effect exists. However, these methods reduce the power of statistical tests, increasing the risk of falsely concluding that a treatment was not effective. Especially in the context of expensive IEs where sample sizes are often small, such corrections should be avoided when possible. This is where index measures come into play. By combining several indicators into a single index, we reduce the total number of hypotheses being tested. This consolidation not only simplifies our analysis but also inherently controls for the risk of multiple hypothesis testing errors. Consequently, our statistical analysis becomes not just simpler, but more accurate and meaningful, focusing on robust, interpretable results rather than being clouded by potential statistical anomalies."
  },
  {
    "objectID": "slides/lees-quant.html#creating-index-measures",
    "href": "slides/lees-quant.html#creating-index-measures",
    "title": "Quantitative Analysis",
    "section": "Creating Index Measures",
    "text": "Creating Index Measures\nWhen to create an index measure\n\nWhen you have many ways of measuring a single concept\n\nThis is true for outcome measures, treatment measures, and covariates\n\n\nBenefits of index measures\n\nSimplifies analysis (fewer graphs, tables, etc.)\nReduces number of hypotheses being tested\n\n\nThis is a crucial technique, especially when you have numerous ways to measure a single concept. This applies to various types of measures including outcome measures, treatment measures, and covariates. Now, why do we create index measures? There are two main benefits: First, it simplifies our analysis. By combining multiple indicators into a single index, we reduce the complexity of our data. This means fewer graphs and tables, and a more streamlined presentation of our results. Second, creating an index measure helps in reducing the number of hypotheses being tested. This is important because it minimizes the risk of statistical errors that can occur from multiple testing."
  },
  {
    "objectID": "slides/lees-quant.html#additive-scale",
    "href": "slides/lees-quant.html#additive-scale",
    "title": "Quantitative Analysis",
    "section": "Additive Scale",
    "text": "Additive Scale\nWhat is an additive scale?\n\nSimple sum across columns (index = column_1 + column_2)\n\nWhen to use an additive scale\n\nWhen variables are measured on a common scale\nWhen you are interested in a cumulative amount of something\n\nNumber of times someone engaged in a specific behavior\nAmount of money from several different sources\n\n\n\nAn additive scale is a straightforward yet powerful statistical tool used to combine multiple variables into a single index. This is achieved by simply summing up the values across columns. For instance, if you have two variables, the index would be calculated as the sum of column_1 and column_2. Now, let’s talk about when it’s appropriate to use an additive scale. This method is particularly useful when the variables you’re combining are measured on a common scale. This ensures that each variable contributes equally to the final index score. An additive scale is also beneficial when you’re interested in a cumulative quantity of something. For example, if you want to measure the total number of times an individual has engaged in a specific behavior, or if you’re looking to calculate the total amount of money received from several different sources. By using an additive scale, you can create a single measure that represents a comprehensive view of these cumulative or combined behaviors or contributions, making your data analysis both simpler and more meaningful."
  },
  {
    "objectID": "slides/lees-quant.html#additive-scale-1",
    "href": "slides/lees-quant.html#additive-scale-1",
    "title": "Quantitative Analysis",
    "section": "Additive Scale",
    "text": "Additive Scale\nBenefits of additive scales\n\nInterpretability: number on the original scale\nSimplicity: Just plain addition\n\n\nLet’s dive into some of the key benefits of using additive scales in our data analysis. First, we have interpretability. One of the major advantages of an additive scale is that the resultant index remains on the original scale of the variables. This means that the numbers produced are easy to understand and interpret because they directly relate to the familiar measures of the variables involved. Second is simplicity. The process involves plain addition, which makes it not only easy to calculate but also straightforward to explain to stakeholders or audiences who may not be familiar with complex statistical techniques. This simplicity also reduces errors in computation and data handling, making additive scales a reliable choice for many research scenarios."
  },
  {
    "objectID": "slides/lees-quant.html#additive-scale-2",
    "href": "slides/lees-quant.html#additive-scale-2",
    "title": "Quantitative Analysis",
    "section": "Additive Scale",
    "text": "Additive Scale\n\n\nShow code\nset.seed(123)\n# Create a data frame with 200 observations\ndata &lt;- data.frame(\n  variable1 = rnorm(200, mean = 50, sd = 10),  # Generate 200 random numbers for the first variable\n  variable2 = rnorm(200, mean = 30, sd = 5)   # Generate 200 random numbers for the second variable\n)\n\n# Create an additive index variable\n# This index is simply the sum of variable1 and variable2\ndata &lt;- data %&gt;%\n  mutate(additive_index = variable1 + variable2)\n\n# Print the first few rows of the generated data\n# head() function shows the first six rows of the data frame by default\n# The sum of variable1 and variable2 is equivalent to the additive_index value\nhead(data)\n\n\n  variable1 variable2 additive_index\n1  44.39524  40.99405       85.38930\n2  47.69823  36.56206       84.26029\n3  65.58708  28.67427       94.26136\n4  50.70508  32.71597       83.42105\n5  51.29288  27.92830       79.22118\n6  67.15065  27.61877       94.76942"
  },
  {
    "objectID": "slides/lees-quant.html#averaged-z-scores",
    "href": "slides/lees-quant.html#averaged-z-scores",
    "title": "Quantitative Analysis",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\nWhat is a z-score?\n\nZ = (X - \\mu) / \\sigma\nStandardized: Mean of 0 and standard deviation of 1\n\nWhen to use averaged z-scores\n\nWhen variables are measured on different scales\nWhen variables cannot be summed\n\n\nAnother useful method is the ‘Averaged Z-Scores.’ Let’s start by understanding what a z-score is. A z-score, or standard score, is a way to describe a value’s relationship to the mean of a group of values, measured in terms of standard deviations from the mean. Mathematically, it’s expressed as Z=(X−μ)/σZ=(X−μ)/σ, where X is the variable, μ is the mean of that variable, and σ is the variable’s standard deviation. This standardization process transforms the data to have a mean of 0 and a standard deviation of 1. Now, when should we use averaged z-scores? This approach is particularly useful in two scenarios: - When variables are measured on different scales: If you’re working with variables that have different units or scales, converting them into z-scores allows them to be compared and combined meaningfully. This is because z-scores normalize the data, removing the units and putting everything on a common scale. - When variables cannot be summed directly: There are cases where it’s not appropriate or possible to sum variables directly, perhaps due to their differing natures or units. Averaged z-scores provide a way to aggregate these variables into a single index while retaining the statistical integrity of the data. By averaging these standardized scores, we can create composite indices that allow us to conduct a single hypotheses that combines multiple measures of the same outcome."
  },
  {
    "objectID": "slides/lees-quant.html#averaged-z-scores-1",
    "href": "slides/lees-quant.html#averaged-z-scores-1",
    "title": "Quantitative Analysis",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\n\n\nBenefits of averaged z-scores\n\nInterpretability: Standard deviations from the mean\nOutlier detection: abs(3)\n\n\nContinuing our discussion on averaged z-scores, let’s focus on the key benefits this statistical method brings to our data analysis. Firstly, interpretability is a major advantage. By transforming our data into z-scores, we represent each value as its distance from the mean in terms of standard deviations. This means that the transformed data points tell us how many standard deviations they lie from the average. A z-score of +1.0 indicates a value one standard deviation above the mean, while -1.0 indicates one standard deviation below. This standardization makes it easier to understand where each data point stands in relation to the group average, enhancing our ability to interpret complex data sets. For this reason, z-scores are often used to report the results of IEs. A general rule of thumb is that effects of less than 0.2 standard deviations is a small effect, between .2 and .4 is a medium effect, and anything above .4 is a very large effect. Another significant benefit is in outlier detection. By using the criterion of z-scores exceeding an absolute value of 3, we can easily identify outliers. Values that fall beyond three standard deviations from the mean are typically considered extreme. It is often important to drop extreme values from analyses to ensure that these outliers are not driving your findings."
  },
  {
    "objectID": "slides/lees-quant.html#averaged-z-scores-2",
    "href": "slides/lees-quant.html#averaged-z-scores-2",
    "title": "Quantitative Analysis",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\n\n\nShow code\nset.seed(123)\n# Create a data frame with 200 observations\ndata &lt;- data.frame(\n  variable1 = rnorm(200, mean = 50, sd = 10),  # Generate 200 random numbers for the first variable\n  variable2 = rnorm(200, mean = 30, sd = 5),   # Generate 200 random numbers for the second variable\n  variable3 = rnorm(200, mean = 20, sd = 3)    # Generate 200 random numbers for the third variable\n)\n\n# Standardize each variable to have a mean of 0 and a standard deviation of 1\n# Here, we create three new variables (z_variable1, z_variable2, z_variable3)\n# which are the standardized (z-score) versions of variable1, variable2, and variable3.\ndata &lt;- data %&gt;%\n  mutate(\n    z_variable1 = (variable1 - mean(variable1)) / sd(variable1),  # Z-score for variable1\n    z_variable2 = (variable2 - mean(variable2)) / sd(variable2),  # Z-score for variable2\n    z_variable3 = (variable3 - mean(variable3)) / sd(variable3)   # Z-score for variable3\n  )"
  },
  {
    "objectID": "slides/lees-quant.html#averaged-z-scores-3",
    "href": "slides/lees-quant.html#averaged-z-scores-3",
    "title": "Quantitative Analysis",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\n\n\nShow code\n# Create an index by averaging the z-scores of the variables\n# In this case, select(., starts_with(\"z_variable\")) selects all columns whose names start with \"z_variable\"\n# rowMeans(select(., starts_with(\"z_variable\"))) calculates the average of these selected columns for each row\ndata &lt;- data %&gt;%\n  mutate(average_z_score = rowMeans(select(., starts_with(\"z_variable\"))))\n\n# Print the first few rows of the generated data\nhead(data)\n\n\n  variable1 variable2 variable3 z_variable1 z_variable2 z_variable3\n1  44.39524  40.99405  19.77933 -0.58516612   2.1655224 -0.10917385\n2  47.69823  36.56206  16.49405 -0.23496233   1.2754946 -1.24421681\n3  65.58708  28.67427  18.09576  1.66173180  -0.3085246 -0.69083761\n4  50.70508  32.71597  19.91348  0.08384457   0.5031252 -0.06282826\n5  51.29288  27.92830  22.01209  0.14616629  -0.4583306  0.66222741\n6  67.15065  27.61877  15.04836  1.82751140  -0.5204911 -1.74369079\n  average_z_score\n1      0.49039415\n2     -0.06789484\n3      0.22078985\n4      0.17471384\n5      0.11668771\n6     -0.14555682"
  },
  {
    "objectID": "slides/lees-quant.html#fancier-index-techniques",
    "href": "slides/lees-quant.html#fancier-index-techniques",
    "title": "Quantitative Analysis",
    "section": "Fancier index techniques",
    "text": "Fancier index techniques\n\n\n\nPrincipal Component Analysis\nFactor Analysis\nInverse Covariance Weighting\n\n\nThere are also more sophisticated techniques available to create index variables. These methods not only help us create robust indices but also enrich our analytical capabilities. First, we have Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This technique is especially useful when you want to reduce the dimensionality of your dataset while retaining as much variance as possible. It helps in creating a new underlying variable that explains most of the variability in your data. Next, Factor Analysis is another technique that is similar to PCA but focuses more on latent variables that cannot be measured directly. This method examines the interrelationships among a large number of variables and condenses them into a few factors for easier interpretation and analysis. Factor analysis is particularly useful when your variables are related and you suspect that they are measuring underlying factors. Lastly, Inverse Covariance Weighting. This method weights variables inversely to their covariance with other variables, aiming to minimize the impact of highly correlated variables in the analysis. It’s a way to adjust the contribution of each variable to the index based on how much unique information it provides."
  },
  {
    "objectID": "slides/lees-quant.html#interaction-terms",
    "href": "slides/lees-quant.html#interaction-terms",
    "title": "Quantitative Analysis",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nWhat is an interaction term?\n\nSimple linear models assume that the effect of predictors is independent of other factors\nInteraction terms allow us to estimate the difference in the slope of a predictor across unit characteristics\n\nWhat are interaction terms used for?\n\nHeterogeneous effects\nDifference-in-differences\n\n\nInteraction terms are crucial role in understanding heterogeneous effects in regression models. Let’s start by understanding what an interaction term is. In simple linear regression models, we typically assume that the effect of one predictor on the outcome variable is independent of other factors. However, this assumption often oversimplifies reality, as the impact of a predictor can vary depending on the levels or presence of another predictor. This is crucial in fields such as medicine, where the effectiveness of a treatment might depend on patients’ age or genetic background, and in economics, where the impact of a policy might differ across regions or demographic groups. For example, we might want to test the impact of a new teaching method on students test scores. However, we might expect that the effectiveness of this new teaching method in improving students’ test scores differs between students from low and high socioeconomic backgrounds. You might also recognize interaction terms from our difference-in-differences tutorial. In policy evaluation, interaction terms are used to implement the difference-in-differences estimation. This technique compares the before-and-after differences in outcomes between a treatment group and a control group, thus controlling for common trends that might affect both groups. Interaction terms are incorporated into regression models to allow us to estimate how the effect of one variable changes as another variable changes. Essentially, these terms help us identify and quantify synergies or interdependencies between variables."
  },
  {
    "objectID": "slides/lees-quant.html#interaction-terms-1",
    "href": "slides/lees-quant.html#interaction-terms-1",
    "title": "Quantitative Analysis",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nY_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1}*X_{i2} + \\epsilon_i\nExample: Continuous outcome with two binary predictors\n\n\\alpha: Intercept when X_{i1} and X_{i2} are 0\n\\beta_1 Slope when X_{i2} = 0\n\\beta_2 Difference in \\alpha between X_{i2}=0 and X_{i2}=1\n\\beta_3 Difference in \\beta_1 between X_{i2}=0 and X_{i2}=1\n\n\nThe formula on the slide illustrates how to include an interaction term in a regression model: Yi=α+β1Xi1+β2Xi2+β3Xi1Xi2+ϵiYi=α+β1Xi1+β2Xi2+β3Xi1Xi2+ϵi In this equation: α represents the intercept, the expected value of Y when both Xi1 and Xi2 are 0. β1 shows the change in Y when Xi1 changes from 0 to 1, assuming Xi2 is 0. It reflects the isolated effect of Xi1. β2 indicates the difference in intercept (α) when Xi2 changes from 0 to 1, assuming Xi1 remains 0 β3 measures the difference in the slope of Xi1 between the scenarios when Xi2 is 0 and when Xi2 is 1. This coefficient is crucial for understanding how the relationship between Y and Xi1 changes in the presence of Xi2. By adding β3 to β1, we are able to observe how the slope of the predictor Xi1Xi1 varies across different levels of Xi2, providing a richer understanding of the data dynamics. This is particularly useful in policy analysis, medical research, and economic studies, where the effect of an intervention may depend on individual or contextual characteristics. Imagine Xi1 captures whether or not at student was exposed to the new teaching method and Xi2 captures whether or not the student is high-socioeconomic status. If the interaction term β3 is significantly positive, it suggests that high SES students benefit more from the new method compared to low SES students. This could lead to insights that help tailor educational strategies to maximize their effectiveness across different demographic groups."
  },
  {
    "objectID": "slides/lees-quant.html#interaction-terms-2",
    "href": "slides/lees-quant.html#interaction-terms-2",
    "title": "Quantitative Analysis",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\n\nInteractions\nset.seed(123)\n# Create a data frame with 200 observations\ndata &lt;- data.frame(\n  treatment = rep(c(1, 0), each = 100),      # Create a binary treatment variable: 1 for treatment, 0 for control\n  group = rep(c(1, 0), each = 50, times = 2), # Create a binary group variable: 1 for Group 1, 0 for Group 2\n  outcome = c(rnorm(50, mean = 10, sd = 2),  # Group 1: treatment\n              rnorm(50, mean = 10, sd = 2),  # Group 2: treatment\n              rnorm(50, mean = 10, sd = 2),  # Group 1: control\n              rnorm(50, mean = 15, sd = 2))  # Group 2: control\n)\n\n# Run linear regression model with interaction term (outcome ~ treatment * group)\ninteraction_model &lt;- lm(outcome ~ treatment * group, data = data)\n\n# modelsummary() function provides a summary of the regression models\nmodelsummary(\n  list(\n    lm(outcome ~ treatment + group, data = data),    # Model without interaction term\n    lm(outcome ~ treatment * group, data = data)     # Model with interaction term\n  ),\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.',\n  output = \"dataframe\"\n)\n\n\n       part              term         statistic           Model 1\n1 estimates       (Intercept) modelsummary_tmp1 13.737*** (0.283)\n2 estimates         treatment modelsummary_tmp1 -2.104*** (0.326)\n3 estimates             group modelsummary_tmp1 -2.905*** (0.326)\n4 estimates treatment × group modelsummary_tmp1                  \n5       gof          Num.Obs.                                 200\n6       gof           R2 Adj.                               0.374\n            Model 2\n1 15.078*** (0.265)\n2 -4.785*** (0.375)\n3 -5.585*** (0.375)\n4  5.361*** (0.531)\n5               200\n6             0.586"
  },
  {
    "objectID": "slides/lees-quant.html#survey-experiments",
    "href": "slides/lees-quant.html#survey-experiments",
    "title": "Quantitative Analysis",
    "section": "Survey Experiments",
    "text": "Survey Experiments\n\n10 Things to Know About Survey Experiments by Christopher Grady\nPew’s Question Wording Experiments\nSniderman (2018): Review of advances in survey experiment design\nLeeper’s survey experiments mini-course: Leeper was an academic political scientists and is now a Research Scientist at Meta"
  },
  {
    "objectID": "workshops/lees_R_doc.html",
    "href": "workshops/lees_R_doc.html",
    "title": "Live R Coding Session",
    "section": "",
    "text": "Before using R to illustrate basic programming concepts and data analysis tools, we will get familiar with the RStudio layout.\n\n\nRStudio has four primary panels that will help you interact with your data. We will use the default layout of these panels.\n\nSource panel: Top left\n\nEdit files to create ‘scripts’ of code\n\nConsole panel: Bottom left\n\nAccepts code as input\nDisplays output when we run code\n\nEnvironment panel: Top right\n\nEverything that R is holding in memory\nObjects that you create in the console or source panels will appear here\nYou can clear the environment with the broom icon\n\nViewer panel: Bottom-right\n\nView graphics that you generate\nNavigate files\n\n\n\n\n\nLet’s use these panels to create and interact with data.\nConsole:\n\nPerform a calculation: type 2 + 2 into the console panel and hit ENTER\nCreate and store an object: type sum = 2 + 2 into the console panel and hit ENTER\n\nSource:\n\nStart an R script: Open new .R file (button in top-left below “File”)\nCreate and store an object: type sum = 2 + 3 into the source panel and hit cntrl+ENTER\n\nEnvironment:\n\nConfirm that the object sum is stored in our environment\nUse rm(sum) to clear the object from the environment\nClear the environment with the broom icon\n\nViewer:\n\nNavigate through your computer’s files\nCreate a plot in the source panel"
  },
  {
    "objectID": "workshops/lees_R_doc.html#rstudio-contains-4-panels",
    "href": "workshops/lees_R_doc.html#rstudio-contains-4-panels",
    "title": "Live R Coding Session",
    "section": "",
    "text": "RStudio has four primary panels that will help you interact with your data. We will use the default layout of these panels.\n\nSource panel: Top left\n\nEdit files to create ‘scripts’ of code\n\nConsole panel: Bottom left\n\nAccepts code as input\nDisplays output when we run code\n\nEnvironment panel: Top right\n\nEverything that R is holding in memory\nObjects that you create in the console or source panels will appear here\nYou can clear the environment with the broom icon\n\nViewer panel: Bottom-right\n\nView graphics that you generate\nNavigate files"
  },
  {
    "objectID": "workshops/lees_R_doc.html#illustration",
    "href": "workshops/lees_R_doc.html#illustration",
    "title": "Live R Coding Session",
    "section": "",
    "text": "Let’s use these panels to create and interact with data.\nConsole:\n\nPerform a calculation: type 2 + 2 into the console panel and hit ENTER\nCreate and store an object: type sum = 2 + 2 into the console panel and hit ENTER\n\nSource:\n\nStart an R script: Open new .R file (button in top-left below “File”)\nCreate and store an object: type sum = 2 + 3 into the source panel and hit cntrl+ENTER\n\nEnvironment:\n\nConfirm that the object sum is stored in our environment\nUse rm(sum) to clear the object from the environment\nClear the environment with the broom icon\n\nViewer:\n\nNavigate through your computer’s files\nCreate a plot in the source panel"
  },
  {
    "objectID": "workshops/lees_R_doc.html#objects-where-values-are-saved-in-r",
    "href": "workshops/lees_R_doc.html#objects-where-values-are-saved-in-r",
    "title": "Live R Coding Session",
    "section": "Objects: where values are saved in R",
    "text": "Objects: where values are saved in R\n“Object” is a generic term for anything that R stores in the environment. This can include anything from an individual number or word, to lists of values, to entire datasets.\nImportantly, objects belong to different “classes” depending on the type of values that they store.\n\nNumerics are numbers\nCharacters are text or strings like \"hello world\" and \"welcome to R\".\nFactors are a group of characters/strings with a fixed number of unique values\nLogicals are either TRUE or FALSE\n\n\n# Create a numeric object\nmy_number = 5.6\n# Check the class\nclass(my_number)\n\n[1] \"numeric\"\n\n# Create a character object\nmy_character = \"welcome to R\"\n# Check the class\nclass(my_character)\n\n[1] \"character\"\n\n# Create a logical object\nmy_logical = TRUE\n# Check the class\nclass(my_logical)\n\n[1] \"logical\"\n\n\nR can perform operations on objects.\n\n# Create a numeric object\nmy_number = 5.6\n# Check the class\nclass(my_number)\n\n[1] \"numeric\"\n\n# Perform a calculation\nmy_number = my_number + 5\n\nThe class of an object determines the type of operations you can perform on it. Some operations can only be run on numeric objects (numbers).\n\n# Create a character object\nmy_number = \"5.6\"\n# Check the class\nclass(my_number)\n# Perform a calculation\nmy_number + 5\nround(my_number)\n\nR contains functions that can convert some objects to different factors.\n\n# Convert character to numeric\nmy_number = as.numeric(\"5\")\nclass(my_number)\n\n[1] \"numeric\"\n\nmy_number &lt;- 5\n\n# But R is only so smart\nmy_number = as.numeric(\"five\")\nprint(my_number)\n\n[1] NA"
  },
  {
    "objectID": "workshops/lees_R_doc.html#data-structures",
    "href": "workshops/lees_R_doc.html#data-structures",
    "title": "Live R Coding Session",
    "section": "Data Structures",
    "text": "Data Structures\nThe most simple objects are single values, but most data analysis involves more complicated data structures.\n\nLists\nLists are a type of data structure that store multiple values together. Lists are created using c() and allow you to perform operations on a series of values.\n\n# Create a numeric list (also called a \"vector\")\nnumeric_vector = c(6, 11, 13, 31)\n# Print the vector\nprint(numeric_vector)\n\n[1]  6 11 13 31\n\n# Check the class\nclass(numeric_vector)\n\n[1] \"numeric\"\n\n# Calculate the mean\nmean(numeric_vector)\n\n[1] 15.25\n\n\nAn important part of working with more complex data structures is called “indexing.” Indexing allows you to extract specific values from a data structure.\n\n# Extract the 2nd element from the list\nnumeric_vector[2]\n\n[1] 11\n\n# Extract elements 2-4\nnumeric_vector[2:4]\n\n[1] 11 13 31\n\n# Extract elements 1-2\nnumeric_vector[c(TRUE, TRUE, FALSE, FALSE)]\n\n[1]  6 11\n\n\n\n\nDataframes\nData frames are the most common type of data structure used in research. Data frames combine multiple lists of values into a single object.\n\n# Create a dataframe\nmy_data = data.frame(\n  x1 = rnorm(100, mean = 1, sd = 1),\n  x2 = rnorm(100, mean = 1, sd = 1)\n)\n\nclass(my_data)\n\n[1] \"data.frame\"\n\n\nAnything that comes in a spreadsheet (for example, an excel file) can be loaded into an R environment as a dataframe. R works most easily when spreadsheets are saved as a .csv file.\n\n# Use `read.csv()` to load data from a website\ndat = read.csv(\"https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv\") \n\n# Use `read.csv()` to load data from your computer's Downloads folder\n# dat = read.csv(\"/home/jeremy/Downloads/clean_endline_did.csv\")\n\nIn most data frames, rows correspond to observations and the columns correspond to variables that describe the observations. Here, we are looking at survey data from an RCT involving university students in Addis Ababa. Each row correspondents to a different survey respondent, and each column represents their answers to a different question from the survey."
  },
  {
    "objectID": "workshops/lees_R_doc.html#loading-packages",
    "href": "workshops/lees_R_doc.html#loading-packages",
    "title": "Live R Coding Session",
    "section": "Loading Packages",
    "text": "Loading Packages\nPackages are an extremely important part of data analysis with R.\n\nR gives you access to thousands of “packages” that are created by users\nPackages contain bundles of code called “functions” that can execute specific tasks\nUse install.packages() to install a package and library() to load a package\n\nIn the next section, we’ll use the package dplyr to perform some data cleaning. dplyr is part of a universe of packages called tidyverse. Since this is one of the most important packages in the R ecosystem, let’s install and load it.\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nWhen you are searching online or asking ChatGPT how to perform a specific task in R, it often helps to specify that you are looking for a solution in dplyr."
  },
  {
    "objectID": "workshops/lees_R_doc.html#creating-variables",
    "href": "workshops/lees_R_doc.html#creating-variables",
    "title": "Live R Coding Session",
    "section": "Creating Variables",
    "text": "Creating Variables\nImagine we want to analyze the relationship between whether a respondent moved to to Addis Ababa to attend university and their level of political participation. However, there are two problems:\n\nWe don’t have a specific variable that measures whether or not respondents moved\nWe have many measures of participation\n\nHow can we create a variable measuring whether the respondent moved to Addis Ababa? We have a multiple-choice question asking students about what region they come from.\nLet’s start by investigating this variable.\n\n# The name of the variable in our dataframe is `q8_baseline`\ntable(dat$q8_baseline)\n\n\n                                        Addis Ababa \n                                                283 \n                                        Afar Region \n                                                  1 \n                                      Amhara Region \n                                                174 \n                           Benishangul-Gumuz Region \n                                                  5 \n                                   Dire Dawa (city) \n                                                  5 \n                                     Gambela Region \n                                                  1 \n                                      Harari Region \n                                                  4 \n                                      Oromia Region \n                                                205 \n                                              Other \n                                                 12 \n                                  Prefer not to say \n                                                 19 \n                                      Sidama Region \n                                                 35 \n                                      Somali Region \n                                                  5 \n                 South West Ethiopia Peoples Region \n                                                  6 \nSouthern Nations, Nationalities, and Peoples Region \n                                                 65 \n                                      Tigray Region \n                                                  5 \n\ndat = dat %&gt;% # this is called a \"pipe\"\n  # give our variable a better name\n  rename(home_region = q8_baseline) \n\ndat = dat %&gt;%\n  # drop respondents who report \"Prefer not to say\"\n  filter(!home_region == \"Prefer not to say\") \n\ndat = dat %&gt;%\n  # clean home region variable using `mutate()`\n  mutate(\n    # Shorten a long name to an abbreviation\n    home_region = ifelse(home_region == \"Southern Nations, Nationalities, and Peoples Region\", \"SNNPR\", home_region),\n    # remove the word \"Region\" from every observation of this column\n    home_region = str_remove(home_region, \" Region\"),\n    home_region = str_remove(home_region, \" \\\\(city\\\\)\")\n    )\n\n# Check if it worked\ntable(dat$home_region)\n\n\n                Addis Ababa                        Afar \n                        283                           1 \n                     Amhara           Benishangul-Gumuz \n                        174                           5 \n                  Dire Dawa                     Gambela \n                          5                           1 \n                     Harari                      Oromia \n                          4                         205 \n                      Other                      Sidama \n                         12                          35 \n                      SNNPR                      Somali \n                         65                           5 \nSouth West Ethiopia Peoples                      Tigray \n                          6                           5 \n\n\n\n# Chain these all together for more concise code\ndat = read_csv(\"https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv\" ) %&gt;%\n  # give our variable a better name\n  rename(home_region = q8_baseline) %&gt;%\n  # drop respondents who report \"Prefer not to say\"\n  filter(!home_region == \"Prefer not to say\") %&gt;%\n  # clean home region variable\n  mutate(\n    # Shorten a long name to an abbreviation\n    home_region = case_when(\n      home_region == \"Southern Nations, Nationalities, and Peoples Region\" ~ \"SNNPR\",\n      home_region == \"South West Ethiopia Peoples Region\" ~ \"SWEPR\",\n      TRUE ~ home_region\n    ),\n    # remove the word \"Region\" from every observation of this column\n    home_region = str_remove(home_region, \" Region| \\\\(city\\\\)\")\n    )\n\n# Check if it worked\ntable(dat$home_region)\n\n\n      Addis Ababa              Afar            Amhara Benishangul-Gumuz \n              283                 1               174                 5 \n        Dire Dawa           Gambela            Harari            Oromia \n                5                 1                 4               205 \n            Other            Sidama             SNNPR            Somali \n               12                35                65                 5 \n            SWEPR            Tigray \n                6                 5 \n\n\nNow that we’ve cleaned-up the names, we want to create a variable that tells us whether or not each respondent is originally from Addis Ababa. This will let us measure whether or not they moved to Addis in order to attend college.\n\n# Creating a measure of whether a respondent moved to Addis Ababa in order to at independent variable\ndat = dat %&gt;% \n  mutate(\n    moved = case_when(\n      home_region == \"Addis Ababa\" ~ 0,\n      TRUE ~ 1\n    ) \n  )\n\ntable(dat$moved, dat$home_region)\n\n   \n    Addis Ababa Afar Amhara Benishangul-Gumuz Dire Dawa Gambela Harari Oromia\n  0         283    0      0                 0         0       0      0      0\n  1           0    1    174                 5         5       1      4    205\n   \n    Other Sidama SNNPR Somali SWEPR Tigray\n  0     0      0     0      0     0      0\n  1    12     35    65      5     6      5\n\n\nNow, we need to create our second variable measuring levels of political participation. Remember, the challenge is that we have multiple measures of participation. Let’s start with two measures:\n\nNumber of times you’ve contacted gov’t official q13_4_1\nNumber of times you’ve signed a petition q13_5_1\n\n\n# Check out the distribution of our variables\ndat %&gt;% select(q13_4_1, q13_5_1) %&gt;% head(5)\n\n# A tibble: 5 × 2\n  q13_4_1 q13_5_1\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       0       0\n2       3       0\n3       0       0\n4       0       0\n5       0       2\n\nmean(dat$q13_4_1)\n\n[1] NA\n\nhist(dat$q13_5_1)\n\n\n\n\n\n\n\n# Create a single measuree\ndat = dat %&gt;%\n  mutate(\n      add_participation = q13_4_1 + q13_5_1\n\n  )\n\nhist(dat$add_participation)\n\n\n\n\n\n\n\n# how do investigate NA values?\n\nOne thing we need to be careful with are NA values. We need to think carefully about why NA values are in our data and how to handle them appropriately.\nQuestion: Thinking about the data that you have worked with, what are the most common sources of NA values?\n\n## Find our two participation measures\nadd_ecols = grep(\"q13_4_1$|q13_5_1$\", names(dat), value = T)\n\ndat = dat %&gt;%\n  mutate(add_participation =  rowSums(across(add_ecols) ) )\n  #mutate(add_participation_end =  rowSums(across(add_ecols), na.rm = T) )\n\n# dat = dat %&gt;%\n#   mutate(home_region = na_if(home_region, \"Prefer not to say\")) %&gt;%"
  },
  {
    "objectID": "workshops/lees_R_doc.html#merging-datasets",
    "href": "workshops/lees_R_doc.html#merging-datasets",
    "title": "Live R Coding Session",
    "section": "Merging Datasets",
    "text": "Merging Datasets\nOften, data analysis projects will require you to use variables from more than one dataset. This will require you to combine separate datasets into a single dataset in R, which is called merging. A merge uses an identifier (administrative unit names, respondent IDs, etc.) that is present in both datasets to combine them into one.\nQuestion: Thinking about the data that you have worked with, have you ever had to merge multiple datasets? What was the data? How did you do it?\n\n## Here we are selecting the unit-identifier (response_id) and the two variables we have created and saving them to a separate dataframe\nnew_vars = dat %&gt;%\n  select(response_id, moved, add_participation)\n\n## After you do some cleaning or create new variables, you may want to save that dataframe for future use\n# write.csv(new_dat, here::here(\"workshops/aau_survey/new_vars.csv\"))\n\n## Drop the new variables from our dataframe so that we can merge them back in\ndat = dat %&gt;%\n  select( -moved, -add_participation)\n\n## Check to make sure the merge will work as expected\n# nrow(dat)\n# nrow(new_vars)\n# dat$response_id[! dat$response_id %in% new_vars$response_id]\n\n## Use dplyr's `full_join` function  to merge datasets\ndat = dat %&gt;%\n  full_join(., new_vars)"
  },
  {
    "objectID": "workshops/lees_R_doc.html#tables",
    "href": "workshops/lees_R_doc.html#tables",
    "title": "Live R Coding Session",
    "section": "Tables",
    "text": "Tables\n\n#install.packages(\"gt\")\nlibrary(gt)\n\ndesciptives = dat %&gt;%\n  group_by(moved) %&gt;%\n  summarise(observations = n(), \n            share = observations / nrow(dat),\n            mean = mean(add_participation, na.rm = T)) %&gt;%\n  mutate(moved = case_when(moved == 0 ~ \"No\",\n                           moved == 1 ~ \"Yes\"),\n         share = share *100)\n\ngt(desciptives) %&gt;%\n  tab_header(\n    title = \"Moving to University and Political Participation\",\n  ) %&gt;%\n  fmt_number(\n    columns = 3:4,\n    decimals = 2,\n    use_seps = FALSE\n  ) %&gt;%\n  cols_label(\n    moved = md(\"Moved to&lt;br&gt;Addis\"),\n    observations = md(\"Respondents&lt;br&gt;(count)\"),\n    share = md(\"Respondents&lt;br&gt;(%)\"),\n    mean = md(\"Participation&lt;br&gt;(mean score)\")\n  ) %&gt;%\n  gtsave(filename = \"tab_1.png\")"
  },
  {
    "objectID": "workshops/lees_R_doc.html#figures",
    "href": "workshops/lees_R_doc.html#figures",
    "title": "Live R Coding Session",
    "section": "Figures",
    "text": "Figures\nThere are many ways to create figures in R, but ggplot is the most popular.\n\nggplot(dat, aes(add_participation)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  ggplot(aes(add_participation)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  ggplot(aes(x = add_participation, fill = q3_baseline)) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  mutate(moved = fct_rev(as.factor(moved))) %&gt;%\n  ggplot(aes(x = add_participation, fill = moved)) +\n  geom_histogram(aes(y = (..count..) / sum(..count..) * 100),\n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(y = \"Percentage\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.position = c(.8, .8))\n\n\n\n\n\n\n\n\nYou can do tremendous amounts of customization with ggplot to create extremely informative and professional plots.\n\n## demographics\ndemos = dat %&gt;%\n  drop_na(q3_baseline) %&gt;% \n  select(`Respondent Gender` = q3_baseline, `Work as a student?` = q4_baseline, \n         `Rural or urban?` = q5_baseline, `Financial support from family?` = q6_baseline, \n         `Home region` = home_region,\n         `Student Year` = class_year) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name, value) %&gt;% \n  tally() %&gt;% \n  mutate(pct = n/sum(n)) %&gt;% \n  top_n(n = 5, wt = pct)\n\ndemos$name = factor(demos$name, levels = c('Home region', 'Rural or urban?', 'Student Year', 'Respondent Gender', 'Work as a student?', 'Financial support from family?'))\n\ndemos &lt;- demos %&gt;%\n  group_by(name) %&gt;%\n  mutate(total_n = sum(n)) %&gt;%\n  ungroup()\n\nggplot(demos , aes(y = value, x = pct)) + \n  geom_col(fill = \"grey\") + \n  facet_wrap( ~name, scales = \"free\") + \n  scale_y_discrete(labels = scales::label_wrap(30)) + \n  hrbrthemes::scale_x_percent(limits = c(0, 1)) + \n  labs(x = \"Percent of respondents to the survey\", y = NULL, \n       title = \"Demographic characteristics of baseline respondents\", \n       subtitle = glue::glue(\"Number of respondents = {scales::comma(nrow(df))}\"), \n       caption = \"Note: home region only displays top five categories by size.\") +\n  geom_text(\n    aes(\n      label = glue::glue(\"n = {total_n}\")\n    ), \n    x = 0.92, y = -Inf, \n    vjust = -1, hjust = 1, \n    inherit.aes = FALSE\n  )\n\n\n\n\n\n\n\n#ggsave(\"/home/jeremy/Downloads/demographics.png\")"
  },
  {
    "objectID": "workshops/lees_R_doc.html#averaged-z-scores",
    "href": "workshops/lees_R_doc.html#averaged-z-scores",
    "title": "Live R Coding Session",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\n\n## Find participation measures that are based on likert\n# baseline\nbcols = grep(\"^q13_.*_baseline$\", names(dat), value = T)\ndat[, paste0(bcols, \"_st\")] = dat[, bcols]\nbcols = paste0(bcols,\"_st\")\n\n# endline\necols = grep(\"^q13_[1-7]_\\\\d$\", names(dat), value = T)\ndat[, paste0(ecols, \"_st\")] = dat[, ecols]\necols = paste0(ecols,\"_st\")\n\n\n# Create treatment variable\ndat = dat %&gt;% mutate(moved = case_when(home_region == \"Addis Ababa\" ~ 0, TRUE ~ 1) )\n\n# clean q13_\nlevels = c(\"Never\", \"Once or Twice\", \"More than twice\", \"More than 5 times\", \n           \"More than 10 times\")\ndat = dat %&gt;% \n  mutate(across(c(bcols), \n                .fns = ~ factor(.x, levels = levels)))\n\n# Create z-score function from Kling, Liberman, and Katz (2007)\nz_score = function(x, y){\n  # calculate mean and sd of control group\n  c_mean = mean( as.numeric( unlist(x[, y])) , na.rm = T)\n  c_sd = sd( as.numeric( unlist(x[, y])) , na.rm = T)\n  # subtract control group mean; divide by control group SD\n  ( as.numeric(x[, y, drop = TRUE]) - c_mean) / c_sd\n}\n\n# calculate z-scores\nfor (i in c(bcols, ecols)) {\n  dat[,i] = z_score(dat, i)\n}\n\ndat = dat %&gt;% \n  rowwise() %&gt;% \n  mutate( z_participation_end = mean(c_across(all_of(bcols)), na.rm = TRUE)) %&gt;% \n  mutate( z_participation_base = mean(c_across(all_of(ecols)), na.rm = TRUE)) %&gt;%\n  ungroup()\n\n\nregd = dat %&gt;% select(z_participation_end, z_participation_base, moved, response_id ) %&gt;%\n  pivot_longer(cols = c(z_participation_end, z_participation_base),\n               names_to = \"time\",\n               values_to = \"z_participation\") %&gt;%\n  mutate(time = case_when(time == \"z_participation_end\" ~ 1,\n                          TRUE ~ 0))\n\nmodels &lt;- list()\nmodels[['Bivariate']] = lm(z_participation ~ moved, regd)\nmodels[['Multivariate']] = lm(z_participation ~ moved + time, regd)\nmodels[['Interaction']] = lm(z_participation ~ moved + time + moved*time, regd)\n\nmodelsummary(\n  models,\n  estimate  = \"{estimate}{stars} ({std.error})\",\n             statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')\n\n\n\n\n\nBivariate\nMultivariate\nInteraction\n\n\n\n\n(Intercept)\n−0.159*** (0.026)\n−0.156*** (0.030)\n−0.156*** (0.037)\n\n\nmoved\n0.247*** (0.032)\n0.247*** (0.032)\n0.247*** (0.046)\n\n\ntime\n\n−0.004 (0.031)\n−0.005 (0.052)\n\n\nmoved × time\n\n\n0.001 (0.065)\n\n\nNum.Obs.\n1597\n1597\n1597\n\n\nR2 Adj.\n0.035\n0.034\n0.034"
  },
  {
    "objectID": "slides/lees-sampling.html",
    "href": "slides/lees-sampling.html",
    "title": "Review of Sampling and Population Characteristics",
    "section": "",
    "text": "This activity draws on two sources:\n\nChapter 3 in Data Analysis for Social Science: A Friendly and Practical Introduction (DSS)\n\ninteractive visualization: sampling\n\nChapter 7 in Modern Dive\n\n\nThis tutorial draws heavily on two resources. An online copy of the DSS textbook chapter can be purchased at the Princeton Press website. Modern Dive is a free, open-source textbook created with R + Markdown and hosted in Github pages."
  },
  {
    "objectID": "slides/lees-sampling.html#resources",
    "href": "slides/lees-sampling.html#resources",
    "title": "Review of Sampling and Population Characteristics",
    "section": "",
    "text": "This activity draws on two sources:\n\nChapter 3 in Data Analysis for Social Science: A Friendly and Practical Introduction (DSS)\n\ninteractive visualization: sampling\n\nChapter 7 in Modern Dive\n\n\nThis tutorial draws heavily on two resources. An online copy of the DSS textbook chapter can be purchased at the Princeton Press website. Modern Dive is a free, open-source textbook created with R + Markdown and hosted in Github pages."
  },
  {
    "objectID": "slides/lees-sampling.html#inferring-population-characteristics",
    "href": "slides/lees-sampling.html#inferring-population-characteristics",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Inferring Population Characteristics",
    "text": "Inferring Population Characteristics\n\nWe often want to describe a population of interest\n\nWhat is the average number of years of education completed by a country’s citizens?\nWhat share the teachers in a district receive their salary on time?\n\nWe usually can’t collect data on the entire population\nSampling allows us to estimate population characteristics from a subset of the population\nRandom sampling makes it most likely that our sample is representative of the population\n\n\nOne common goal of research is to make inferences about the characteristics of a broader population. - For example, we might want to know the average number of years of education completed by a country’s citizens - Or we might want to know what share of the teachers in a district received salary payments on-time. - However, it is extremely rare that we can collect data on the entire population. Collecting data from every member of the population is extremely expensive and logistically very difficult. This is why most countries only collect census data on their citizens once every 10 years. - Fortunately, this doesn’t mean that we can’t get good answers to these questions. Sampling allows us to estimate these quantities of interest by collecting data on a subset of the population of interest. - The method of sampling and the size of our sample determine how reliable this estimate will be. - Random sampling allows us to assume that the units in our sample (people, teachers, etc) will be a good representation of the full population"
  },
  {
    "objectID": "slides/lees-sampling.html#sample-size-and-variance",
    "href": "slides/lees-sampling.html#sample-size-and-variance",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Sample Size and Variance",
    "text": "Sample Size and Variance\n\nRandom samples are used to calculate “point estimates” of population parameters (mean, median, proportion, etc.)\nLarge samples have a higher chance of resembling the population, and allow us to more confident in our estimates\n\nCheck out this interactive visualization\nObserve how increases in sample size lead to a sample that is more and more similar to the population\n\nSampling variation means that any specific sample will have a parameter value that is above or below the value in the population\n\nCalculate the standard error to quantify how far your estimate is likely to be from the true parameter value\n\n\n\nRandom samples are used to calculate “point estimates” of parameters (mean, median, proportion, etc.). With a large sample size, it is more likely that your sample will resemble the broader population on the parameter value you are trying to estimate. In a new tab, open this interactive visualization of how sample size leads to better estimates of population parameters. Follow the instructions to watch how increases in sample size lead to a sample that is more and more similar to the population. Random samples exhibit natural “sampling variation” that leads to any specific sample to have a parameter value that is above or below the true value of the parameter in the population that you are trying to infer from the sample. You can calculate an approximation of the standard error to quantify how far your estimate is likely to be from the true parameter value based on the amount of variation in the data and the sample size."
  },
  {
    "objectID": "slides/lees-sampling.html#getting-a-representative-samples",
    "href": "slides/lees-sampling.html#getting-a-representative-samples",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Getting a Representative Samples",
    "text": "Getting a Representative Samples\n\nSampling frame: Comprehensive list of units in the population\n\nPopulation census, administrative records\nCreate yourself on-the-ground\n\nMulti-stage sampling\n\nPopulations are often nested within multiple units (citizens are in neighborhoods, citizens, districts)\nRandomly sampling which higher-level units to target can make randomly selecting individual units much easier\n\n\n\nIn order to draw a random sample from a population, you must have a “sampling frame”. A sampling frame is a comprehensive list of every unit in the population. If your population of interest is the citizens of a country, you may be able to get a sampling frame through a census or through government records. If a recent census or administrative records aren’t available, you might need to construct a sampling frame yourself. - In development research, we often mix these two approaches in multi-stage sampling. For example, to collect a random sample of citizens in a specific city to complete a survey, you might first randomly sample neighborhoods from government records and then work with community leaders in each neighborhood to create a list of every household from which you can draw a random sample."
  },
  {
    "objectID": "slides/lees-sampling.html#sampling-strategies",
    "href": "slides/lees-sampling.html#sampling-strategies",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Sampling Strategies",
    "text": "Sampling Strategies\n\nProbability sampling strategies:\n\nSimple random sampling\nStratified sampling\n\nAssume that unit and item non-response are effectively random\n\n\nProbability sampling is a method used in statistical research to select a sample of elements from a larger population in such a way that each element has a known, non-zero chance of being selected. This approach allows the results from the sample to be generalized to the larger population, which is a key aspect of statistical inference. There are multiple approaches to random sampling, which we will discuss below. However, they all assume that your data collection is perfect, meaning that you are able to successfull collect data from every unit that is selected to be included in the sample. In the real world, this is almost never the case. We will return to issues of non-response at the end of this activity."
  },
  {
    "objectID": "slides/lees-sampling.html#simple-random-sampling",
    "href": "slides/lees-sampling.html#simple-random-sampling",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nEach unit within the population has the same probability of being chosen\nCan be logistically difficult\nMay lead to the exclusion of small but important groups within the population\n\n\nSimple Random Sampling is the most straightforward approach. In SRS, each unit within the population has the exact some probability of being included in your sample. However, it is often logistically difficult to collect data in this manner. For example, if you randomly select citizens from the entire population to answer an in-person survey, you will probably need to send an enumerator to a different part of the country for every citizen (unless you randomly sample multiple respondents that live near one another by chance). Also, SRS can miss small but important groups within the population. If you want to make inferences about parameter values for subgroups within the population, such as citizens living in a specific region of the country, you may need a more sophisticated approach to random sampling."
  },
  {
    "objectID": "slides/lees-sampling.html#stratified-sampling",
    "href": "slides/lees-sampling.html#stratified-sampling",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Stratified sampling",
    "text": "Stratified sampling\n\n\nAllows for more precise estimates of small group characteristics\nDivide the entire population into homogeneous strata\n\nEx. Gender, geographic location, etc.\n\nRandomly sample units from each stratum\n\nDecreases risk that certain strata will be absent or under-represented\nNumber of units sampled can be constant across strata or proportional to the strata’s size\nIf units are not sampled according to their proportion of the population, estimating population parameters requires weighting\n\n\n\n\nStratified sampling provides a more nuanced approach to random sampling. This approach can help to ensure that we collect enough data about various groups within the population so that we can make inferences about those groups. Here, we begin by dividing the population into groups of similar units, for example by gender or geographic location. We then randomly sample a certain number of units from within those strata. This decreases the risk that members of certain strata will be absent or under-represented in your sample. The number of units sampled from each strata can be constant across strata (will will lead small groups to be over-represented or over-sampled compared to their share of the broader population) or proportional to their share of the broader population. If the number of units from each strata does not correspond to that strata’s share of the population, you must apply weights when estimating the value of population parameters. It is important to note that, in order to perform stratified sampling, you must have data on which strata each unit belongs to in your sample frame."
  },
  {
    "objectID": "slides/lees-sampling.html#non-response-bias",
    "href": "slides/lees-sampling.html#non-response-bias",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Non-Response Bias",
    "text": "Non-Response Bias\n\nUnit non-response (no data)\n\nWhen you cannot get data on certain units that were selected to be included in your sample\nEx. A citizen declines to take your survey\n\nItem non-response (missing data)\n\nWhen you cannot get data on all relevant characteristics of a unit that is included in your sample\nA survey respondent chooses not to answer specific questions\n\nSelection into sample can bias estimates, induce spurious relationships\n\n\nRandom sampling ensures that the statistically most likely outcome is that our sample is representative of the broader population - However, non-response can introduce bias that makes the sample less representative - Random sampling works because no characteristics of units makes them more or less likely to be included in the sample - Statistically, the most likely outcome is that the sample resembles the population at large - When citizens refuse to take a survey (unit non-response) or answer certain questions (item non-response), or administrators at a certain school are unwilling to provide data on teacher payments (unit non-response) or have incomplete records for some teachers (item non-response), this can create bias. Citizens with more years of education may be less likely to respond, or schools with worse records of on-time payments might be less able to provide accurate data. - If the characteristics that make units select whether to be included are also correlated with the population characteristic that you want to estimate, this creates bias. - This introduces what we call “selection bias,” meaning that sampling is no longer random because certain units select whether to be included in the sample - All data collection suffers from selection bias, so it is important to think carefully about how the specific types of bias you encountered might affect your estimates of population characteristics."
  },
  {
    "objectID": "slides/lees-sampling.html#dealing-with-non-response",
    "href": "slides/lees-sampling.html#dealing-with-non-response",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Dealing with Non-Response",
    "text": "Dealing with Non-Response\n\nUnit non-response (no data)\n\nIncentives\nFancy weighting\n\nItem non-response (missing data)\n\nSelf-administration\n\nAlways look at the distribution of non-response before making inferences with your data\n\n\nThere are many strategies to reduce the threats to inference posed by non-response. To reduce unit non-response, you can offer incentives or apply weighting to try to account for higher levels of non-response among certain types of units. To reduce item non-response, you can allow individual surveys to be self-administered or emphasize anonymity to reduce hesitation to sharing information. However, none of these strategies are guaranteed to work, so it is always important to investigate patterns in both unit and item non-response in your sample. Investigate whether sampled units in certain areas or strata were less likely to provide data than others, or whether certain types of units were less likely to provide certain types of data or information."
  },
  {
    "objectID": "slides/lees-sampling.html#quiz-questions",
    "href": "slides/lees-sampling.html#quiz-questions",
    "title": "Review of Sampling and Population Characteristics",
    "section": "Quiz Questions",
    "text": "Quiz Questions\n\nWhich method would be a more reliable way to estimate the proportion of citizens in your country that support a specific policy?\n\nAsking 100 citizens selected randomly from the entire population\nAsking 1,000 citizens that respond to a poll posted on social media\n\nWhat is selection bias?\n\nWhen the population characteristics that you want to estimate differ systematically across specific groups within the populatioon\nWhen characteristics that make units more or less likely to provide data are correlated with the population characteristics that you want to estimate\n\n\n\nAnswers: - Option 1 - Option 2"
  },
  {
    "objectID": "slides/lees-R.html",
    "href": "slides/lees-R.html",
    "title": "R and RStudio",
    "section": "",
    "text": "This activity draws on Chapter 1 in Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\n\n\n\n\n\nIn the quantitative methods tutorial, we will use R and RStudio to illustrate basic statistical concepts\nR is a programming language and free computing environment for statistical computing and graphics\nRStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools\nThink of R as a car’s engine and RStudio as the dashboard\n\n\nR is a programming language and free computing environment for statistical computing and graphics. It can be runs on a wide variety of operating systems, including Linux, Windows, and MacOS. RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools.\nYou can think of R as a car’s engine and RStudio as the dashboard. In the same way that having access to a speedometer and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\nR and RStudio are entirely free and updated on a regular basis, making them much more accessible and widely used than paid alternatives. You can conduct nearly any analysis using R, ranging from calculating the average of a variable to creating maps and conducting spatial analysis to conducting the analysis for an impact evaluation.\n\n\n\n\n\nDownload and install R by going to https://cloud.r-project.org/\n\nWindows: Click on “Download R for Windows”, then click on “base”, then click on the Download link\nmacOS user: Click on “Download R for macOS”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number\nLinux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup\n\n\n\nIf you don’t have R and RStudio already installed, that will be the first step. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio.\n\n\n\n\n\nDownload and install RStudio at https://www.rstudio.com/products/rstudio/download/\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page\nClick on the download link corresponding to your computer’s operating system\n\n\n\nOnce you have installed R, you can now install RStudio.\n\n\n\n\n\nUsing R by opening RStudio\n\n\n\n\n\n\nAfter you open RStudio, you should see something like this\n\n\n\nNote the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this tutorial, you’ll come to learn what purpose each of these panes serves.\n\n\n\n\n\nCustomize the appearance of your R IDE by going to Tools -&gt; Global Options -&gt; Appearance\n\n\n\n\n\n\nRstudio contains 4 panes: source, console, environment, and viewer\nThe left pane is the source where you write code\nTry typing 2 + 2 into the source panel and hitting ENTER\n\nThe console panel pop-up in the bottom-left and return a value of 4\n\n\n[INSERT SCREEN RECORDING]\n\n\n\n\nThe top-right includes the Environment panel\n\nThis contains information about any objects that you load into your environment\nType sum = 2 + 2 into the source and hit ENTER\nThe object sum should appear in your environment\n\nThe bottom-right includes the Viewer panel, where you will be able to see graphics that you generate\n\n\n\n\n\nRunning code: telling R to perform an act by giving it commands in the source or console\nObjects: where values are saved in R. On the last slide, you created the object sum.\nData types:\n\nIntegers are whole numbers like -1, 0, 2, 4092\nDoubles are integers with decimal values like -24.932 and 0.8\nLogicals are either TRUE or FALSE\nCharacters are text or strings like \"hello world\" and \"welcome to R\"\n\n\n\n\n\n\n\n\n\nVectors are a series of values. These are created using the c() function. For example, c(6, 11, 13, 31) creates a four element vector of integers.\nFactors are a group of characters/strings with a fixed number of unique values\nData frames are objects where the rows correspond to observations and the columns correspond to variables that describe the observations\n\n\n\n\n\nConditionals:\n\nTesting for equality in R is done using ==. For example, 2 + 1 == 3 will return TRUE\nBoolean algebra: Operators such as &lt; (less than), &lt;= (less than or equal), and !=(not equal to). For example, 3 + 5 &lt;= 1 will return FALSE\nLogical operators: & represent “and” while | represents “or.” For example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since both clauses are not TRUE\n\n\n\n\n\n\nR gives you accesss to thousands of “packages” that are created by users\nPackages contain datasets and bundles of code called “functions” that can execute specific tasks\nUse install.packages() to install a package\n\nInsert the name of the package contained in quotation marks\nStart by installing the dplyr package\n\n\n\n\n\n\nLoad data into your environment by “reading-in” a spreadsheet\nSpreadsheets should be saved as a .csv file\nUse read.csv() to pull data from a spreadsheet on your harddrive into your R/RStudio environment\n\nWithin the parentheses, add the full file pathway where the .csv file is stored\n\n\n[INSERT SCREEN RECORDING]\n\n\n\n\nWhen you run code, R often provides feedback in the console\nThere are 3 main types of feedback that appear in RED\n\nErrors: When the RED text is prefaced with “Error in…”, your code will not work. The error message will try to explain the problem.\nWarnings: When the red text is prefaced with “Warning:”, your code will still work, but there still might be a problem. The warning message will try to explain the problem.\nMessages: When the red text doesn’t start with “Error” or “Warning”, it’s just a friendly message"
  },
  {
    "objectID": "slides/lees-R.html#resources",
    "href": "slides/lees-R.html#resources",
    "title": "R and RStudio",
    "section": "",
    "text": "This activity draws on Chapter 1 in Statistical Inference via Data Science: A ModernDive into R and the Tidyverse"
  },
  {
    "objectID": "slides/lees-R.html#what-are-r-and-rstudio",
    "href": "slides/lees-R.html#what-are-r-and-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "In the quantitative methods tutorial, we will use R and RStudio to illustrate basic statistical concepts\nR is a programming language and free computing environment for statistical computing and graphics\nRStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools\nThink of R as a car’s engine and RStudio as the dashboard\n\n\nR is a programming language and free computing environment for statistical computing and graphics. It can be runs on a wide variety of operating systems, including Linux, Windows, and MacOS. RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools.\nYou can think of R as a car’s engine and RStudio as the dashboard. In the same way that having access to a speedometer and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\nR and RStudio are entirely free and updated on a regular basis, making them much more accessible and widely used than paid alternatives. You can conduct nearly any analysis using R, ranging from calculating the average of a variable to creating maps and conducting spatial analysis to conducting the analysis for an impact evaluation."
  },
  {
    "objectID": "slides/lees-R.html#installing-r",
    "href": "slides/lees-R.html#installing-r",
    "title": "R and RStudio",
    "section": "",
    "text": "Download and install R by going to https://cloud.r-project.org/\n\nWindows: Click on “Download R for Windows”, then click on “base”, then click on the Download link\nmacOS user: Click on “Download R for macOS”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number\nLinux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup\n\n\n\nIf you don’t have R and RStudio already installed, that will be the first step. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio."
  },
  {
    "objectID": "slides/lees-R.html#installing-rstudio",
    "href": "slides/lees-R.html#installing-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "Download and install RStudio at https://www.rstudio.com/products/rstudio/download/\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page\nClick on the download link corresponding to your computer’s operating system\n\n\n\nOnce you have installed R, you can now install RStudio."
  },
  {
    "objectID": "slides/lees-R.html#using-r-via-rstudio",
    "href": "slides/lees-R.html#using-r-via-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "Using R by opening RStudio"
  },
  {
    "objectID": "slides/lees-R.html#using-r-via-rstudio-1",
    "href": "slides/lees-R.html#using-r-via-rstudio-1",
    "title": "R and RStudio",
    "section": "",
    "text": "After you open RStudio, you should see something like this\n\n\n\nNote the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this tutorial, you’ll come to learn what purpose each of these panes serves."
  },
  {
    "objectID": "slides/lees-R.html#peronalization",
    "href": "slides/lees-R.html#peronalization",
    "title": "R and RStudio",
    "section": "",
    "text": "Customize the appearance of your R IDE by going to Tools -&gt; Global Options -&gt; Appearance"
  },
  {
    "objectID": "slides/lees-R.html#layout",
    "href": "slides/lees-R.html#layout",
    "title": "R and RStudio",
    "section": "",
    "text": "Rstudio contains 4 panes: source, console, environment, and viewer\nThe left pane is the source where you write code\nTry typing 2 + 2 into the source panel and hitting ENTER\n\nThe console panel pop-up in the bottom-left and return a value of 4\n\n\n[INSERT SCREEN RECORDING]"
  },
  {
    "objectID": "slides/lees-R.html#layout-1",
    "href": "slides/lees-R.html#layout-1",
    "title": "R and RStudio",
    "section": "",
    "text": "The top-right includes the Environment panel\n\nThis contains information about any objects that you load into your environment\nType sum = 2 + 2 into the source and hit ENTER\nThe object sum should appear in your environment\n\nThe bottom-right includes the Viewer panel, where you will be able to see graphics that you generate"
  },
  {
    "objectID": "slides/lees-R.html#basic-programming-concepts",
    "href": "slides/lees-R.html#basic-programming-concepts",
    "title": "R and RStudio",
    "section": "",
    "text": "Running code: telling R to perform an act by giving it commands in the source or console\nObjects: where values are saved in R. On the last slide, you created the object sum.\nData types:\n\nIntegers are whole numbers like -1, 0, 2, 4092\nDoubles are integers with decimal values like -24.932 and 0.8\nLogicals are either TRUE or FALSE\nCharacters are text or strings like \"hello world\" and \"welcome to R\""
  },
  {
    "objectID": "slides/lees-R.html#basic-programming-concepts-1",
    "href": "slides/lees-R.html#basic-programming-concepts-1",
    "title": "R and RStudio",
    "section": "",
    "text": "Vectors are a series of values. These are created using the c() function. For example, c(6, 11, 13, 31) creates a four element vector of integers.\nFactors are a group of characters/strings with a fixed number of unique values\nData frames are objects where the rows correspond to observations and the columns correspond to variables that describe the observations"
  },
  {
    "objectID": "slides/lees-R.html#basic-programming-concepts-2",
    "href": "slides/lees-R.html#basic-programming-concepts-2",
    "title": "R and RStudio",
    "section": "",
    "text": "Conditionals:\n\nTesting for equality in R is done using ==. For example, 2 + 1 == 3 will return TRUE\nBoolean algebra: Operators such as &lt; (less than), &lt;= (less than or equal), and !=(not equal to). For example, 3 + 5 &lt;= 1 will return FALSE\nLogical operators: & represent “and” while | represents “or.” For example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since both clauses are not TRUE"
  },
  {
    "objectID": "slides/lees-R.html#loading-packages",
    "href": "slides/lees-R.html#loading-packages",
    "title": "R and RStudio",
    "section": "",
    "text": "R gives you accesss to thousands of “packages” that are created by users\nPackages contain datasets and bundles of code called “functions” that can execute specific tasks\nUse install.packages() to install a package\n\nInsert the name of the package contained in quotation marks\nStart by installing the dplyr package"
  },
  {
    "objectID": "slides/lees-R.html#loading-data",
    "href": "slides/lees-R.html#loading-data",
    "title": "R and RStudio",
    "section": "",
    "text": "Load data into your environment by “reading-in” a spreadsheet\nSpreadsheets should be saved as a .csv file\nUse read.csv() to pull data from a spreadsheet on your harddrive into your R/RStudio environment\n\nWithin the parentheses, add the full file pathway where the .csv file is stored\n\n\n[INSERT SCREEN RECORDING]"
  },
  {
    "objectID": "slides/lees-R.html#errors-warnings-and-messages",
    "href": "slides/lees-R.html#errors-warnings-and-messages",
    "title": "R and RStudio",
    "section": "",
    "text": "When you run code, R often provides feedback in the console\nThere are 3 main types of feedback that appear in RED\n\nErrors: When the RED text is prefaced with “Error in…”, your code will not work. The error message will try to explain the problem.\nWarnings: When the red text is prefaced with “Warning:”, your code will still work, but there still might be a problem. The warning message will try to explain the problem.\nMessages: When the red text doesn’t start with “Error” or “Warning”, it’s just a friendly message"
  },
  {
    "objectID": "assignments/install-R.html",
    "href": "assignments/install-R.html",
    "title": "R and RStudio",
    "section": "",
    "text": "This activity draws on Chapter 1 in Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\n\n\n\n\n\nIn the quantitative methods tutorial, we will use R and RStudio to illustrate basic statistical concepts\nR is a programming language and free computing environment for statistical computing and graphics\nRStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools\nThink of R as a car’s engine and RStudio as the dashboard\n\n\nR is a programming language and free computing environment for statistical computing and graphics. It can be runs on a wide variety of operating systems, including Linux, Windows, and MacOS. RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools.\nYou can think of R as a car’s engine and RStudio as the dashboard. In the same way that having access to a speedometer and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\nR and RStudio are entirely free and updated on a regular basis, making them much more accessible and widely used than paid alternatives. You can conduct nearly any analysis using R, ranging from calculating the average of a variable to creating maps and conducting spatial analysis to conducting the analysis for an impact evaluation.\n\n\n\n\n\nDownload and install R by going to https://cloud.r-project.org/\n\nWindows: Click on “Download R for Windows”, then click on “base”, then click on the Download link\nmacOS user: Click on “Download R for macOS”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number\nLinux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup\n\n\n\nIf you don’t have R and RStudio already installed, that will be the first step. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio.\n\n\n\n\n\nDownload and install RStudio at https://www.rstudio.com/products/rstudio/download/\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page\nClick on the download link corresponding to your computer’s operating system\n\n\n\nOnce you have installed R, you can now install RStudio.\n\n\n\n\n\nUsing R by opening RStudio\n\n\n\n\n\n\nAfter you open RStudio, you should see something like this\n\n\n\nNote the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this tutorial, you’ll come to learn what purpose each of these panes serves.\n\n\n\n\n\nCustomize the appearance of your R IDE by going to Tools -&gt; Global Options -&gt; Appearance"
  },
  {
    "objectID": "assignments/install-R.html#resources",
    "href": "assignments/install-R.html#resources",
    "title": "R and RStudio",
    "section": "",
    "text": "This activity draws on Chapter 1 in Statistical Inference via Data Science: A ModernDive into R and the Tidyverse"
  },
  {
    "objectID": "assignments/install-R.html#what-are-r-and-rstudio",
    "href": "assignments/install-R.html#what-are-r-and-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "In the quantitative methods tutorial, we will use R and RStudio to illustrate basic statistical concepts\nR is a programming language and free computing environment for statistical computing and graphics\nRStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools\nThink of R as a car’s engine and RStudio as the dashboard\n\n\nR is a programming language and free computing environment for statistical computing and graphics. It can be runs on a wide variety of operating systems, including Linux, Windows, and MacOS. RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools.\nYou can think of R as a car’s engine and RStudio as the dashboard. In the same way that having access to a speedometer and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\nR and RStudio are entirely free and updated on a regular basis, making them much more accessible and widely used than paid alternatives. You can conduct nearly any analysis using R, ranging from calculating the average of a variable to creating maps and conducting spatial analysis to conducting the analysis for an impact evaluation."
  },
  {
    "objectID": "assignments/install-R.html#installing-r",
    "href": "assignments/install-R.html#installing-r",
    "title": "R and RStudio",
    "section": "",
    "text": "Download and install R by going to https://cloud.r-project.org/\n\nWindows: Click on “Download R for Windows”, then click on “base”, then click on the Download link\nmacOS user: Click on “Download R for macOS”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number\nLinux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup\n\n\n\nIf you don’t have R and RStudio already installed, that will be the first step. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio."
  },
  {
    "objectID": "assignments/install-R.html#installing-rstudio",
    "href": "assignments/install-R.html#installing-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "Download and install RStudio at https://www.rstudio.com/products/rstudio/download/\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page\nClick on the download link corresponding to your computer’s operating system\n\n\n\nOnce you have installed R, you can now install RStudio."
  },
  {
    "objectID": "assignments/install-R.html#using-r-via-rstudio",
    "href": "assignments/install-R.html#using-r-via-rstudio",
    "title": "R and RStudio",
    "section": "",
    "text": "Using R by opening RStudio"
  },
  {
    "objectID": "assignments/install-R.html#using-r-via-rstudio-1",
    "href": "assignments/install-R.html#using-r-via-rstudio-1",
    "title": "R and RStudio",
    "section": "",
    "text": "After you open RStudio, you should see something like this\n\n\n\nNote the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this tutorial, you’ll come to learn what purpose each of these panes serves."
  },
  {
    "objectID": "assignments/install-R.html#peronalization",
    "href": "assignments/install-R.html#peronalization",
    "title": "R and RStudio",
    "section": "",
    "text": "Customize the appearance of your R IDE by going to Tools -&gt; Global Options -&gt; Appearance"
  },
  {
    "objectID": "slides/lees-pages.html",
    "href": "slides/lees-pages.html",
    "title": "Github Pages",
    "section": "",
    "text": "With newer versions of RStudio, Quarto comes pre-packaged in the installation files. To check whether you have Quarto on your computer, open RStudio, click on the “Terminal” tab in the bottom left window, type quarto check into the terminal, and hit ENTER. If you have a new enough version of RStudio, the terminal should return something like what you see in the first screenshot below.\nCheck Installation Terminal\n\n\n\n\n\n\n\n\nIf you DON’T have a new enough version of RStudio, you’ll receive some error message. To fix this, all you should need to do is install the newest version of RStudio by going to the posit website and installing RStudio.\n\n\n\nCreate a Quarto\n\n\n\n\n\n\nKeep a repository of your website\nPush changes to your website via Github\nSee changes almost instantly\n\n\n\n\nMoving to RStudio\n\nFile \\(\\rightarrow\\) New Project \\(\\rightarrow\\) New Directory \\(\\rightarrow\\) Quarto Website\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange output director to docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelete _site folder (now its using docs)\nCreate data folder to store your dataset\nAdd final project .qmd file to your repo (or drop it into index.qmd)\nUse _quarto.yml to add new pages to navigation bar\nRender index.qmd; confirm that other pages have been rendered\nPush commit and check that the website updated"
  },
  {
    "objectID": "slides/lees-pages.html#installing-quarto",
    "href": "slides/lees-pages.html#installing-quarto",
    "title": "Github Pages",
    "section": "",
    "text": "With newer versions of RStudio, Quarto comes pre-packaged in the installation files. To check whether you have Quarto on your computer, open RStudio, click on the “Terminal” tab in the bottom left window, type quarto check into the terminal, and hit ENTER. If you have a new enough version of RStudio, the terminal should return something like what you see in the first screenshot below.\nCheck Installation Terminal"
  },
  {
    "objectID": "slides/lees-pages.html#installing-quarto-1",
    "href": "slides/lees-pages.html#installing-quarto-1",
    "title": "Github Pages",
    "section": "",
    "text": "If you DON’T have a new enough version of RStudio, you’ll receive some error message. To fix this, all you should need to do is install the newest version of RStudio by going to the posit website and installing RStudio.\n\n\n\nCreate a Quarto"
  },
  {
    "objectID": "slides/lees-pages.html#publishing-a-free-website-with-github-pages",
    "href": "slides/lees-pages.html#publishing-a-free-website-with-github-pages",
    "title": "Github Pages",
    "section": "",
    "text": "Keep a repository of your website\nPush changes to your website via Github\nSee changes almost instantly"
  },
  {
    "objectID": "slides/lees-pages.html#create-a-website",
    "href": "slides/lees-pages.html#create-a-website",
    "title": "Github Pages",
    "section": "",
    "text": "Moving to RStudio\n\nFile \\(\\rightarrow\\) New Project \\(\\rightarrow\\) New Directory \\(\\rightarrow\\) Quarto Website"
  },
  {
    "objectID": "slides/lees-pages.html#create-a-website-2",
    "href": "slides/lees-pages.html#create-a-website-2",
    "title": "Github Pages",
    "section": "",
    "text": "Change output director to docs"
  },
  {
    "objectID": "slides/lees-pages.html#host-your-final-project",
    "href": "slides/lees-pages.html#host-your-final-project",
    "title": "Github Pages",
    "section": "",
    "text": "Delete _site folder (now its using docs)\nCreate data folder to store your dataset\nAdd final project .qmd file to your repo (or drop it into index.qmd)\nUse _quarto.yml to add new pages to navigation bar\nRender index.qmd; confirm that other pages have been rendered\nPush commit and check that the website updated"
  },
  {
    "objectID": "workshops/survey_data.html",
    "href": "workshops/survey_data.html",
    "title": "Final Report Example",
    "section": "",
    "text": "Does moving to a new city reduce the political engagement of youth?\nI hypothesize that moving to a new city will reduce an individuals’ likelihood of engaging in political or civic action. I argue that this is the case for several reasons:\n\nYouth have low levels of political engagement, often driven by lack of information and experience (Holbein and Hillygus 2020)\nYouth that move have less information and experience with engagement in their new city\nYouth that move probably have fewer social ties, and ties are important for facilitating engagement (Campbell 2013)"
  },
  {
    "objectID": "workshops/survey_data.html#research-question",
    "href": "workshops/survey_data.html#research-question",
    "title": "Final Report Example",
    "section": "",
    "text": "Does moving to a new city reduce the political engagement of youth?\nI hypothesize that moving to a new city will reduce an individuals’ likelihood of engaging in political or civic action. I argue that this is the case for several reasons:\n\nYouth have low levels of political engagement, often driven by lack of information and experience (Holbein and Hillygus 2020)\nYouth that move have less information and experience with engagement in their new city\nYouth that move probably have fewer social ties, and ties are important for facilitating engagement (Campbell 2013)"
  },
  {
    "objectID": "workshops/survey_data.html#research-design",
    "href": "workshops/survey_data.html#research-design",
    "title": "Final Report Example",
    "section": "Research Design",
    "text": "Research Design\nI will use a linear model to estimate the relationship between levels of political engagement and whether or not an individual has moved to a new city. This design relies on the assumption that individuals that moved to a new city had similar propensities to participate to those that did not, conditional on observable covariates.\nWhile this assumption is unrealistic, I will take several measures to rule-out potential sources of baseline differences in the propensity of moving and non-moving students to participate.\nFirst, I will identify specific types of participation that are more and less likely to be affected by whether at student moves. If my hypothesis is correct, we should expect to see the more likely forms of participation to be different between moving and non-moving students, while the less likely forms of participation should nto be different between moving and non-moving students.\nSecond, I will condition my analysis to students that move from one urban place to another urban place. While students that move from a rural area to a city are likely to be different in many important ways that could affect their propensity to participate, those that move from one city to another city will be less different.\nThird, I will condition my analysis on students socio-economic status (SES). SES is one of the strongest predictors of political engagement, and students from different SES backgrounds will be different in ways that affect their propensity to participate.\nFourth, I will account for the length of time since respondents moved to their new city. My mechanisms linking moving to a new city with lower engagement (information and social ties) would predict that this effect reduces over time. Therefore, the gap between moving and non-moving students should be smaller for respondents that have been in their new city for longer."
  },
  {
    "objectID": "workshops/survey_data.html#research-context",
    "href": "workshops/survey_data.html#research-context",
    "title": "Final Report Example",
    "section": "Research Context",
    "text": "Research Context\nI will student this research question among students at Addis Ababa University (AAU). AAU is Ethiopia’s top university, and students from around the country (and the continent) move to Addis Ababa in order to study there. Given the frequency with which youth move to a new city in order to obtain education, and the importance of universities and sites of political socialization, this is an important context in which to study the effects of moving on participation."
  },
  {
    "objectID": "workshops/survey_data.html#data-and-variables",
    "href": "workshops/survey_data.html#data-and-variables",
    "title": "Final Report Example",
    "section": "Data and Variables",
    "text": "Data and Variables\nI am using a representative survey of students from Addis Ababa University in Ethiopia collected by DevLab researchers. This data contains two waves collected in May-June and October-November of 2022. A total of 825 students completed both waves of the survey.\nThe variables being used fall into two categories. First, we use variables based on survey questions measuring respondent demographics, including whether or not the student is originally from Addis Ababa, whether they came from an urban or rural setting, the number of years they have been at university, and whether or not they work to support themselves and receive financial support from their parents. Second, we use variables based on survey questions measuring political participation."
  },
  {
    "objectID": "workshops/survey_data.html#describing-the-sample",
    "href": "workshops/survey_data.html#describing-the-sample",
    "title": "Final Report Example",
    "section": "Describing the Sample",
    "text": "Describing the Sample\n\nDemographics\n\n\n\n\n\n\n\n\n\n\n\nLevels of Engagement"
  },
  {
    "objectID": "workshops/survey_data.html#results",
    "href": "workshops/survey_data.html#results",
    "title": "Final Report Example",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nContact Official\nAttend Meeting\nNGO Volunteer\nSign Petition\n\n\n\n\n(Intercept)\n1.044*** (0.114)\n0.752*** (0.116)\n1.304*** (0.181)\n0.354*** (0.090)\n\n\nmoved\n−0.178 (0.141)\n0.580*** (0.143)\n0.117 (0.222)\n0.529*** (0.110)\n\n\nNum.Obs.\n802\n801\n803\n801\n\n\nR2 Adj.\n0.001\n0.019\n−0.001\n0.027"
  },
  {
    "objectID": "workshops/w1_quarto.html",
    "href": "workshops/w1_quarto.html",
    "title": "R, RStudio, and Quarto",
    "section": "",
    "text": "Before using R to illustrate basic programming concepts and data analysis tools, we will get familiar with the RStudio layout.\n\n\nRStudio has four primary panels that will help you interact with your data. We will use the default layout of these panels.\n\nSource panel: Top left\n\nEdit files to create ‘scripts’ of code\n\nConsole panel: Bottom left\n\nAccepts code as input\nDisplays output when we run code\n\nEnvironment panel: Top right\n\nEverything that R is holding in memory\nObjects that you create in the console or source panels will appear here\nYou can clear the environment with the broom icon\n\nViewer panel: Bottom-right\n\nView graphics that you generate\nNavigate files\n\n\n\n\n\nLet’s use these panels to create and interact with data.\nConsole:\n\nPerform a calculation: type 2 + 2 into the console panel and hit ENTER\nCreate and store an object: type sum = 2 + 2 into the console panel and hit ENTER\n\nSource:\n\nStart an R script: Open new .R file (button in top-left below “File”)\nCreate and store an object: type sum = 2 + 3 into the source panel and hit cntrl+ENTER\n\nEnvironment:\n\nConfirm that the object sum is stored in our environment\nUse rm(sum) to clear the object from the environment\nClear the environment with the broom icon\n\nViewer:\n\nNavigate through your computer’s files\nCreate a plot in the source panel"
  },
  {
    "objectID": "workshops/w1_quarto.html#rstudio-contains-4-panels",
    "href": "workshops/w1_quarto.html#rstudio-contains-4-panels",
    "title": "R, RStudio, and Quarto",
    "section": "",
    "text": "RStudio has four primary panels that will help you interact with your data. We will use the default layout of these panels.\n\nSource panel: Top left\n\nEdit files to create ‘scripts’ of code\n\nConsole panel: Bottom left\n\nAccepts code as input\nDisplays output when we run code\n\nEnvironment panel: Top right\n\nEverything that R is holding in memory\nObjects that you create in the console or source panels will appear here\nYou can clear the environment with the broom icon\n\nViewer panel: Bottom-right\n\nView graphics that you generate\nNavigate files"
  },
  {
    "objectID": "workshops/w1_quarto.html#illustration",
    "href": "workshops/w1_quarto.html#illustration",
    "title": "R, RStudio, and Quarto",
    "section": "",
    "text": "Let’s use these panels to create and interact with data.\nConsole:\n\nPerform a calculation: type 2 + 2 into the console panel and hit ENTER\nCreate and store an object: type sum = 2 + 2 into the console panel and hit ENTER\n\nSource:\n\nStart an R script: Open new .R file (button in top-left below “File”)\nCreate and store an object: type sum = 2 + 3 into the source panel and hit cntrl+ENTER\n\nEnvironment:\n\nConfirm that the object sum is stored in our environment\nUse rm(sum) to clear the object from the environment\nClear the environment with the broom icon\n\nViewer:\n\nNavigate through your computer’s files\nCreate a plot in the source panel"
  },
  {
    "objectID": "workshops/w1_quarto.html#formatting",
    "href": "workshops/w1_quarto.html#formatting",
    "title": "R, RStudio, and Quarto",
    "section": "Formatting",
    "text": "Formatting\n\nQuarto is an extension of markdown\n\nYou can italics, bold, and link\nCreate sections and subsections with #, ##, ###\n\nDeclare title, file type, references, in yaml (top section)\nCommenting-out text\n\n\nWrite Formulas\n\nUse math mode to render formulas\n\n\\[\n\\widehat{ATE} = \\overline{Y}_{treatment\\_group} - \\overline{Y}_{control\\_group}\n\\]\n\n\nIntegrate External Image\nYou can also integrate external images that are saved in your project folder.\n\n\n\nHere is an image from my repo with a caption\n\n\n\n\nAutomate References\nCite cool work by outstanding scholars (Springman 2022) and automatically generate a references list at the end of your document.\n\nFind paper on scholar.google.com\n\nSelect “Cite → BibTex\n\nCreate references.bib file\n\nStore BibTex reference in file\nAdd references.bib to your yaml\n\n\n\n\nAdd Flow Charts\nThere is also built-in functionality to do cool stuff like create flow charts using mermaid.\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\n\n\nAdd Footnotes\nWrite a sentence and then add a footnote.1\n\n\nTables By Hand\nYou can create tables by hand using markdown syntax.\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1"
  },
  {
    "objectID": "workshops/w1_quarto.html#working-with-code",
    "href": "workshops/w1_quarto.html#working-with-code",
    "title": "R, RStudio, and Quarto",
    "section": "Working with Code",
    "text": "Working with Code\nTo integrate code into our document, we can use code chunks. This allows us to create self-contained, reproducible documents.\n\nCode Chunks Options\nCode chunks come with specific options to control behavior. Let’s break down each option:\n#| echo: true          # The code will be shown in the rendered output.\n#| warning: false       # Any warnings produced by the code will be suppressed.\n#| label: fig-fake-data # This assigns an internal label to the chunk, useful for referencing.\n#| fig-cap: \"Fake data figure\" # This sets the caption for any figures generated by the chunk.\n#| fig-width: 5\n\n\nFigures\nCheck out Figure 1\n\n\n\n\n\n\n\n\nFigure 1: Fake data figure\n\n\n\n\n\n\n\nDescriptive Tables\n\n\n\n\nTable 1: Fake data table\n\n\n\n\n\n\n\n\n\nDescriptive Table of Random Data\n\n\nSummary statistics: Mean and Standard Deviation\n\n\nVariable\nMean\nSD\n\n\n\n\nx\n0.5200910\n0.2942882\n\n\ny\n0.4770270\n0.2766775\n\n\nsize\n5.6385087\n2.6461582\n\n\ncolor\n0.5130599\n0.2326477\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Results\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\n(Intercept)\n0.003 (0.006)\n−0.001 (0.003)\n\n\nx1\n−0.003 (0.011)\n−0.787*** (0.013)\n\n\nx2\n\n0.394*** (0.006)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.000\n0.808\n\n\nR2 Adj.\n−0.001\n0.807\n\n\nAIC\n−1768.6\n−3415.7\n\n\nBIC\n−1753.9\n−3396.1\n\n\nLog.Lik.\n887.295\n1711.866\n\n\nF\n0.076\n2095.146\n\n\n\n\n\n\n\n\n\nInline Code\nUse code within the text to describe your data (n = 50) for a more reproducible workflow. If your data changes, the writing automatically updates."
  },
  {
    "objectID": "workshops/w1_quarto.html#objects-where-values-are-saved-in-r",
    "href": "workshops/w1_quarto.html#objects-where-values-are-saved-in-r",
    "title": "R, RStudio, and Quarto",
    "section": "Objects: where values are saved in R",
    "text": "Objects: where values are saved in R\n“Object” is a generic term for anything that R stores in the environment. This can include anything from an individual number or word, to lists of values, to entire datasets.\nImportantly, objects belong to different “classes” depending on the type of values that they store.\n\nNumerics are numbers\nCharacters are text or strings like \"hello world\" and \"welcome to R\".\nFactors are a group of characters/strings with a fixed number of unique values\nLogicals are either TRUE or FALSE\n\n\n# Create a numeric object\nmy_number = 5.6\n# Check the class\nclass(my_number)\n\n[1] \"numeric\"\n\n# Create a character object\nmy_character = \"welcome to R\"\n# Check the class\nclass(my_character)\n\n[1] \"character\"\n\n# Create a logical object\nmy_logical = TRUE\n# Check the class\nclass(my_logical)\n\n[1] \"logical\"\n\n\nR can perform operations on objects.\n\n# Create a numeric object\nmy_number = 5.6\n# Check the class\nclass(my_number)\n\n[1] \"numeric\"\n\n# Perform a calculation\nmy_number = my_number + 5\n\nThe class of an object determines the type of operations you can perform on it. Some operations can only be run on numeric objects (numbers).\n\n# Create a character object\nmy_number = \"5.6\"\n# Check the class\nclass(my_number)\n# Perform a calculation\nmy_number + 5\nround(my_number)\n\nR contains functions that can convert some objects to different factors.\n\n# Convert character to numeric\nmy_number = as.numeric(\"5\")\nclass(my_number)\n\n[1] \"numeric\"\n\nmy_number &lt;- 5\n\n# But R is only so smart\nmy_number = as.numeric(\"five\")\nprint(my_number)\n\n[1] NA"
  },
  {
    "objectID": "workshops/w1_quarto.html#data-structures",
    "href": "workshops/w1_quarto.html#data-structures",
    "title": "R, RStudio, and Quarto",
    "section": "Data Structures",
    "text": "Data Structures\nThe most simple objects are single values, but most data analysis involves more complicated data structures.\n\nLists\nLists are a type of data structure that store multiple values together. Lists are created using c() and allow you to perform operations on a series of values.\n\n# Create a numeric list (also called a \"vector\")\nnumeric_vector = c(6, 11, 13, 31)\n# Print the vector\nprint(numeric_vector)\n\n[1]  6 11 13 31\n\n# Check the class\nclass(numeric_vector)\n\n[1] \"numeric\"\n\n# Calculate the mean\nmean(numeric_vector)\n\n[1] 15.25\n\n\nAn important part of working with more complex data structures is called “indexing.” Indexing allows you to extract specific values from a data structure.\n\n# Extract the 2nd element from the list\nnumeric_vector[2]\n\n[1] 11\n\n# Extract elements 2-4\nnumeric_vector[2:4]\n\n[1] 11 13 31\n\n# Extract elements 1-2\nnumeric_vector[c(TRUE, TRUE, FALSE, FALSE)]\n\n[1]  6 11\n\n\n\n\nDataframes\nData frames are the most common type of data structure used in research. Data frames combine multiple lists of values into a single object.\n\n# Create a dataframe\nmy_data = data.frame(\n  x1 = rnorm(100, mean = 1, sd = 1),\n  x2 = rnorm(100, mean = 1, sd = 1)\n)\n\nclass(my_data)\n\n[1] \"data.frame\"\n\n\nAnything that comes in a spreadsheet (for example, an excel file) can be loaded into an R environment as a dataframe. R works most easily when spreadsheets are saved as a .csv file.\n\n# Use `read.csv()` to load data from a website\ndat = read.csv(\"https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv\") \n\n# Use `read.csv()` to load data from your computer's Downloads folder\n# dat = read.csv(\"/home/jeremy/Downloads/clean_endline_did.csv\")\n\nIn most data frames, rows correspond to observations and the columns correspond to variables that describe the observations. Here, we are looking at survey data from an RCT involving university students in Addis Ababa. Each row correspondents to a different survey respondent, and each column represents their answers to a different question from the survey."
  },
  {
    "objectID": "workshops/w1_quarto.html#loading-packages",
    "href": "workshops/w1_quarto.html#loading-packages",
    "title": "R, RStudio, and Quarto",
    "section": "Loading Packages",
    "text": "Loading Packages\nPackages are an extremely important part of data analysis with R.\n\nR gives you access to thousands of “packages” that are created by users\nPackages contain bundles of code called “functions” that can execute specific tasks\nUse install.packages() to install a package and library() to load a package\n\nIn the next section, we’ll use the package dplyr to perform some data cleaning. dplyr is part of a universe of packages called tidyverse. Since this is one of the most important packages in the R ecosystem, let’s install and load it.\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nWhen you are searching online or asking ChatGPT how to perform a specific task in R, it often helps to specify that you are looking for a solution in dplyr."
  },
  {
    "objectID": "workshops/w1_quarto.html#creating-variables",
    "href": "workshops/w1_quarto.html#creating-variables",
    "title": "R, RStudio, and Quarto",
    "section": "Creating Variables",
    "text": "Creating Variables\nImagine we want to analyze the relationship between whether a respondent moved to to Addis Ababa to attend university and their level of political participation. However, there are two problems:\n\nWe don’t have a specific variable that measures whether or not respondents moved\nWe have many measures of participation\n\nHow can we create a variable measuring whether the respondent moved to Addis Ababa? We have a multiple-choice question asking students about what region they come from.\nLet’s start by investigating this variable.\n\n# The name of the variable in our dataframe is `q8_baseline`\ntable(dat$q8_baseline)\n\n\n                                        Addis Ababa \n                                                283 \n                                        Afar Region \n                                                  1 \n                                      Amhara Region \n                                                174 \n                           Benishangul-Gumuz Region \n                                                  5 \n                                   Dire Dawa (city) \n                                                  5 \n                                     Gambela Region \n                                                  1 \n                                      Harari Region \n                                                  4 \n                                      Oromia Region \n                                                205 \n                                              Other \n                                                 12 \n                                  Prefer not to say \n                                                 19 \n                                      Sidama Region \n                                                 35 \n                                      Somali Region \n                                                  5 \n                 South West Ethiopia Peoples Region \n                                                  6 \nSouthern Nations, Nationalities, and Peoples Region \n                                                 65 \n                                      Tigray Region \n                                                  5 \n\ndat = dat %&gt;% # this is called a \"pipe\"\n  # give our variable a better name\n  rename(home_region = q8_baseline) \n\ndat = dat %&gt;%\n  # drop respondents who report \"Prefer not to say\"\n  filter(!home_region == \"Prefer not to say\") \n\ndat = dat %&gt;%\n  # clean home region variable using `mutate()`\n  mutate(\n    # Shorten a long name to an abbreviation\n    home_region = ifelse(home_region == \"Southern Nations, Nationalities, and Peoples Region\", \"SNNPR\", home_region),\n    # remove the word \"Region\" from every observation of this column\n    home_region = str_remove(home_region, \" Region\"),\n    home_region = str_remove(home_region, \" \\\\(city\\\\)\")\n    )\n\n# Check if it worked\ntable(dat$home_region)\n\n\n                Addis Ababa                        Afar \n                        283                           1 \n                     Amhara           Benishangul-Gumuz \n                        174                           5 \n                  Dire Dawa                     Gambela \n                          5                           1 \n                     Harari                      Oromia \n                          4                         205 \n                      Other                      Sidama \n                         12                          35 \n                      SNNPR                      Somali \n                         65                           5 \nSouth West Ethiopia Peoples                      Tigray \n                          6                           5 \n\n\n\n# Chain these all together for more concise code\ndat = read_csv(\"https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv\" ) %&gt;%\n  # give our variable a better name\n  rename(home_region = q8_baseline) %&gt;%\n  # drop respondents who report \"Prefer not to say\"\n  filter(!home_region == \"Prefer not to say\") %&gt;%\n  # clean home region variable\n  mutate(\n    # Shorten a long name to an abbreviation\n    home_region = case_when(\n      home_region == \"Southern Nations, Nationalities, and Peoples Region\" ~ \"SNNPR\",\n      home_region == \"South West Ethiopia Peoples Region\" ~ \"SWEPR\",\n      TRUE ~ home_region\n    ),\n    # remove the word \"Region\" from every observation of this column\n    home_region = str_remove(home_region, \" Region| \\\\(city\\\\)\")\n    )\n\n# Check if it worked\ntable(dat$home_region)\n\n\n      Addis Ababa              Afar            Amhara Benishangul-Gumuz \n              283                 1               174                 5 \n        Dire Dawa           Gambela            Harari            Oromia \n                5                 1                 4               205 \n            Other            Sidama             SNNPR            Somali \n               12                35                65                 5 \n            SWEPR            Tigray \n                6                 5 \n\n\nNow that we’ve cleaned-up the names, we want to create a variable that tells us whether or not each respondent is originally from Addis Ababa. This will let us measure whether or not they moved to Addis in order to attend college.\n\n# Creating a measure of whether a respondent moved to Addis Ababa in order to at independent variable\ndat = dat %&gt;% \n  mutate(\n    moved = case_when(\n      home_region == \"Addis Ababa\" ~ 0,\n      TRUE ~ 1\n    ) \n  )\n\ntable(dat$moved, dat$home_region)\n\n   \n    Addis Ababa Afar Amhara Benishangul-Gumuz Dire Dawa Gambela Harari Oromia\n  0         283    0      0                 0         0       0      0      0\n  1           0    1    174                 5         5       1      4    205\n   \n    Other Sidama SNNPR Somali SWEPR Tigray\n  0     0      0     0      0     0      0\n  1    12     35    65      5     6      5\n\n\nNow, we need to create our second variable measuring levels of political participation. Remember, the challenge is that we have multiple measures of participation. Let’s start with two measures:\n\nNumber of times you’ve contacted gov’t official q13_4_1\nNumber of times you’ve signed a petition q13_5_1\n\n\n# Check out the distribution of our variables\ndat %&gt;% select(q13_4_1, q13_5_1) %&gt;% head(5)\n\n# A tibble: 5 × 2\n  q13_4_1 q13_5_1\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       0       0\n2       3       0\n3       0       0\n4       0       0\n5       0       2\n\nmean(dat$q13_4_1)\n\n[1] NA\n\nhist(dat$q13_5_1)\n\n\n\n\n\n\n\n# Create a single measuree\ndat = dat %&gt;%\n  mutate(\n      add_participation = q13_4_1 + q13_5_1\n\n  )\n\nhist(dat$add_participation)\n\n\n\n\n\n\n\n# how do investigate NA values?\n\nOne thing we need to be careful with are NA values. We need to think carefully about why NA values are in our data and how to handle them appropriately.\nQuestion: Thinking about the data that you have worked with, what are the most common sources of NA values?\n\n## Find our two participation measures\nadd_ecols = grep(\"q13_4_1$|q13_5_1$\", names(dat), value = T)\n\ndat = dat %&gt;%\n  mutate(add_participation =  rowSums(across(add_ecols) ) )\n  #mutate(add_participation_end =  rowSums(across(add_ecols), na.rm = T) )\n\n# dat = dat %&gt;%\n#   mutate(home_region = na_if(home_region, \"Prefer not to say\")) %&gt;%"
  },
  {
    "objectID": "workshops/w1_quarto.html#merging-datasets",
    "href": "workshops/w1_quarto.html#merging-datasets",
    "title": "R, RStudio, and Quarto",
    "section": "Merging Datasets",
    "text": "Merging Datasets\nOften, data analysis projects will require you to use variables from more than one dataset. This will require you to combine separate datasets into a single dataset in R, which is called merging. A merge uses an identifier (administrative unit names, respondent IDs, etc.) that is present in both datasets to combine them into one.\nQuestion: Thinking about the data that you have worked with, have you ever had to merge multiple datasets? What was the data? How did you do it?\n\n## Here we are selecting the unit-identifier (response_id) and the two variables we have created and saving them to a separate dataframe\nnew_vars = dat %&gt;%\n  select(response_id, moved, add_participation)\n\n## After you do some cleaning or create new variables, you may want to save that dataframe for future use\n# write.csv(new_dat, here::here(\"workshops/aau_survey/new_vars.csv\"))\n\n## Drop the new variables from our dataframe so that we can merge them back in\ndat = dat %&gt;%\n  select( -moved, -add_participation)\n\n## Check to make sure the merge will work as expected\n# nrow(dat)\n# nrow(new_vars)\n# dat$response_id[! dat$response_id %in% new_vars$response_id]\n\n## Use dplyr's `full_join` function  to merge datasets\ndat = dat %&gt;%\n  full_join(., new_vars)"
  },
  {
    "objectID": "workshops/w1_quarto.html#tables",
    "href": "workshops/w1_quarto.html#tables",
    "title": "R, RStudio, and Quarto",
    "section": "Tables",
    "text": "Tables\n\n#install.packages(\"gt\")\nlibrary(gt)\n\ndesciptives = dat %&gt;%\n  group_by(moved) %&gt;%\n  summarise(observations = n(), \n            share = observations / nrow(dat),\n            mean = mean(add_participation, na.rm = T)) %&gt;%\n  mutate(moved = case_when(moved == 0 ~ \"No\",\n                           moved == 1 ~ \"Yes\"),\n         share = share *100)\n\ngt(desciptives) %&gt;%\n  tab_header(\n    title = \"Moving to University and Political Participation\",\n  ) %&gt;%\n  fmt_number(\n    columns = 3:4,\n    decimals = 2,\n    use_seps = FALSE\n  ) %&gt;%\n  cols_label(\n    moved = md(\"Moved to&lt;br&gt;Addis\"),\n    observations = md(\"Respondents&lt;br&gt;(count)\"),\n    share = md(\"Respondents&lt;br&gt;(%)\"),\n    mean = md(\"Participation&lt;br&gt;(mean score)\")\n  ) \n#%&gt;%\n # gtsave(filename = \"tab_1.png\")"
  },
  {
    "objectID": "workshops/w1_quarto.html#figures-1",
    "href": "workshops/w1_quarto.html#figures-1",
    "title": "R, RStudio, and Quarto",
    "section": "Figures",
    "text": "Figures\nThere are many ways to create figures in R, but ggplot is the most popular.\n\nggplot(dat, aes(add_participation)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  ggplot(aes(add_participation)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  ggplot(aes(x = add_participation, fill = q3_baseline)) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\ndat %&gt;%\n  filter(add_participation &lt; 15) %&gt;%\n  mutate(moved = fct_rev(as.factor(moved))) %&gt;%\n  ggplot(aes(x = add_participation, fill = moved)) +\n  geom_histogram(aes(y = (..count..) / sum(..count..) * 100),\n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(y = \"Percentage\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.position = c(.8, .8))\n\n\n\n\n\n\n\n\nYou can do tremendous amounts of customization with ggplot to create extremely informative and professional plots.\n\n## demographics\ndemos = dat %&gt;%\n  drop_na(q3_baseline) %&gt;% \n  select(`Respondent Gender` = q3_baseline, `Work as a student?` = q4_baseline, \n         `Rural or urban?` = q5_baseline, `Financial support from family?` = q6_baseline, \n         `Home region` = home_region,\n         `Student Year` = class_year) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name, value) %&gt;% \n  tally() %&gt;% \n  mutate(pct = n/sum(n)) %&gt;% \n  top_n(n = 5, wt = pct)\n\ndemos$name = factor(demos$name, levels = c('Home region', 'Rural or urban?', 'Student Year', 'Respondent Gender', 'Work as a student?', 'Financial support from family?'))\n\ndemos &lt;- demos %&gt;%\n  group_by(name) %&gt;%\n  mutate(total_n = sum(n)) %&gt;%\n  ungroup()\n\nggplot(demos , aes(y = value, x = pct)) + \n  geom_col(fill = \"grey\") + \n  facet_wrap( ~name, scales = \"free\") + \n  scale_y_discrete(labels = scales::label_wrap(30)) + \n  hrbrthemes::scale_x_percent(limits = c(0, 1)) + \n  labs(x = \"Percent of respondents to the survey\", y = NULL, \n       title = \"Demographic characteristics of baseline respondents\", \n       subtitle = glue::glue(\"Number of respondents = {scales::comma(nrow(df))}\"), \n       caption = \"Note: home region only displays top five categories by size.\") +\n  geom_text(\n    aes(\n      label = glue::glue(\"n = {total_n}\")\n    ), \n    x = 0.92, y = -Inf, \n    vjust = -1, hjust = 1, \n    inherit.aes = FALSE\n  )\n\n\n\n\n\n\n\n#ggsave(\"/home/jeremy/Downloads/demographics.png\")"
  },
  {
    "objectID": "workshops/w1_quarto.html#averaged-z-scores",
    "href": "workshops/w1_quarto.html#averaged-z-scores",
    "title": "R, RStudio, and Quarto",
    "section": "Averaged Z-Scores",
    "text": "Averaged Z-Scores\n\n## Find participation measures that are based on likert\n# baseline\nbcols = grep(\"^q13_.*_baseline$\", names(dat), value = T)\ndat[, paste0(bcols, \"_st\")] = dat[, bcols]\nbcols = paste0(bcols,\"_st\")\n\n# endline\necols = grep(\"^q13_[1-7]_\\\\d$\", names(dat), value = T)\ndat[, paste0(ecols, \"_st\")] = dat[, ecols]\necols = paste0(ecols,\"_st\")\n\n\n# Create treatment variable\ndat = dat %&gt;% mutate(moved = case_when(home_region == \"Addis Ababa\" ~ 0, TRUE ~ 1) )\n\n# clean q13_\nlevels = c(\"Never\", \"Once or Twice\", \"More than twice\", \"More than 5 times\", \n           \"More than 10 times\")\ndat = dat %&gt;% \n  mutate(across(c(bcols), \n                .fns = ~ factor(.x, levels = levels)))\n\n# Create z-score function from Kling, Liberman, and Katz (2007)\nz_score = function(x, y){\n  # calculate mean and sd of control group\n  c_mean = mean( as.numeric( unlist(x[, y])) , na.rm = T)\n  c_sd = sd( as.numeric( unlist(x[, y])) , na.rm = T)\n  # subtract control group mean; divide by control group SD\n  ( as.numeric(x[, y, drop = TRUE]) - c_mean) / c_sd\n}\n\n# calculate z-scores\nfor (i in c(bcols, ecols)) {\n  dat[,i] = z_score(dat, i)\n}\n\ndat = dat %&gt;% \n  rowwise() %&gt;% \n  mutate( z_participation_end = mean(c_across(all_of(bcols)), na.rm = TRUE)) %&gt;% \n  mutate( z_participation_base = mean(c_across(all_of(ecols)), na.rm = TRUE)) %&gt;%\n  ungroup()\n\n\nregd = dat %&gt;% select(z_participation_end, z_participation_base, moved, response_id ) %&gt;%\n  pivot_longer(cols = c(z_participation_end, z_participation_base),\n               names_to = \"time\",\n               values_to = \"z_participation\") %&gt;%\n  mutate(time = case_when(time == \"z_participation_end\" ~ 1,\n                          TRUE ~ 0))\n\nmodels &lt;- list()\nmodels[['Bivariate']] = lm(z_participation ~ moved, regd)\nmodels[['Multivariate']] = lm(z_participation ~ moved + time, regd)\nmodels[['Interaction']] = lm(z_participation ~ moved + time + moved*time, regd)\n\nmodelsummary(\n  models,\n  estimate  = \"{estimate}{stars} ({std.error})\",\n             statistic = NULL,\n  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')\n\n\n\n\n\nBivariate\nMultivariate\nInteraction\n\n\n\n\n(Intercept)\n−0.159*** (0.026)\n−0.156*** (0.030)\n−0.156*** (0.037)\n\n\nmoved\n0.247*** (0.032)\n0.247*** (0.032)\n0.247*** (0.046)\n\n\ntime\n\n−0.004 (0.031)\n−0.005 (0.052)\n\n\nmoved × time\n\n\n0.001 (0.065)\n\n\nNum.Obs.\n1597\n1597\n1597\n\n\nR2 Adj.\n0.035\n0.034\n0.034"
  },
  {
    "objectID": "workshops/w1_quarto.html#footnotes",
    "href": "workshops/w1_quarto.html#footnotes",
    "title": "R, RStudio, and Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInlines notes are easier to write, since you don’t have to pick an identifier and move down to type the note.↩︎"
  },
  {
    "objectID": "slides/07-inference.html#agenda",
    "href": "slides/07-inference.html#agenda",
    "title": "Linear Regression and Uncertainty",
    "section": "Agenda",
    "text": "Agenda\n\nParameters vs. Estimates\nConfidence Intervals\nThe Parameters in Linear Regression\nQuantifying Uncertainty in Linear Regression"
  },
  {
    "objectID": "slides/07-inference.html#from-sample-to-population",
    "href": "slides/07-inference.html#from-sample-to-population",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "From sample to population",
    "text": "From sample to population\n\n\nOften we want to know some characteristic about a population of interest\nBut we only have a sample of that population\nHow do we make inferences about the population with what we learn from our sample?"
  },
  {
    "objectID": "slides/07-inference.html#empirical-example-brexit",
    "href": "slides/07-inference.html#empirical-example-brexit",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Empirical Example: Brexit",
    "text": "Empirical Example: Brexit\n\n\nData from a random sample of British citizens: the British Electoral Survey\nWe want to learn the probability that a British citizen supports Brexiting in the country.\n\nThis is a population parameter\n\nSuch probability is the same in expectation as the proportion of pro-brexiting citizens\n\nWhy?"
  },
  {
    "objectID": "slides/07-inference.html#empirical-example-brexit-1",
    "href": "slides/07-inference.html#empirical-example-brexit-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Empirical Example: Brexit",
    "text": "Empirical Example: Brexit\n\n\nShow code\nbrex &lt;- read.csv(here::here(\"./slides/code/BES.csv\"))\nstr(brex)\n\n\n'data.frame':   30895 obs. of  4 variables:\n $ vote     : chr  \"leave\" \"leave\" \"stay\" \"leave\" ...\n $ leave    : int  1 1 0 1 NA 0 1 1 1 1 ...\n $ education: int  3 NA 5 4 2 4 3 2 3 4 ...\n $ age      : int  60 56 73 64 68 85 78 51 59 68 ...\n\n\nShow code\ntable(brex$vote, useNA = \"always\")\n\n\n\n   don't know         leave          stay wouldn't vote          &lt;NA&gt; \n         2314         13692         14352           537             0 \n\n\nShow code\nbrex$exit &lt;- ifelse(brex$vote==\"leave\", 1, \n                ifelse(brex$vote == \"stay\", 0, NA))\nprop.table(table(brex$exit))*100\n\n\n\n       0        1 \n51.17672 48.82328"
  },
  {
    "objectID": "slides/07-inference.html#empirical-example-brexit-2",
    "href": "slides/07-inference.html#empirical-example-brexit-2",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Empirical Example: Brexit",
    "text": "Empirical Example: Brexit\n\nRandom Variable: the outcome of some process where there’s uncertainty\n\nThink of flipping a coin: we do not know if the result of a single flip will be heads or tails, but we know that with p = .5 it will be the former\n\n“Support of Brexit” is a Random Variable:\n\n\\(Support \\sim \\text{Bernoulli}(p)\\)\n\nWhere \\(E(Support) = p\\)"
  },
  {
    "objectID": "slides/07-inference.html#empirical-example-brexit-3",
    "href": "slides/07-inference.html#empirical-example-brexit-3",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Empirical Example: Brexit",
    "text": "Empirical Example: Brexit\n\nWe want to know what \\(p\\) is!\nBut it is a population parameter and we only have a sample\nWe can estimate \\(\\hat{p}\\) by doing\n\n\nbrex &lt;- dplyr::filter(brex, is.na(exit)==F)\n(phat &lt;- mean(brex$exit, na.rm =T))\n\n[1] 0.4882328\n\n\n\nTo see why, imagine a fair coin toss. How many Heads do you expect after flipping the coin 100 times?"
  },
  {
    "objectID": "slides/07-inference.html#sampling-distribution-of-hatp",
    "href": "slides/07-inference.html#sampling-distribution-of-hatp",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Sampling Distribution of \\(\\hat{p}\\)",
    "text": "Sampling Distribution of \\(\\hat{p}\\)\n\nOn expectation, our sample \\(E[\\hat{p}] = p\\)\n\nThis property is called unbiasedness\nAs our sample size increases, our \\(\\hat{p}\\) converges in probability to p\n\nBut if our sample were slightly different, we would have gotten a different \\(\\hat{p}\\)!\n\nWe need to account for this uncertainty!\n\nLet’s simulate the different sample means we would have gotten if our sample changed slightly:"
  },
  {
    "objectID": "slides/07-inference.html#sampling-distribution-of-hatp-1",
    "href": "slides/07-inference.html#sampling-distribution-of-hatp-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Sampling Distribution of \\(\\hat{p}\\)",
    "text": "Sampling Distribution of \\(\\hat{p}\\)\n\n\nShow code\nrequire(tidyverse)\nset.seed(7)\n\nout.means &lt;- c()\nfor (i in 1:1000) {\n  temp_dat &lt;- sample_n(brex, nrow(brex), replace = T)\n  out.means[i] &lt;- mean(temp_dat$exit, na.rm = T)\n  rm(temp_dat)\n}\n\nhist(out.means)\nabline(v = mean(out.means), col = \"red\", lwd = 2, add = T)"
  },
  {
    "objectID": "slides/07-inference.html#standard-errors",
    "href": "slides/07-inference.html#standard-errors",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nThe standard deviation of the sampling distribution of an estimator is called “standard error”\nBy calculating the standard error we can know the shape of the sampling distribution. This helps us do two important things:\n\nConstruct confidence intervals (what is the range within which the true value is likely to be?)\nDo hypothesis testing (p-values and statistical significance)"
  },
  {
    "objectID": "slides/07-inference.html#confidence-intervals",
    "href": "slides/07-inference.html#confidence-intervals",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nRange of values that likely includes the true value of our parameter of interest\n\nThe range that includes a pre-specified proportion of the density of the sampling distribution\n\nInterpretation: “With X% confidence, the true parameter is within the confidence interval”\n\nMore specifically “If I drew millions of samples and constructed a confidence interval for each one, my true parameter would be inside the CI X% of the times”"
  },
  {
    "objectID": "slides/07-inference.html#confidence-intervals-1",
    "href": "slides/07-inference.html#confidence-intervals-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nBecause of the properties of the normal distribution, we know that 95% of the density will be within the following range:\n\n\\[\\begin{align*}\nCI_{95\\%} = \\hat{p} - 1.96 \\times \\sqrt{\\frac{Var (Support)}{n}},\\\\\n\\hat{p} + 1.96 \\times \\sqrt{\\frac{Var (Support)}{n}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression",
    "href": "slides/07-inference.html#linear-regression",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nWe can think of the parameters of a linear regression in the same way.\n\n\\[\\begin{equation*}\nY_i = \\alpha + \\beta X_i + \\varepsilon_i\n\\end{equation*}\\]\n\n\\(\\alpha\\) an intercept, common to all units.\n\\(\\beta\\) the slope, common to all units.\nWe need to describe the relationship between X and Y with a line using information from our sample"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-1",
    "href": "slides/07-inference.html#linear-regression-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nEstimates of \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) have uncertainty\n\n\n\nThey have their own sampling distributions!\n\nCLT: They are also normal\n\nWe can use what I know about normal distributions to quantify their uncertainty\nWe can construct confidence intervals in the exact same way!\nOr do hypothesis tests"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-2",
    "href": "slides/07-inference.html#linear-regression-2",
    "title": "Linear Regression and Uncertainty",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nEstimates of \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) are uncertain\nThey have their own sampling distributions!\n\nCLT: They are also normal\n\nWe can use what I know about normal distributions to quantify their uncertainty\nWe can construct confidence intervals in the exact same way!\nOr do hypothesis tests"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-3",
    "href": "slides/07-inference.html#linear-regression-3",
    "title": "Linear Regression and Uncertainty",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nEstimates of \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) are uncertain\nThey have their own sampling distributions!\n\nCLT: They are also normal\n\nWe can use what I know about normal distributions to quantify their uncertainty\nWe can construct confidence intervals in the exact same way!\nOr do hypothesis tests"
  },
  {
    "objectID": "slides/07-inference.html#hypothesis-testing-and-p-values",
    "href": "slides/07-inference.html#hypothesis-testing-and-p-values",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Hypothesis Testing and P-values",
    "text": "Hypothesis Testing and P-values\n\nWe are often interested in determining whether the true parameter is different from zero with a pre-specified level of confidence\n\n\\[\\begin{align*}\nH_0: \\beta = 0 \\\\\nH_1: \\beta \\neq 0\n\\end{align*}\\]\n\nWe are going to reject \\(H_0\\) in favor of \\(H_1\\) if we are sufficiently confident we aren’t making a (type I) mistake"
  },
  {
    "objectID": "slides/07-inference.html#p-value",
    "href": "slides/07-inference.html#p-value",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "P-value",
    "text": "P-value\n\nAssume the true effect/parameter is 0\n“Draw” the sampling distribution of the parameter\n\nRemember we know that its sd = se\n\nCalculate the probability of observing an estimate at least as extreme as the one you observed with your sample if the true parameter is zero\nIf you are doing a two-tailed test, use absolute value"
  },
  {
    "objectID": "slides/07-inference.html#p-value-1",
    "href": "slides/07-inference.html#p-value-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "P-value",
    "text": "P-value\n\nSimulated data to visualize the p-value"
  },
  {
    "objectID": "slides/07-inference.html#statistical-significance",
    "href": "slides/07-inference.html#statistical-significance",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Statistical Significance",
    "text": "Statistical Significance\n\nIf you are using a level of statistical significance of 95%, you reject \\(H0: \\beta = 0\\) if \\(\\text{p-value} \\leq .05\\)\nIf you are using a level of statistical significance of 99%, you reject \\(H0: \\beta = 0\\) if \\(\\text{p-value} \\leq .01\\)\nWhen we have estimates with a p-value less or equal to that, we say our coefficient is “statistically significant”\nIt just means we are sure enough the parameter is different from zero\nIn papers, they report this with different number of stars!"
  },
  {
    "objectID": "slides/07-inference.html#summing-up-statistical-vs.-scientific-significante",
    "href": "slides/07-inference.html#summing-up-statistical-vs.-scientific-significante",
    "title": "Linear Regression and Uncertainty",
    "section": "Summing Up: Statistical vs. Scientific Significante",
    "text": "Summing Up: Statistical vs. Scientific Significante\n\nStatistical significance is NOT a measure of importance\nStatistical significance just means an effect or difference is likely NOT to be zero\nBut it might still be super small as to be socially irrelevant\nThe more observations we have, the better “powered” we are to detect small effects"
  },
  {
    "objectID": "slides/07-inference.html#todays-roadmap",
    "href": "slides/07-inference.html#todays-roadmap",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Today’s Roadmap",
    "text": "Today’s Roadmap\n\nTargeting Population Parameters with Sample Estimates\nThe Uncertainty in Sample Estimates\nLinear Regression and its Parameters\nQuantifying Uncertainty in Linear Regression’s Estimated Parameters"
  },
  {
    "objectID": "slides/07-inference.html#from-sample-to-population-1",
    "href": "slides/07-inference.html#from-sample-to-population-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "From sample to population",
    "text": "From sample to population\n\n\nThe first step is to make sure our sample is representative of the population\n\nWhat do we mean by this?\nWhy is it important?\n\nHow can we achieve representativeness?\nBeware! We not only need to sample a representative group of individuals, we also need a representative group of them to answer our questions"
  },
  {
    "objectID": "slides/07-inference.html#sampling-distribution-of-hatp-2",
    "href": "slides/07-inference.html#sampling-distribution-of-hatp-2",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Sampling Distribution of \\(\\hat{p}\\)",
    "text": "Sampling Distribution of \\(\\hat{p}\\)\n\nThat’s just a normal distribution!\nAll normal distributions can be described by their mean and their standard deviation\nThis one is called “sampling distribution of the sample mean”\n\nCentered around our estimate \\(\\hat{p}\\)\n\\(SE =  \\sqrt{\\frac{Var (Support)}{n}}\\)\n\nKnowing it is a normal distribution helps us quantify the uncertainty in our estimates"
  },
  {
    "objectID": "slides/07-inference.html#its-not-all-about-sample-size",
    "href": "slides/07-inference.html#its-not-all-about-sample-size",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "It’s not all about sample size!",
    "text": "It’s not all about sample size!\n\n1936 Presidential Election in the US: FDR vs Alf Landon\nA newspaper called the Literary Digest ran a survey and asked 10 million (!!!) Americans who they would vote for\n\n2.4 million of them answered"
  },
  {
    "objectID": "slides/07-inference.html#its-not-all-about-sample-size-1",
    "href": "slides/07-inference.html#its-not-all-about-sample-size-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "It’s not (all) about sample size!",
    "text": "It’s not (all) about sample size!"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-4",
    "href": "slides/07-inference.html#linear-regression-4",
    "title": "Linear Regression and Uncertainty",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nEstimates of \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) are uncertain\nThey have their own sampling distributions!\n\nCLT: They are also normal\n\nWe can use what I know about normal distributions to quantify their uncertainty\nWe can construct confidence intervals in the exact same way!\nOr do hypothesis tests"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-intuition",
    "href": "slides/07-inference.html#linear-regression-intuition",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Intuition",
    "text": "Linear Regression: Intuition\n\nOur data includes the outcome \\(y_i\\) and our explanatory variable \\(x_i\\)\nBut we can draw infinite lines through those points\nHow to choose the correct \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\), like we chose the correct \\(\\widehat{p}\\)?"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-intuition-1",
    "href": "slides/07-inference.html#linear-regression-intuition-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Intuition",
    "text": "Linear Regression: Intuition\n\nIf we have a slope and an intercept, for every \\(X_i\\), the equation of a line gives us a predicted \\(Y_i\\), or \\(\\widehat{Y_i}\\)\nSo, for each plausible estimates of \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\) we can calculate the prediction error\n\n\\[\\begin{align*}\n\\hat{\\varepsilon}_i =& Y_i - \\hat{Y_i} \\\\\n\\hat{\\varepsilon}_i =& Y_i - \\hat{\\alpha} - \\hat{\\beta} x_i\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-simulation",
    "href": "slides/07-inference.html#linear-regression-simulation",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Simulation",
    "text": "Linear Regression: Simulation\n\n# Simulated data\nset.seed(8)\n# TRUE alpha and beta\nalpha &lt;- 5\nbeta &lt;- -.216\nx &lt;- rnorm(100, 4, .8) # \nerror &lt;- rnorm(100, 0, 1)\n# relationship is linear by construction because I'm simulation god!!\ny &lt;- alpha + (beta*x) + error"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-simulation-1",
    "href": "slides/07-inference.html#linear-regression-simulation-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Simulation",
    "text": "Linear Regression: Simulation\n\n\nShow code\n# Fit linear model\nmodel &lt;- lm(y ~ x)\n# Predict $y_hat$ or the expected y given the model and x\ny_pred &lt;- predict(model)\n# Plot the data\nplot(x, y, main=\"Visualizing OLS\", xlab=\"X\", ylab=\"Y\", pch=16, col=\"gray45\")\n# Add best fit line\nabline(model, col=\"maroon\", lwd=2)\n# Draw vertical lines showing each prediction error\nsegments(x, y, x, y_pred, col=\"purple\", lty=2)"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-intuition-2",
    "href": "slides/07-inference.html#linear-regression-intuition-2",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Intuition",
    "text": "Linear Regression: Intuition\n\nIf we add up the prediction error for each observation, we get the Sum of Squared Residuals\n\n\\[\\begin{equation*}\nSSR = \\widehat{\\varepsilon}^2 = (Y_i - \\widehat{\\alpha} - \\widehat{\\beta}X_i)^2\n\\end{equation*}\\]\n\nMinimizing this objective yields the “ordinary least squares” (OLS) estimates of \\(\\alpha\\) and \\(\\beta\\)"
  },
  {
    "objectID": "slides/07-inference.html#linear-regression-interpretation",
    "href": "slides/07-inference.html#linear-regression-interpretation",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Linear Regression: Interpretation",
    "text": "Linear Regression: Interpretation\n\nWhy are \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) different from \\(\\alpha\\) and \\(\\beta\\)?\n\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4061 -0.8055  0.1533  0.6918  2.4457 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.9658     0.5027   9.879   &lt;2e-16 ***\nx            -0.2082     0.1251  -1.664   0.0993 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 98 degrees of freedom\nMultiple R-squared:  0.02748,   Adjusted R-squared:  0.01755 \nF-statistic: 2.769 on 1 and 98 DF,  p-value: 0.09932\n\n\n         x \n-0.2081563 \n\n\n[1] 0.1250993"
  },
  {
    "objectID": "slides/07-inference.html#statistical-significance-1",
    "href": "slides/07-inference.html#statistical-significance-1",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Statistical Significance",
    "text": "Statistical Significance\n\nIn our example, would we reject \\(H0: \\beta = 0\\) with 90% confidence? With 95% confidence? With 99%?\n\n\n\nShow code\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4061 -0.8055  0.1533  0.6918  2.4457 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.9658     0.5027   9.879   &lt;2e-16 ***\nx            -0.2082     0.1251  -1.664   0.0993 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 98 degrees of freedom\nMultiple R-squared:  0.02748,   Adjusted R-squared:  0.01755 \nF-statistic: 2.769 on 1 and 98 DF,  p-value: 0.09932"
  },
  {
    "objectID": "slides/07-inference.html#p-value-2",
    "href": "slides/07-inference.html#p-value-2",
    "title": "Linear Regression and Uncertainty",
    "section": "P-value",
    "text": "P-value"
  },
  {
    "objectID": "slides/07-inference.html#summing-up-2-takeaways",
    "href": "slides/07-inference.html#summing-up-2-takeaways",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Summing Up: 2 Takeaways",
    "text": "Summing Up: 2 Takeaways\n\nThe representative-ness of our samples is crucial to make inferences abt. population\n\nThe more observations we have, the better “powered” we are to detect small effects\nBut sample size does not substitute for representative-ness\n\nStatistical significance is important but it is NOT a measure of SCIENTIFIC importance\n\nStatistical significance just means an effect or difference is likely NOT to be zero"
  },
  {
    "objectID": "materials/c1_accountability.html",
    "href": "materials/c1_accountability.html",
    "title": "Representation and Accountability",
    "section": "",
    "text": "Manin, Przeworski, and Stokes. Chapter 1 (1999)\nFerraz and Finan, 2011"
  },
  {
    "objectID": "materials/c1_accountability.html#required-readings",
    "href": "materials/c1_accountability.html#required-readings",
    "title": "Representation and Accountability",
    "section": "",
    "text": "Manin, Przeworski, and Stokes. Chapter 1 (1999)\nFerraz and Finan, 2011"
  },
  {
    "objectID": "materials/c1_accountability.html#slides",
    "href": "materials/c1_accountability.html#slides",
    "title": "Representation and Accountability",
    "section": "Slides",
    "text": "Slides\n\nSlides"
  },
  {
    "objectID": "materials/c2_accountability.html",
    "href": "materials/c2_accountability.html",
    "title": "Representation and Accountability",
    "section": "",
    "text": "Dunning et. al.(2019)"
  },
  {
    "objectID": "materials/c2_accountability.html#required-readings",
    "href": "materials/c2_accountability.html#required-readings",
    "title": "Representation and Accountability",
    "section": "",
    "text": "Dunning et. al.(2019)"
  },
  {
    "objectID": "materials/c2_accountability.html#slides",
    "href": "materials/c2_accountability.html#slides",
    "title": "Representation and Accountability",
    "section": "Slides",
    "text": "Slides\n\nSlides\n\n\nWorkshop\n\nDownload the R Script for multiple regression basics"
  },
  {
    "objectID": "slides/07-inference.html#housekeeping",
    "href": "slides/07-inference.html#housekeeping",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nYou have two assignments next week!"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is the schedule for the semester. You can find the materials for each course meeting under the “Content” links for that week. You should complete the required readings before each meeting.\nHere’s a guide to the schedule:\n\nMaterials (): This page contains the readings and slides for the meeting\nWorkshop (): A link to the workshop for that week\nAssignment (): This page contains the instructions for each assignment\n\nThe readings refer to following texts:\n\nDSS: Data Analysis for Social Science: A Friendly and Practical Introduction (DSS) by Kosuke Imai and Elena Llaudet \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading\n\n\nMaterials\n\n\nWorkshop\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\nJanuary 15\n\n\nIntroduction to the course (JS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nJanuary 20\n\n\nMLK Jr. Day (no classes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 22\n\n\nCorrelation vs. Causation (CT)\n\n\nDSS Ch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nJanuary 27\n\n\nCausality (CT)\n\n\nDSS Ch 5\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 29\n\n\nRstudio and Quarto Workshop (JS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nFebruary 3\n\n\nDemocracy and Autocracy (CT)\n\n\nPrzeworski 2024, Grossman et. al., 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 5\n\n\nDemocracy and Development (CT)\n\n\nAcemoglu et. al., 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\nFebruary 10\n\n\nFinal Project Overview (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 12\n\n\nLinear Regression and Uncertainty (CT)\n\n\nDSS Ch 3 and 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nFebruary 17\n\n\nRepresentation and Accountability (CT)\n\n\nManin, Przeworski, and Stokes, 1999; Ferraz and Finan, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 19\n\n\nRepresentation and Accountability Reconsidered (CT)\n\n\nDunning et. al. 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nFebruary 24\n\n\nGithub Workshop (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 26\n\n\nWorkshop\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\nMarch 3\n\n\nCrime and Punishment (CT)\n\n\nBateson, 2012; Weaver and Lerman, 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 5\n\n\nCrime and Punishment 2 (CT)\n\n\nGustavo A. Flores-Macías and Jessica Zarkin, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nMarch 10\n\n\nSpring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 12\n\n\nSpring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nMarch 17\n\n\nGender (CT)\n\n\nChattopadhyay and Duflo, 2004\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 19\n\n\nMigration 1 (JS)\n\n\n3ie; Sviatschi (2022); Gazeaud (2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\nMarch 24\n\n\nText as Data workshop (CT)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 26\n\n\nFinal Project Essentials (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\nMarch 31\n\n\nForeign Aid and NGOs 1 (JS)\n\n\nBriggs (2016); Briggs (2021)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 2\n\n\nForeign Aid and NGOs 2 (JS)\n\n\nCruz (2017); Bold et al. (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\nApril 7\n\n\nClimate and Development (JS)\n\n\nO’Brien-Udry (2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 9\n\n\nClimate and Development 2 (JS)\n\n\nNamrata (2023); Alberto et al. (2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\n\nApril 14\n\n\nFinal Project Preparation (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 16\n\n\nGithub Pages (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\n\n\n\n\nApril 21\n\n\nState Capacity (JS)\n\n\nFukuyama (2013); Ekeh (1975); Di Maro et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 23\n\n\nTBD (JS)\n\n\nNo readings\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\n\n\n\n\nApril 30\n\n\nConclusion (JS)\n\n\nAssorted short pieces\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 17\n\n\n\n\nMay 10\n\n\nFinal Project Due"
  },
  {
    "objectID": "slides/07-inference.html#computing-cis-example",
    "href": "slides/07-inference.html#computing-cis-example",
    "title": "Sampling, Linear Regression, and Uncertainty",
    "section": "Computing CI’s: Example",
    "text": "Computing CI’s: Example\n\n\nShow code\n# standard deviation of the sampling distribution computed with the formula\nse &lt;- round(sqrt(var(brex$exit, na.rm = T)/nrow(brex)),3)\n# An analytic solution to the confidence interval\n(ci_95 &lt;- c(phat - (1.96*se), phat + (1.96*se)))\n\n\n[1] 0.4823528 0.4941128\n\n\nShow code\n# We can check that it's the same as the interval that leave 95% of the mass inside\n# of the sampling distribution \nquantile(out.means, c(.025, .975))\n\n\n     2.5%     97.5% \n0.4824196 0.4939738"
  },
  {
    "objectID": "slides/c2-account1.html#assignments",
    "href": "slides/c2-account1.html#assignments",
    "title": "Representation and Accountability I",
    "section": "Assignments",
    "text": "Assignments\n\nTwo assignments due TODAY/Wed at midnight:\n\n\nCreating a Final Project Git Repo\nIdea for Final Research Project\n\n\nPlease send a Slack DM to both me and Jeremy"
  },
  {
    "objectID": "slides/c2-account1.html#agenda",
    "href": "slides/c2-account1.html#agenda",
    "title": "Representation and Accountability I",
    "section": "Agenda",
    "text": "Agenda\n\nBackground (Accountability 101)\nFerraz and Finan, 2011"
  },
  {
    "objectID": "slides/c2-account1.html#przeworski-stokes-and-manin-1999",
    "href": "slides/c2-account1.html#przeworski-stokes-and-manin-1999",
    "title": "Representation and Accountability I",
    "section": "Przeworski, Stokes, and Manin, 1999",
    "text": "Przeworski, Stokes, and Manin, 1999\n\nWhat do we mean by representation?\n\nActing in the voter’s best interests\nWhat is the claim connecting democracy and representation?\nUnder democracy, governments are representative because they are elected.\nBut why?"
  },
  {
    "objectID": "slides/c2-account1.html#accountability-view",
    "href": "slides/c2-account1.html#accountability-view",
    "title": "Representation and Accountability I",
    "section": "Accountability view",
    "text": "Accountability view\nElections serve to hold governments accountable for their past actions\n\n\nFundamentally retrospective\nVoters retain politicians only when they acted in their best interest\nPoliticians anticipate this and serve!"
  },
  {
    "objectID": "slides/c2-account1.html#what-are-the-issues-with-these-mechanisms",
    "href": "slides/c2-account1.html#what-are-the-issues-with-these-mechanisms",
    "title": "Representation and Accountability I",
    "section": "What are the issues with these mechanisms?",
    "text": "What are the issues with these mechanisms?\n\nCitizens are not omniscient, for better and for worse.\n\nThey do not know everything - Imperfect evaluation of what politicians should be doing and of whether they did what they ought to have done\nPoliticians have goals, interests, and values of their own and monitoring is costly."
  },
  {
    "objectID": "slides/c2-account1.html#pitfals-in-the-mandate-view",
    "href": "slides/c2-account1.html#pitfals-in-the-mandate-view",
    "title": "Representation and Accountability I",
    "section": "Pitfals in the Mandate View",
    "text": "Pitfals in the Mandate View\nIs it plausible to think governments will do what they propose?\n\nWhat if politicians come to believe the policy would be bad for the country?\nWhat if they voters changed their mind?\nWhat if they are motivated by private interest? I.e. become rich?\nWhat if politicians need money of special interests to campaign?\nWhat if reputations matter?\n\nWe may end up in a world where politicians deviate from their mandate becaue it’s in the voters interests, or where they do not, even if doing so would be better for voters!"
  },
  {
    "objectID": "slides/c2-account1.html#is-it-desirable",
    "href": "slides/c2-account1.html#is-it-desirable",
    "title": "Representation and Accountability I",
    "section": "Is it desirable?",
    "text": "Is it desirable?\n\nPoliticians are not legally compelled to abide by their platform in any democratic system!\nWhy?\n\nWe want our representativs to learn! To deliberate! To think! To respond to flexible conditions"
  },
  {
    "objectID": "slides/c2-account1.html#pitfals-in-the-accountability",
    "href": "slides/c2-account1.html#pitfals-in-the-accountability",
    "title": "Representation and Accountability I",
    "section": "Pitfals in the Accountability",
    "text": "Pitfals in the Accountability\n\nIdea is reward or punish depending on their performance\nIs it plausible to think citizens have enough information to evaluate politicians?\nWhat if politicians do not value getting reelected"
  },
  {
    "objectID": "slides/c2-account1.html#the-vote-our-one-blunt-tool",
    "href": "slides/c2-account1.html#the-vote-our-one-blunt-tool",
    "title": "Representation and Accountability I",
    "section": "The Vote: Our One Blunt Tool",
    "text": "The Vote: Our One Blunt Tool\n\nIn the accountability view, voters are retrospective.\n\nThey use the vote to punish\n\nIn the mandate view, voters are prospective\n\nThey use the vote to select the best policy / best politicians\n\nIn reality, voters want to do both: select good policy and punish bad behavior\nBut we only have one vote. Can we achieve both goals?"
  },
  {
    "objectID": "slides/c2-account1.html#research-question",
    "href": "slides/c2-account1.html#research-question",
    "title": "Representation and Accountability I",
    "section": "Research question",
    "text": "Research question\n\n\nBroadly: Do institutions affect accountability?\nSpecifically: Do elections work as a disciplining device?\nReelection Incentives \\(\\rightarrow\\) corruption"
  },
  {
    "objectID": "slides/c2-account1.html#theory",
    "href": "slides/c2-account1.html#theory",
    "title": "Representation and Accountability I",
    "section": "Theory",
    "text": "Theory\n\n\nPoliticians who can get reelected have fewer incentives to steal funds\n\nWhy?\n\n\n\nThey want to get reelected! To get reelected, they need to serve the citizens.\nVoting as a disciplining tool"
  },
  {
    "objectID": "slides/c2-account1.html#data",
    "href": "slides/c2-account1.html#data",
    "title": "Representation and Accountability I",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "slides/c2-account1.html#research-design",
    "href": "slides/c2-account1.html#research-design",
    "title": "Representation and Accountability I",
    "section": "Research Design",
    "text": "Research Design\n\nWhat is the treatment group?\nWhat is the control group?\nWhat are plausible threats to inference?"
  },
  {
    "objectID": "slides/c2-account1.html#findings",
    "href": "slides/c2-account1.html#findings",
    "title": "Representation and Accountability I",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/c2-account1.html#implications-for-accountabillity",
    "href": "slides/c2-account1.html#implications-for-accountabillity",
    "title": "Representation and Accountability I",
    "section": "Implications for accountabillity",
    "text": "Implications for accountabillity"
  },
  {
    "objectID": "slides/c2-account1.html#mandate-view",
    "href": "slides/c2-account1.html#mandate-view",
    "title": "Representation and Accountability I",
    "section": "Mandate view:",
    "text": "Mandate view:\nElections serve to select good policy\n\n\nFundamentally prospective\nDuring campaign parties inform citizens what policies they want to follow and why\nVoters select the best one\nPoliticians do what they propose (the platform is the mandate)"
  },
  {
    "objectID": "slides/c2-account1.html#taking-stock",
    "href": "slides/c2-account1.html#taking-stock",
    "title": "Representation and Accountability I",
    "section": "Taking stock",
    "text": "Taking stock\n\nDo we feel these mechanisms are plausible?\nWhat might be some issues with these characterizations of representation?"
  },
  {
    "objectID": "slides/c2-account1.html#reconsidering",
    "href": "slides/c2-account1.html#reconsidering",
    "title": "Representation and Accountability I",
    "section": "Reconsidering",
    "text": "Reconsidering\n\nCitizens are not omniscient, for better and for worse\n\nImperfect evaluation of what politicians should be doing and of whether they did what they ought to have done\nPoliticians have goals, interests, and values of their own and monitoring is costly."
  },
  {
    "objectID": "slides/c2-account1.html#reconsidering-some-potential-pitfalls",
    "href": "slides/c2-account1.html#reconsidering-some-potential-pitfalls",
    "title": "Representation and Accountability I",
    "section": "Reconsidering: Some potential pitfalls",
    "text": "Reconsidering: Some potential pitfalls\n\n\nCitizens are not omniscient, for better and for worse\nImperfect evaluation of what politicians should do\nImperfect evaluation of whether they did what they ought to have done\nPoliticians have goals, interests, and values of their own, and monitoring is costly"
  },
  {
    "objectID": "slides/c2-account1.html#pitfalls-in-the-mandate-view",
    "href": "slides/c2-account1.html#pitfalls-in-the-mandate-view",
    "title": "Representation and Accountability I",
    "section": "Pitfalls in the Mandate View",
    "text": "Pitfalls in the Mandate View\n\nIs it plausible to think governments will do what they propose?\nIs it desirable?\nPoliticians are not legally compelled to abide by their platform in any democratic system! Why?"
  },
  {
    "objectID": "slides/c2-account1.html#case-data-hypotheses",
    "href": "slides/c2-account1.html#case-data-hypotheses",
    "title": "Representation and Accountability I",
    "section": "Case, Data, Hypotheses",
    "text": "Case, Data, Hypotheses\n\nContext: Municipalities in Brazil\n\nBrazilian mayors allowed to run for reelection starting in 2000\n\nData: Misappropriated Funds\n\nRandom audits of municipalities since 2003\n\n\n\n\nHypothesis 1: Mayors that can still get reelected will steal less\nHypothesis 2: Especially when the “theft” is very visible"
  },
  {
    "objectID": "slides/c2-account1.html#research-design-1",
    "href": "slides/c2-account1.html#research-design-1",
    "title": "Representation and Accountability I",
    "section": "Research Design",
    "text": "Research Design"
  },
  {
    "objectID": "slides/c2-account1.html#research-design-2",
    "href": "slides/c2-account1.html#research-design-2",
    "title": "Representation and Accountability I",
    "section": "Research Design",
    "text": "Research Design\n  Constrain the Comparison\n\n\nCompare mayors who barely won and barely lost\nCompare 1st term mayors that later won to 2nd term mayors\nCompare mayors of equal experience\nCompare similar places only"
  },
  {
    "objectID": "slides/c2-account1.html#findings-1",
    "href": "slides/c2-account1.html#findings-1",
    "title": "Representation and Accountability I",
    "section": "Findings",
    "text": "Findings\n\nMayors who can still run for reelection are less corrupt.\n\nAround 1.9-4 pp less corrupt than 2nd term mayors\n\nBecause around 7.4% of all resources were stolen under 2nd term mayors, a 1.9-4 pp decrease is HUGE and A LOT of money"
  },
  {
    "objectID": "slides/c2-account1.html#implications-for-representation",
    "href": "slides/c2-account1.html#implications-for-representation",
    "title": "Representation and Accountability I",
    "section": "Implications for Representation",
    "text": "Implications for Representation\n\nThat 1st term mayors are less corrupt suggests votes are a good disciplining device\n\nThe accountability theory seems to have empirical support\n\nBut… is this evidence that voting does not help select good politicians?\nCould both mechanisms be operational?"
  },
  {
    "objectID": "slides/c2-account1.html#policy-implications",
    "href": "slides/c2-account1.html#policy-implications",
    "title": "Representation and Accountability I",
    "section": "Policy Implications",
    "text": "Policy Implications\n\nCorruption is more pervasive in weakly institutionalized settings\n\nHow to make local governments in those settings more accountable?\n\nImplications for term limits?\nImplications for politicians’ salaries?"
  },
  {
    "objectID": "slides/c2-account1.html#pitfals-in-the-accountability-view",
    "href": "slides/c2-account1.html#pitfals-in-the-accountability-view",
    "title": "Representation and Accountability I",
    "section": "Pitfals in the Accountability View",
    "text": "Pitfals in the Accountability View\n\nIdea is reward or punish depending on their performance\nIs it plausible to think citizens have enough information to evaluate politicians?\nWhat if politicians do not value getting reelected"
  },
  {
    "objectID": "slides/c2-account2.html#agenda",
    "href": "slides/c2-account2.html#agenda",
    "title": "Representation and Accountability II",
    "section": "Agenda",
    "text": "Agenda\n\nDunning et al. (2019)\nPower\nMultivariate Regression"
  },
  {
    "objectID": "slides/c2-account2.html#barriers-hindering-accumulation",
    "href": "slides/c2-account2.html#barriers-hindering-accumulation",
    "title": "Representation and Accountability II",
    "section": "Barriers hindering accumulation",
    "text": "Barriers hindering accumulation\n\n\nWhat is accumulation?\n\n\nBuilding knowledge across studies\n\n\n\nWhat makes it difficult?\n\n\n\nLimited replication\nHeterogeneity of design and measurement\nPublication bias"
  },
  {
    "objectID": "slides/c2-account2.html#barriers-hindering-accumulation-1",
    "href": "slides/c2-account2.html#barriers-hindering-accumulation-1",
    "title": "Representation and Accountability II",
    "section": "Barriers hindering accumulation",
    "text": "Barriers hindering accumulation"
  },
  {
    "objectID": "slides/c2-account2.html#facilitating-accumulation",
    "href": "slides/c2-account2.html#facilitating-accumulation",
    "title": "Representation and Accountability II",
    "section": "Facilitating accumulation",
    "text": "Facilitating accumulation\n\n\nWhat solutions are discussed by the authors?\n\n\nPre-registration\nHarmonizing theory, measurement, and estimation\nPublication of null results"
  },
  {
    "objectID": "slides/c2-account2.html#facilitating-accumulation-1",
    "href": "slides/c2-account2.html#facilitating-accumulation-1",
    "title": "Representation and Accountability II",
    "section": "Facilitating accumulation",
    "text": "Facilitating accumulation"
  },
  {
    "objectID": "slides/c2-account2.html#models-of-political-accountability",
    "href": "slides/c2-account2.html#models-of-political-accountability",
    "title": "Representation and Accountability II",
    "section": "Models of political accountability",
    "text": "Models of political accountability\nAccountability requires that voters:\n\nObserve performance\n\nAttribution (who’s fault?)\nBenchmarking (is this good or bad relative to what I thought?)\n\nLearn from what they see\nHave credible alternatives"
  },
  {
    "objectID": "slides/c2-account2.html#models-of-political-accountability-1",
    "href": "slides/c2-account2.html#models-of-political-accountability-1",
    "title": "Representation and Accountability II",
    "section": "Models of political accountability",
    "text": "Models of political accountability\nWhat does the literature say?\n\nTheory is mixed\n\nPartisan and sectarian attachments are strong\nVoters may be reluctant to update their beliefs\n\nExperimental results are mixed\n\nDemobilization\nLimited recall\nEphemeral effects"
  },
  {
    "objectID": "slides/c2-account2.html#research-design",
    "href": "slides/c2-account2.html#research-design",
    "title": "Representation and Accountability II",
    "section": "Research Design",
    "text": "Research Design\nIntervention\n\n\nInformation (Q) on political performance\n\nLegislative behavior\nSpending irregularities\nBudget allocation\nCandidate experience"
  },
  {
    "objectID": "slides/c2-account2.html#research-design-1",
    "href": "slides/c2-account2.html#research-design-1",
    "title": "Representation and Accountability II",
    "section": "Research Design",
    "text": "Research Design\n\nIs the treatment good news, or bad news?\n\n\nSome voters will be pleasantly surprised (Good News)\nOthers will be disappointed (Bat News)\nDepending on their prior beliefs (P) of how well politicians were performing"
  },
  {
    "objectID": "slides/c2-account2.html#ecological-validity",
    "href": "slides/c2-account2.html#ecological-validity",
    "title": "Representation and Accountability II",
    "section": "Ecological Validity",
    "text": "Ecological Validity\n\nHow is information disseminated?\nWhat is the real world activity being tested?"
  },
  {
    "objectID": "slides/c2-account2.html#describing-information",
    "href": "slides/c2-account2.html#describing-information",
    "title": "Representation and Accountability II",
    "section": "Describing Information",
    "text": "Describing Information\n\n\n\ngood news group is above the line\ncorrelation is positive but weak"
  },
  {
    "objectID": "slides/c2-account2.html#problems-hindering-accumulation",
    "href": "slides/c2-account2.html#problems-hindering-accumulation",
    "title": "Representation and Accountability II",
    "section": "Problems hindering accumulation",
    "text": "Problems hindering accumulation"
  },
  {
    "objectID": "slides/c2-account2.html#research-design-2",
    "href": "slides/c2-account2.html#research-design-2",
    "title": "Representation and Accountability II",
    "section": "Research Design",
    "text": "Research Design\n\n\nCore hypotheses:\n\n\nGood news increases voter support for incumbents\nBad news decreases voter support for incumbents\nEffect of information will increase with gap between Q and P (the “suprise” degree)\nStrongest for nonpartisan and non-coethnic voters"
  },
  {
    "objectID": "slides/c2-account2.html#findings",
    "href": "slides/c2-account2.html#findings",
    "title": "Representation and Accountability II",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/c2-account2.html#why-the-null-results",
    "href": "slides/c2-account2.html#why-the-null-results",
    "title": "Representation and Accountability II",
    "section": "Why the null results?",
    "text": "Why the null results?\n\n\nSource credibility?\n\nNo…\n\nLack of retention?\nLack of updating on politician performance?\nLack of updating about politician quality?\nIntervention is too weak?"
  },
  {
    "objectID": "slides/c2-account2.html#statistical-power",
    "href": "slides/c2-account2.html#statistical-power",
    "title": "Representation and Accountability II",
    "section": "Statistical Power",
    "text": "Statistical Power\n\n\nCan we update on “null results”?\n\nStatistical power is the probability of correctly detecting a true effect in a study\nHigher power reduces the risk of a false negative\n\nThe larger your sample size and the bigger the effect size, the more powered you are, for a given level of statistical significance"
  },
  {
    "objectID": "slides/c2-account2.html#findings-1",
    "href": "slides/c2-account2.html#findings-1",
    "title": "Representation and Accountability II",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/c2-account2.html#findings-2",
    "href": "slides/c2-account2.html#findings-2",
    "title": "Representation and Accountability II",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/c2-account2.html#findings-3",
    "href": "slides/c2-account2.html#findings-3",
    "title": "Representation and Accountability II",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/c2-account2.html#policy-implications",
    "href": "slides/c2-account2.html#policy-implications",
    "title": "Representation and Accountability II",
    "section": "Policy Implications",
    "text": "Policy Implications\n\nShould interventions to provide information be re-thought?\nWhy did Ferraz and Finan (2011) find an effect but here they do not?"
  },
  {
    "objectID": "slides/c2-account1.html",
    "href": "slides/c2-account1.html",
    "title": "Representation and Accountability I",
    "section": "",
    "text": "Two assignments due TODAY at midnight:\n\n\nCreating a Final Project Git Repo\nIdea for Final Research Project\n\n\nPlease send a Slack DM to both me and Jeremy"
  },
  {
    "objectID": "slides/c2-account2.html#democracy-and-accountability",
    "href": "slides/c2-account2.html#democracy-and-accountability",
    "title": "Representation and Accountability II",
    "section": "Democracy and Accountability",
    "text": "Democracy and Accountability\n\n\nBasic idea thus far:\nIf voters can observe how politicians behave, votes can discipline politicians, then we’ll have better (behaved) politicians"
  },
  {
    "objectID": "slides/c2-account1.html#case-data",
    "href": "slides/c2-account1.html#case-data",
    "title": "Representation and Accountability I",
    "section": "Case, Data",
    "text": "Case, Data\n\nContext: Municipalities in Brazil\n\nBrazilian mayors allowed to run for reelection starting in 2000\n\nData: Misappropriated Funds\n\nRandom audits of municipalities since 2003"
  },
  {
    "objectID": "slides/c2-account1.html#hypotheses",
    "href": "slides/c2-account1.html#hypotheses",
    "title": "Representation and Accountability I",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\nHypothesis 1: Mayors that can still get reelected will steal less\nHypothesis 2: Especially when the “theft” is very visible"
  },
  {
    "objectID": "slides/c2-account1.html#robustness",
    "href": "slides/c2-account1.html#robustness",
    "title": "Representation and Accountability I",
    "section": "Robustness",
    "text": "Robustness\n\n\n“If my story is true, what else should I observe in the data?”\n\n\n1s term mayors procure more $ from the fed. gov (effort)\nEffects smaller where opposition is weak (who else are you going to vote for??)"
  },
  {
    "objectID": "slides/c2-account2.html#democracy-and-accountability-1",
    "href": "slides/c2-account2.html#democracy-and-accountability-1",
    "title": "Representation and Accountability II",
    "section": "Democracy and Accountability",
    "text": "Democracy and Accountability\n \nFunders have had the same idea!\n\nA lot of money has been spent on “information interventions”\n\nTell voters of politicians’ good/bad behavior\n\nAnd yet, we do not really know if they work\n\nIt has been difficult to accumulate information"
  },
  {
    "objectID": "slides/c2-account2.html#publication-bias",
    "href": "slides/c2-account2.html#publication-bias",
    "title": "Representation and Accountability II",
    "section": "Publication bias",
    "text": "Publication bias\n\nDo you think it is easier to publish a study that finds an effect or one that does not find any effect?\nWhat are the consequences for what we, as a discipline, “know”?"
  },
  {
    "objectID": "slides/c2-account2.html#statistical-power-1",
    "href": "slides/c2-account2.html#statistical-power-1",
    "title": "Representation and Accountability II",
    "section": "Statistical Power",
    "text": "Statistical Power\nRecall the interpretation of p-values:\n\n\nThe probability of observing a test statistic at least as extreme as the one you observed if the true parameter value is zero\nOr, the probability of rejecting the null hypothesis when the null was true\nThis is called a “Type I” error: a false positive\nWe also have “Type II” errors: a false negative"
  },
  {
    "objectID": "slides/c2-account2.html#statistical-power-2",
    "href": "slides/c2-account2.html#statistical-power-2",
    "title": "Representation and Accountability II",
    "section": "Statistical Power",
    "text": "Statistical Power\nPower is the probability of correctly accepting the alternative hypothesis\n\n\nThe probability of a true positive\n\nEquals (1 - probability of type II error)\nThe common threashold in the discipline is 80% power\n\n\n\nYou can check out the EGEN power calculator to understand better"
  },
  {
    "objectID": "slides/c2-account2.html#statistical-power-3",
    "href": "slides/c2-account2.html#statistical-power-3",
    "title": "Representation and Accountability II",
    "section": "Statistical Power",
    "text": "Statistical Power\nNull results were not “foregone conclusion”?\n\n\n80% power\nChange the vote of 5/100 voters\nChange turnout of 4/100 voters"
  },
  {
    "objectID": "slides/c2-account2.html#statistical-power-4",
    "href": "slides/c2-account2.html#statistical-power-4",
    "title": "Representation and Accountability II",
    "section": "Statistical Power",
    "text": "Statistical Power\nNull results were not “foregone conclusion”?\n\n\n80% power\nChange the vote of 5/100 voters\nChange turnout of 4/100 voters"
  },
  {
    "objectID": "slides/c2-account2.html#findings-4",
    "href": "slides/c2-account2.html#findings-4",
    "title": "Representation and Accountability II",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "slides/24-git.html#installing-git",
    "href": "slides/24-git.html#installing-git",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git\n\nOpen the terminal/command prompt\nCheck if you have git installed\n\ngit --version"
  },
  {
    "objectID": "slides/24-git.html#installing-git-1",
    "href": "slides/24-git.html#installing-git-1",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git\n\n\nIf yes, consider running an update\ngit update-git-for-windows"
  },
  {
    "objectID": "slides/24-git.html#installing-git-2",
    "href": "slides/24-git.html#installing-git-2",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git\nIf no, install git\n\nInstallation instructions\n\nDownload .exe and double-click\n\nTwo things to watch for:\n\nAdjusting the name of the initial branch: Override the default branch name (select ‘main’)\nAdjusting your PATH: make sure to select command line and 3rd-party"
  },
  {
    "objectID": "slides/24-git.html#installing-git-3",
    "href": "slides/24-git.html#installing-git-3",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git"
  },
  {
    "objectID": "slides/24-git.html#installing-git-4",
    "href": "slides/24-git.html#installing-git-4",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git"
  },
  {
    "objectID": "slides/24-git.html#installing-git-5",
    "href": "slides/24-git.html#installing-git-5",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git"
  },
  {
    "objectID": "slides/24-git.html#installing-git-6",
    "href": "slides/24-git.html#installing-git-6",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git"
  },
  {
    "objectID": "slides/24-git.html#installing-git-7",
    "href": "slides/24-git.html#installing-git-7",
    "title": "Git, Github, Github Pages",
    "section": "Installing git",
    "text": "Installing git"
  },
  {
    "objectID": "slides/24-git.html#git-basics",
    "href": "slides/24-git.html#git-basics",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\nGit is a version control program, so you can avoid…\nanalysis.R\nanalysis_v1.R\nanalysis_v2.R\nanalysis_v2_FINAL.R\n\nVersion control allows you to precisely track changes"
  },
  {
    "objectID": "slides/24-git.html#git-basics-1",
    "href": "slides/24-git.html#git-basics-1",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\n\n\nShow off your productivity\n\n\n\n\n\nShare projects publicly"
  },
  {
    "objectID": "slides/24-git.html#git-basics-2",
    "href": "slides/24-git.html#git-basics-2",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\n\n\nGit hosts data and code\n\n“Remote” (main) on github.com\n“Local” on your harddrive(s) in a designated folder\nResembles Dropbox/Drive synch + Version history\nMany advantages; a few disadvantages"
  },
  {
    "objectID": "slides/24-git.html#git-basics-3",
    "href": "slides/24-git.html#git-basics-3",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\nVersion control\n\n\nPrecisely tracks changes\nRevert to old versions\nAvoid devastating loss\nNote: everything is public by default"
  },
  {
    "objectID": "slides/24-git.html#connect-your-github-account",
    "href": "slides/24-git.html#connect-your-github-account",
    "title": "Git, Github, Github Pages",
    "section": "Connect your GitHub account",
    "text": "Connect your GitHub account\n\n\n\nOpen the terminal and enter the code below\nReplace \"Your Name\"and \"yourname@email.edu\" with your name/email used to sign up for GitHub\nRun the code\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"yourname@email.edu\""
  },
  {
    "objectID": "slides/24-git.html#connect-your-github-account-1",
    "href": "slides/24-git.html#connect-your-github-account-1",
    "title": "Git, Github, Github Pages",
    "section": "Connect your GitHub account",
    "text": "Connect your GitHub account\nCheck that the confirmation worked\ngit config --list"
  },
  {
    "objectID": "slides/24-git.html#install-a-git-client",
    "href": "slides/24-git.html#install-a-git-client",
    "title": "Git, Github, Github Pages",
    "section": "Install a git client",
    "text": "Install a git client\n\nDownload GitHub Desktop .exe\nDouble click the .exe"
  },
  {
    "objectID": "slides/24-git.html#install-a-git-client-1",
    "href": "slides/24-git.html#install-a-git-client-1",
    "title": "Git, Github, Github Pages",
    "section": "Install a git client",
    "text": "Install a git client"
  },
  {
    "objectID": "slides/24-git.html#clone-your-repo",
    "href": "slides/24-git.html#clone-your-repo",
    "title": "Git, Github, Github Pages",
    "section": "Clone your repo",
    "text": "Clone your repo"
  },
  {
    "objectID": "slides/24-git.html#clone-your-repo-1",
    "href": "slides/24-git.html#clone-your-repo-1",
    "title": "Git, Github, Github Pages",
    "section": "Clone your repo",
    "text": "Clone your repo"
  },
  {
    "objectID": "slides/24-git.html#clone-your-repo-2",
    "href": "slides/24-git.html#clone-your-repo-2",
    "title": "Git, Github, Github Pages",
    "section": "Clone your repo",
    "text": "Clone your repo"
  },
  {
    "objectID": "slides/24-git.html#commit-changes",
    "href": "slides/24-git.html#commit-changes",
    "title": "Git, Github, Github Pages",
    "section": "Commit changes",
    "text": "Commit changes"
  },
  {
    "objectID": "slides/24-git.html#push-to-your-repo",
    "href": "slides/24-git.html#push-to-your-repo",
    "title": "Git, Github, Github Pages",
    "section": "Push to your repo",
    "text": "Push to your repo"
  },
  {
    "objectID": "slides/24-git.html#pull-from-your-repo",
    "href": "slides/24-git.html#pull-from-your-repo",
    "title": "Git, Github, Github Pages",
    "section": "Pull from your repo",
    "text": "Pull from your repo"
  },
  {
    "objectID": "slides/24-git.html#create-a-website",
    "href": "slides/24-git.html#create-a-website",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website\nMoving to RStudio\n\nFile \\(\\rightarrow\\) New Project \\(\\rightarrow\\) New Directory \\(\\rightarrow\\) Quarto Website"
  },
  {
    "objectID": "slides/24-git.html#create-a-website-1",
    "href": "slides/24-git.html#create-a-website-1",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website"
  },
  {
    "objectID": "slides/24-git.html#create-a-website-2",
    "href": "slides/24-git.html#create-a-website-2",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website\nChange output director to docs"
  },
  {
    "objectID": "slides/24-git.html#create-a-website-3",
    "href": "slides/24-git.html#create-a-website-3",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website\nMoving to RStudio\n\nFile \\(\\rightarrow\\) New Project \\(\\rightarrow\\) New Directory \\(\\rightarrow\\) Quarto Website"
  },
  {
    "objectID": "slides/24-git.html#create-a-website-4",
    "href": "slides/24-git.html#create-a-website-4",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website"
  },
  {
    "objectID": "slides/24-git.html#create-a-website-5",
    "href": "slides/24-git.html#create-a-website-5",
    "title": "Git, Github, Github Pages",
    "section": "Create a website",
    "text": "Create a website\nChange output director to docs"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages",
    "href": "slides/24-git.html#publish-to-github-pages",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\n\nKeep a repository of your website\nPush changes to your website via Github\nSee changes almost instantly"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-1",
    "href": "slides/24-git.html#publish-to-github-pages-1",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\n\nPublish local git repo to github.com\nUncheck ‘Keep this code private’\nOpen repo on github.com\nSettings \\(\\rightarrow\\) Pages (left-sidebar)"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-2",
    "href": "slides/24-git.html#publish-to-github-pages-2",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-3",
    "href": "slides/24-git.html#publish-to-github-pages-3",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-4",
    "href": "slides/24-git.html#publish-to-github-pages-4",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-5",
    "href": "slides/24-git.html#publish-to-github-pages-5",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-6",
    "href": "slides/24-git.html#publish-to-github-pages-6",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#publish-to-github-pages-7",
    "href": "slides/24-git.html#publish-to-github-pages-7",
    "title": "Git, Github, Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git.html#host-your-final-project",
    "href": "slides/24-git.html#host-your-final-project",
    "title": "Git, Github, Github Pages",
    "section": "Host Your Final Project",
    "text": "Host Your Final Project\n\nDelete _site folder (now its using docs)\nCreate data folder to store your dataset\nAdd final project .qmd file to your repo (or drop it into index.qmd)\nUse _quarto.yml to add new pages to navigation bar\nRender index.qmd; confirm that other pages have been rendered\nPush commit and check that the website updated\n\n\n\n\njrspringman.github.io/psci3200-globaldev/"
  },
  {
    "objectID": "materials/12_github.html",
    "href": "materials/12_github.html",
    "title": "Git, Github, Github Pages",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "materials/12_github.html#required-readings",
    "href": "materials/12_github.html#required-readings",
    "title": "Git, Github, Github Pages",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "materials/12_github.html#slides",
    "href": "materials/12_github.html#slides",
    "title": "Git, Github, Github Pages",
    "section": "Slides",
    "text": "Slides\n\nSlides"
  },
  {
    "objectID": "materials/c3_crime.html",
    "href": "materials/c3_crime.html",
    "title": "Crime and Punishment",
    "section": "",
    "text": "Crime: Bateson (2012)\nPunishment: Weaver and Lerman (2010)"
  },
  {
    "objectID": "materials/c3_crime.html#required-readings",
    "href": "materials/c3_crime.html#required-readings",
    "title": "Crime and Punishment",
    "section": "",
    "text": "Crime: Bateson (2012)\nPunishment: Weaver and Lerman (2010)"
  },
  {
    "objectID": "slides/c2-account2.html",
    "href": "slides/c2-account2.html",
    "title": "Representation and Accountability II",
    "section": "",
    "text": "Dunning et al. (2019)\nPower\nMultivariate Regression"
  },
  {
    "objectID": "materials/25_github2.html",
    "href": "materials/25_github2.html",
    "title": "Github Pages",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "materials/25_github2.html#required-readings",
    "href": "materials/25_github2.html#required-readings",
    "title": "Github Pages",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "materials/25_github2.html#slides",
    "href": "materials/25_github2.html#slides",
    "title": "Github Pages",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "slides/24-git.html",
    "href": "slides/24-git.html",
    "title": "Git, Github, Github Pages",
    "section": "",
    "text": "Expect feedback on Research Question and Data by Wed EOD\nNext deadline is March 5\n\n\n\n\n\n\n\nWarning\n\n\n\nLate assignments deduct 2 points per day\n\n\n\n\n\n\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "materials/c4_crime.html",
    "href": "materials/c4_crime.html",
    "title": "Crime and Punishment II",
    "section": "",
    "text": "Flores-Macías and Zarkin, 2024"
  },
  {
    "objectID": "materials/c4_crime.html#required-readings",
    "href": "materials/c4_crime.html#required-readings",
    "title": "Crime and Punishment II",
    "section": "",
    "text": "Flores-Macías and Zarkin, 2024"
  },
  {
    "objectID": "materials/c4_crime.html#suggested-reading",
    "href": "materials/c4_crime.html#suggested-reading",
    "title": "Crime and Punishment II",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nSlough and Fariss, 2021"
  },
  {
    "objectID": "materials/12_github.html#resources",
    "href": "materials/12_github.html#resources",
    "title": "Git, Github, Github Pages",
    "section": "Resources",
    "text": "Resources\n\nTo learn more about using git with R, check out happygitwithr.com\nIf you don’t have git installed on your laptop, use these installation instructions"
  },
  {
    "objectID": "slides/24-git.html#git-basics-4",
    "href": "slides/24-git.html#git-basics-4",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\nCollaboration\n\n\nWhen you collaborate, it’s clear who to blame\nSimultaneous editing can cause challenges \nPlease don’t create conflicts with yourself"
  },
  {
    "objectID": "slides/24-git.html#git-basics-5",
    "href": "slides/24-git.html#git-basics-5",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\nBasic commands\n\ngit pull origin main\ngit add .\ngit commit -m \"describe your changes or vent frustration\"\ngit push origin main\ngit pull origin main"
  },
  {
    "objectID": "slides/24-git.html#assignments",
    "href": "slides/24-git.html#assignments",
    "title": "Git, Github, Github Pages",
    "section": "Assignments",
    "text": "Assignments\n\nExpect feedback on Research Question and Data by Wed EOD\nNext deadline is March 5\nLate assignments deduct 2 points per day\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "slides/24-git.html#final-project-assignments",
    "href": "slides/24-git.html#final-project-assignments",
    "title": "Git, Github, Github Pages",
    "section": "Final Project Assignments",
    "text": "Final Project Assignments\n\nExpect feedback on Research Question and Data by Wed EOD\nNext deadline is March 5\n\n\n\n\n\n\n\nWarning\n\n\nLate assignments deduct 2 points per day"
  },
  {
    "objectID": "slides/24-git.html#final-project-assignments-1",
    "href": "slides/24-git.html#final-project-assignments-1",
    "title": "Git, Github, Github Pages",
    "section": "Final Project Assignments",
    "text": "Final Project Assignments\n\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreate a GitHub repository\nFeb 17\n\n\nResearch Question and Data\nFeb 19\n\n\nResearch Design\nMar 5\n\n\nSubmit proposal\nApr 2\n\n\nSubmit final project\nMay 10"
  },
  {
    "objectID": "slides/24-git.html#git-basics-6",
    "href": "slides/24-git.html#git-basics-6",
    "title": "Git, Github, Github Pages",
    "section": "git Basics",
    "text": "git Basics\n\n\nGit can be complicated\n\nOften used for serious software development\nBranches, conflicts, merges, rebase\nMassive online community to help with more sophisticated use"
  },
  {
    "objectID": "materials/c3_crime.html#slides",
    "href": "materials/c3_crime.html#slides",
    "title": "Crime and Punishment",
    "section": "Slides",
    "text": "Slides\n\nSlides"
  },
  {
    "objectID": "slides/c3-crime.html",
    "href": "slides/c3-crime.html",
    "title": "Crime and Punishment I",
    "section": "",
    "text": "Bateson, 2012\nWeaver and Lerman, 2010"
  },
  {
    "objectID": "slides/c3-crime.html#agenda",
    "href": "slides/c3-crime.html#agenda",
    "title": "Crime and Punishment I",
    "section": "Agenda",
    "text": "Agenda\n\nBateson, 2012\nWeaver and Lerman, 2010"
  },
  {
    "objectID": "slides/c3-crime.html#what-do-states-do",
    "href": "slides/c3-crime.html#what-do-states-do",
    "title": "Crime and Punishment I",
    "section": "What do states do?",
    "text": "What do states do?\n\nStates provide public goods/services:\n\nHealth, schools, infrastructure\nThe provision of security (internal and external)"
  },
  {
    "objectID": "slides/c3-crime.html#security-and-development",
    "href": "slides/c3-crime.html#security-and-development",
    "title": "Crime and Punishment I",
    "section": "Security and Development",
    "text": "Security and Development\n\nSecurity often thought as a prerequisite of development\n\nWhy?\n\nConversely, internal violence (civil war) characterized as “development in reverse” (Collier, 2004)\n\nWhy?"
  },
  {
    "objectID": "slides/c3-crime.html#the-inevitablity-of-internal-conflic",
    "href": "slides/c3-crime.html#the-inevitablity-of-internal-conflic",
    "title": "Crime and Punishment I",
    "section": "The inevitablity of internal conflic",
    "text": "The inevitablity of internal conflic\n\nYet, states often fail to provide internal security\nParticularly developing states\nPolitical violence, civil war, and most commonly… crime"
  },
  {
    "objectID": "slides/c3-crime.html#crime-in-the-developing-world",
    "href": "slides/c3-crime.html#crime-in-the-developing-world",
    "title": "Crime and Punishment I",
    "section": "Crime in the Developing World",
    "text": "Crime in the Developing World\nCrime often identified as main concern of developing-world citizens"
  },
  {
    "objectID": "slides/c3-crime.html#crime-in-the-developing-world-1",
    "href": "slides/c3-crime.html#crime-in-the-developing-world-1",
    "title": "Crime and Punishment I",
    "section": "Crime in the Developing World",
    "text": "Crime in the Developing World\n\nConsequences for Development?\n\n\nIn Latam, the most violent region in the world, the direct costs of crime in 2022 reached 3.44% of the region’s GDP (Source: IDB)\n\n“The cost of crime is equivalent to 78% of the public budget for education, double the public budget for social assistance, and 12 times the budget for research and development.”\n\nAt least 115k people were murdered in Latin America and the Caribbean during 2023"
  },
  {
    "objectID": "slides/c3-crime.html#police",
    "href": "slides/c3-crime.html#police",
    "title": "Crime and Punishment I",
    "section": "Police",
    "text": "Police\n\nThe state’s main tool to respond to insecurity is the police\nYet police in developing context are often repressive, abusive, over-target poor and marginalized communities.\n\n\n\nCould policing backfire?"
  },
  {
    "objectID": "slides/c3-crime.html#crime-and-its-punishmemnt",
    "href": "slides/c3-crime.html#crime-and-its-punishmemnt",
    "title": "Crime and Punishment I",
    "section": "Crime and its Punishmemnt",
    "text": "Crime and its Punishmemnt\n\nWhat are the consequences of criminal violence?\nWhat are the consequences of poor policing?"
  },
  {
    "objectID": "slides/c3-crime.html#bateson-2012",
    "href": "slides/c3-crime.html#bateson-2012",
    "title": "Crime and Punishment I",
    "section": "Bateson, 2012",
    "text": "Bateson, 2012\nResearch Question: Can crime motivate people to participate more in politics?\n\n\nIf the answer is “no” then crime could trigger a vicious circle of disengagement \\(\\rightarrow\\) poor governance \\(\\rightarrow\\) worse developmental outcomes\n\n\n\n\nIf the answer is “yes” then violence today might (ironically) mean better outcomes tomorrow: crime \\(\\rightarrow\\) participation \\(\\rightarrow\\) Better development outcomes"
  },
  {
    "objectID": "slides/c3-crime.html#theoretical-expectations",
    "href": "slides/c3-crime.html#theoretical-expectations",
    "title": "Crime and Punishment I",
    "section": "Theoretical Expectations",
    "text": "Theoretical Expectations\n\nWhy might we expect crime to depress political participation?\n\n\nFear\nCost of participating\n\n\nWhy might we expect crime to spur political participation?\n\n\nPsychological reasons\nInstrumental reasons"
  },
  {
    "objectID": "slides/c3-crime.html#a-quick-aside",
    "href": "slides/c3-crime.html#a-quick-aside",
    "title": "Crime and Punishment I",
    "section": "A quick aside",
    "text": "A quick aside\n\nWhat were your priors?"
  },
  {
    "objectID": "slides/c3-crime.html#context-data",
    "href": "slides/c3-crime.html#context-data",
    "title": "Crime and Punishment I",
    "section": "Context / Data",
    "text": "Context / Data\n\nAuthor leverages four regional surveys\n\nAmericas Barometer\nEurobarometer\nAsian Barometer\nAfrobarometer\n\nYou can download all of these surveys yourself!"
  },
  {
    "objectID": "slides/c3-crime.html#measurement",
    "href": "slides/c3-crime.html#measurement",
    "title": "Crime and Punishment I",
    "section": "Measurement",
    "text": "Measurement\nWhat is the dependent variable?\n\n\nNon-electoral participation\n\ncommunity action\nprotest\npolitical interest\ntown meetings\n\n\n\nWhat is the treatment or independent variable?\n\n\nDirect or indirect criminal victimization in the past year"
  },
  {
    "objectID": "slides/c3-crime.html#research-design",
    "href": "slides/c3-crime.html#research-design",
    "title": "Crime and Punishment I",
    "section": "Research Design",
    "text": "Research Design\n\nConditional Independence Assumption or Selection on Observables\nTreatment Group:\n\n\nDirect or indirect victims\n\n\nControl Group:\n\n\nNon-victims\n\n\nControls:\n\n\nAge, Education, Gender, Urban, Country FE"
  },
  {
    "objectID": "slides/c3-crime.html#threats-to-inference",
    "href": "slides/c3-crime.html#threats-to-inference",
    "title": "Crime and Punishment I",
    "section": "Threats to Inference",
    "text": "Threats to Inference\nThe author makes causal claims, what crucial assumption is she making?\n\n\nNo confounders!\n\nAfter including controls, who gets victimized is independent of participation decisions\n\nAssumption would not hold if:\n\nPeople who participate more are more likely to be crimed\nPeople who participate more are more likely to report they were crimed"
  },
  {
    "objectID": "slides/c3-crime.html#is-causality-plausible",
    "href": "slides/c3-crime.html#is-causality-plausible",
    "title": "Crime and Punishment I",
    "section": "Is Causality Plausible?",
    "text": "Is Causality Plausible?"
  },
  {
    "objectID": "slides/c3-crime.html#results",
    "href": "slides/c3-crime.html#results",
    "title": "Crime and Punishment I",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/c3-crime.html#updating-on-the-results",
    "href": "slides/c3-crime.html#updating-on-the-results",
    "title": "Crime and Punishment I",
    "section": "Updating on the Results",
    "text": "Updating on the Results\nCrime victims everywhere participate more in politics, regadless of the type of crime\n\nEffect is roughly the same as 5-10 years of education (!!!)\nAuthor also finds that victims are more skeptical of democracy, support authoritarianism, and vigilantism\n\nQ1: Do we believe the results? Q2: What implications for democracy and development?"
  },
  {
    "objectID": "slides/c3-crime.html#weaver-and-lerman-2010",
    "href": "slides/c3-crime.html#weaver-and-lerman-2010",
    "title": "Crime and Punishment I",
    "section": "Weaver and Lerman, 2010",
    "text": "Weaver and Lerman, 2010\nFor US citizens, contact with the criminal justice system is more common now than ever before\n\n1/100 US citizens are incarcerate\n1/3 black men will sever time in prision\n\nCitizens routinely have unwanted/disciplinary interawctions with police\n\nEspecially the disadvantaged populations\n\n\n\nWhat do the authors refer to as the “carceral state”?\n\n- spacially concentrated, punitive, surveillance-oriented system found in some (minority) communities"
  },
  {
    "objectID": "slides/c3-crime.html#research-question",
    "href": "slides/c3-crime.html#research-question",
    "title": "Crime and Punishment I",
    "section": "Research Question",
    "text": "Research Question\nHow and in what ways encounters with the criminal justice system influence citizens’ political attitudes?\n\nMore specifically:\n\nHow does exposure to the criminal justice system socialize those exposed to it?\nWhat do people learn from constant, unwanted, aggressive, interactions with the state’s criminal system?"
  },
  {
    "objectID": "slides/c3-crime.html#theory",
    "href": "slides/c3-crime.html#theory",
    "title": "Crime and Punishment I",
    "section": "Theory",
    "text": "Theory\nInteractions with the criminal system shape participation through two channels:\n\nResources:\n\nPolicy, including security policy, shapes availability of resources (i.e. spend $ in court, have a criminal record, etc.)\nFewer resources \\(\\rightarrow\\) less political participation\n\nLearning:\n\nWhat the authors call “interpretative effects”\nEncounters with bureaucrats teach citizens’ about the goals and nature of the government\nBad experiences with police, courts, etc. may translate into bad impressions about the government"
  },
  {
    "objectID": "slides/c3-crime.html#hypotheses",
    "href": "slides/c3-crime.html#hypotheses",
    "title": "Crime and Punishment I",
    "section": "Hypotheses",
    "text": "Hypotheses\nInteractions with the criminal justice system will:\n\n\nDepress participation and civic engagement\nReduce trust in the government"
  },
  {
    "objectID": "slides/c3-crime.html#data",
    "href": "slides/c3-crime.html#data",
    "title": "Crime and Punishment I",
    "section": "Data",
    "text": "Data\nData comes from two panel surveys\n\nNational Longitudinal Study of Adolescent Health (Add Health)\n\n“20,000 adolescents who were in grades 7-12 during the 1994-95 school year, and have been followed for five waves to date”\n\nFragile Families and Child Well-being Study\n\nBoth surveys are publicly available and VERY RICH (and linked in the slides)"
  },
  {
    "objectID": "slides/c3-crime.html#measurement-1",
    "href": "slides/c3-crime.html#measurement-1",
    "title": "Crime and Punishment I",
    "section": "Measurement",
    "text": "Measurement\nTreatment:\n\n\nCriminal Justice Contact: no encounters, stopped by the police, charged, arrested, served time)\n\n\nOutcomes:\n\n\nElectoral participation\nCivic participation\nPolitical participation\nTrust in government\nCivic obligations (jury, military)"
  },
  {
    "objectID": "slides/c3-crime.html#research-design-1",
    "href": "slides/c3-crime.html#research-design-1",
    "title": "Crime and Punishment I",
    "section": "Research Design",
    "text": "Research Design\nConditional Independence Assumption or Selection on Observables\n\nMultivariate OLS regression\nThreats to inference: confounders!\nWhat might predict participation and contact with criminal system?\n\n\n\nPersonality, criminality, income, education, etc.\nAuthors try to control for all these and run placebos to test whether they might be driving the results"
  },
  {
    "objectID": "slides/c3-crime.html#results-1",
    "href": "slides/c3-crime.html#results-1",
    "title": "Crime and Punishment I",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/c3-crime.html#implications",
    "href": "slides/c3-crime.html#implications",
    "title": "Crime and Punishment I",
    "section": "Implications",
    "text": "Implications\n\nCriminal justice contact deters political and civic life\nEspecially for marginalized populations who are over-policed\nWho should security policy be responsive to?\nImplications for democracy?\nFor inequality?"
  },
  {
    "objectID": "slides/c3-crime.html#bringing-it-together",
    "href": "slides/c3-crime.html#bringing-it-together",
    "title": "Crime and Punishment I",
    "section": "Bringing it together",
    "text": "Bringing it together\n\nBoth crime and its punishment have important implications for politics\nA cause and a consequence of development\nWhat is the appropriate response to crime? A very hard question to answer"
  },
  {
    "objectID": "slides/c3-crime.html#for-researchers",
    "href": "slides/c3-crime.html#for-researchers",
    "title": "Crime and Punishment I",
    "section": "For researchers",
    "text": "For researchers\n\nBoth of these papers were published in the highest ranked journal of political science\nBoth use survey data which you yourself could have downloaded\nYou could have run all the analyses!\nWhy are they published where they are?\nWhy are they so influential?"
  },
  {
    "objectID": "slides/c3-crime.html#why-does-the-state-exist",
    "href": "slides/c3-crime.html#why-does-the-state-exist",
    "title": "Crime and Punishment I",
    "section": "Why does the state exist?",
    "text": "Why does the state exist?\n\nThe provision of security as the central reason for the state\n\nHobbes: Prevent chaos \\(\\rightarrow\\) Protect our lives\nLocke: Prevent abuse \\(\\rightarrow\\) Protect our property (including our lives)"
  },
  {
    "objectID": "slides/24-git2.html#update",
    "href": "slides/24-git2.html#update",
    "title": "Github Pages",
    "section": "",
    "text": "Today’s lecture shifted to Mon\nFeedback on Research Question and Data will come Thursday\nAny research proposals focused on environmental topics?\nMarch 5 Research Design shifted 24 hours\nSending .html files"
  },
  {
    "objectID": "slides/24-git2.html#pull-from-your-repo",
    "href": "slides/24-git2.html#pull-from-your-repo",
    "title": "Git, Github, Github Pages",
    "section": "Pull from your repo",
    "text": "Pull from your repo\n\n\n\n\n\nChange output director to docs"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages",
    "href": "slides/24-git2.html#publish-to-github-pages",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\n\nPublish local git repo to github.com\nUncheck ‘Keep this code private’\nOpen repo on github.com\nSettings \\(\\rightarrow\\) Pages (left-sidebar)"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-1",
    "href": "slides/24-git2.html#publish-to-github-pages-1",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nOpen Github Desktop and add repo"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-2",
    "href": "slides/24-git2.html#publish-to-github-pages-2",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nOpen Github Desktop and add repo"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-3",
    "href": "slides/24-git2.html#publish-to-github-pages-3",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nYou shouldn’t get this error"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-4",
    "href": "slides/24-git2.html#publish-to-github-pages-4",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nUncheck ‘Keep this code private’"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-5",
    "href": "slides/24-git2.html#publish-to-github-pages-5",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nOpen github.com/repo/settings"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-6",
    "href": "slides/24-git2.html#publish-to-github-pages-6",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nSettings \\(\\rightarrow\\) Pages (left-sidebar)"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-7",
    "href": "slides/24-git2.html#publish-to-github-pages-7",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages\nBranch \\(\\rightarrow\\) main \\(\\rightarrow\\) /docs \\(\\rightarrow\\) Save"
  },
  {
    "objectID": "slides/24-git2.html#host-your-final-project",
    "href": "slides/24-git2.html#host-your-final-project",
    "title": "Github Pages",
    "section": "Host Your Final Project",
    "text": "Host Your Final Project\n\nDelete _site folder (now its using docs)\nCreate data folder to store your dataset\nAdd final project .qmd file to your repo (or drop it into index.qmd)\nUse _quarto.yml to add new pages to navigation bar\nRender index.qmd; confirm that other pages have been rendered\nPush commit and check that the website updated"
  },
  {
    "objectID": "slides/24-git2.html#publishing-a-free-website-with-github-pages",
    "href": "slides/24-git2.html#publishing-a-free-website-with-github-pages",
    "title": "Github Pages",
    "section": "Publishing a free website with Github pages",
    "text": "Publishing a free website with Github pages\n\nKeep a repository of your website\nPush changes to your website via Github\nSee changes almost instantly"
  },
  {
    "objectID": "slides/24-git2.html#approach",
    "href": "slides/24-git2.html#approach",
    "title": "Github Pages",
    "section": "Approach",
    "text": "Approach\n\nWe’re going to create a personal website using your git username\nYou can use this method to create separate websites that branch out from your personal site\n\nEx. My personal website \\(\\rightarrow\\) my course website"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website",
    "href": "slides/24-git2.html#create-a-website",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\nMoving to RStudio\n\nFile \\(\\rightarrow\\) New Project \\(\\rightarrow\\) New Directory \\(\\rightarrow\\) Quarto Website"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website-1",
    "href": "slides/24-git2.html#create-a-website-1",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\n\nDirectory Name: Use your git handle (ex. jrspringman)\nSubdirectory: use folder containing your repo from Monday"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website-2",
    "href": "slides/24-git2.html#create-a-website-2",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\n\nCheck Create a git repository\nUncheck Use visual markdown editor"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-8",
    "href": "slides/24-git2.html#publish-to-github-pages-8",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-9",
    "href": "slides/24-git2.html#publish-to-github-pages-9",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-10",
    "href": "slides/24-git2.html#publish-to-github-pages-10",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-11",
    "href": "slides/24-git2.html#publish-to-github-pages-11",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-12",
    "href": "slides/24-git2.html#publish-to-github-pages-12",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#publish-to-github-pages-13",
    "href": "slides/24-git2.html#publish-to-github-pages-13",
    "title": "Github Pages",
    "section": "Publish to Github pages",
    "text": "Publish to Github pages"
  },
  {
    "objectID": "slides/24-git2.html#create-a-free-website-with-github-pages",
    "href": "slides/24-git2.html#create-a-free-website-with-github-pages",
    "title": "Github Pages",
    "section": "Create a free website with Github Pages",
    "text": "Create a free website with Github Pages\n\nKeep a repository of your website\nPush changes to your website via Github\nSee changes almost instantly"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website-3",
    "href": "slides/24-git2.html#create-a-website-3",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\n6 files in your repo"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website-4",
    "href": "slides/24-git2.html#create-a-website-4",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\n\nRender index.qmd to view local instance in browser\nabout.qmd is additional tab\n_quarto.yml is yaml parameters for the entire website\n.Rproj is an R Project file"
  },
  {
    "objectID": "slides/24-git2.html#create-a-website-5",
    "href": "slides/24-git2.html#create-a-website-5",
    "title": "Github Pages",
    "section": "Create a website",
    "text": "Create a website\nIn _quarto.yml, change output director to docs"
  },
  {
    "objectID": "slides/24-git2.html#more-complex-stuff",
    "href": "slides/24-git2.html#more-complex-stuff",
    "title": "Github Pages",
    "section": "More Complex Stuff",
    "text": "More Complex Stuff\n\nTerminal: quarto render\nTerminal: quarto render file.qmd"
  },
  {
    "objectID": "slides/c3-crime.html#the-inevitability-of-internal-conflict",
    "href": "slides/c3-crime.html#the-inevitability-of-internal-conflict",
    "title": "Crime and Punishment I",
    "section": "The inevitability of internal conflict",
    "text": "The inevitability of internal conflict\n\nYet, states often fail to provide internal security\nParticularly developing states\nPolitical violence, civil war, and most commonly… crime"
  },
  {
    "objectID": "slides/c3-crime.html#crime-and-its-punishment",
    "href": "slides/c3-crime.html#crime-and-its-punishment",
    "title": "Crime and Punishment I",
    "section": "Crime and its Punishment",
    "text": "Crime and its Punishment\n\nWhat are the consequences of criminal violence?\nWhat are the consequences of poor policing?"
  },
  {
    "objectID": "slides/c3-crime.html#context",
    "href": "slides/c3-crime.html#context",
    "title": "Crime and Punishment I",
    "section": "Context",
    "text": "Context\n\nFor US citizens, contact with the criminal justice system is more common now than ever before\n\n1/100 US citizens are incarcerate\n1/3 black men will sever time in prison\nCitizens routinely have unwanted/disciplinary interactions with police\nEspecially the disadvantaged populations\n\n\n\nAuthors call the spacial concentrated, punitive, surveillance-oriented system found in some (minority) communities “carceral state”"
  },
  {
    "objectID": "slides/c3-crime.html#upside-down-world",
    "href": "slides/c3-crime.html#upside-down-world",
    "title": "Crime and Punishment I",
    "section": "Upside down world?",
    "text": "Upside down world?\n\nBoth crime and its punishment have important implications for politics\nCrime engenders participation and policing deters it (???)\nA cause and a consequence of development\nWhat is the appropriate response to crime? A very hard question to answer"
  },
  {
    "objectID": "slides/24-git2.html",
    "href": "slides/24-git2.html",
    "title": "Github Pages",
    "section": "",
    "text": "Today’s lecture shifted to Mon\nFeedback on Research Question and Data will come Thursday\nAny research proposals focused on environmental topics?\nMarch 5 Research Design shifted 24 hours\nSending .html files"
  }
]