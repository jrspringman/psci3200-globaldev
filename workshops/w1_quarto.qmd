---
title: "R, RStudio, and Quarto"
author: "Jeremy Springman"
institute: "University of Pennsylvania"

date: last-modified
toc: true

bibliography: references.bib

format: 
  html:
    self-contained: true

editor: source
---

# RStudio Layout

Before using R to illustrate basic programming concepts and data analysis tools, we will get familiar with the RStudio layout.

## Rstudio contains 4 panels

RStudio has four primary panels that will help you interact with your data. We will use the default layout of these panels.

-   Source panel: Top left
    -   Edit files to create 'scripts' of code
-   Console panel: Bottom left
    -   Accepts code as input
    -   Displays output when we run code
-   Environment panel: Top right
    -   Everything that R is holding in memory
    -   Objects that you create in the console or source panels will appear here
    -   You can clear the environment with the broom icon
-   Viewer panel: Bottom-right
    -   View graphics that you generate
    -   Navigate files

## Illustration

Let's use these panels to create and interact with data.

**Console:**

-   Perform a calculation: type `2 + 2` into the console panel and hit ENTER
-   Create and store an object: type `sum = 2 + 2` into the console panel and hit ENTER

**Source:**

-   Start an R script: Open new .R file (button in top-left below "File")
-   Create and store an object: type `sum = 2 + 3` into the source panel and hit cntrl+ENTER

**Environment:**

-   Confirm that the object `sum` is stored in our environment
-   Use `rm(sum)` to clear the object from the environment
-   Clear the environment with the broom icon

**Viewer:**

-   Navigate through your computer's files
-   Create a plot in the source panel

```{r }
#| echo: true
#| include: false
#| warning: false
#| message: false

## Copy this into an .R file
data = data.frame(
  x1 = rnorm(100, mean = 1, sd = 1),
  x2 = rnorm(100, mean = 1, sd = 1)
)

plot(data$x1, data$x2)

```


# Quarto

## Formatting

- Quarto is an extension of `markdown`
  + You can *italics*, **bold**, and [link](url)
  + Create sections and subsections with `#, ##, ###`
- Declare title, file type, references, in `yaml` (top section)
- Commenting-out text

### Write Formulas

-   Use math mode to render formulas

$$
\widehat{ATE} = \overline{Y}_{treatment\_group} - \overline{Y}_{control\_group}
$$

### Integrate External Image

You can also integrate external images that are stored in your project folder.

![Here is an image from my repo with a caption](tab_1.png)




### Automate References

Cite cool work by outstanding scholars [@springman2022political] and automatically generate a references list at the end of your document.

- Find paper on `scholar.google.com`
  + Select *"Cite* &rarr; *BibTex*
- Create `references.bib` file 
  + Store *BibTex* reference in file
  + Add `references.bib` to your `yaml`


### Add Flow Charts

There is also built-in functionality to do cool stuff like create flow charts using `mermaid`.

```{mermaid}
flowchart LR
  A[Hard edge] --> B(Round edge)
  B --> C{Decision}
  C --> D[Result one]
  C --> E[Result two]
```


### 

## Working with Code

To integrate code into our document, we can use code chunks. This allows us to create self-contained, reproducible documents. 

### Code Chunks Options

Code chunks come with specific options to control behavior. Let's break down each option:


```{}
#| echo: true          # The code will be shown in the rendered output.
#| warning: false       # Any warnings produced by the code will be suppressed.
#| label: fig-fake-data # This assigns an internal label to the chunk, useful for referencing.
#| fig-cap: "Fake data figure" # This sets the caption for any figures generated by the chunk.
#| fig-width: 5

```


### Figures

Check out @fig-fake-data

```{r}
#| echo: false
#| warning: false
#| label: fig-fake-data
#| fig-cap: "Fake data figure"
#| fig-width: 15

library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Generate random data
data <- data.frame(
  x = runif(50),
  y = runif(50),
  size = runif(50, min = 1, max = 10),
  color = runif(50)
)

# Create scatterplot
ggplot(data, aes(x = x, y = y, size = size, color = color)) +
  geom_point(alpha = 0.5) +
  scale_size_continuous(range = c(1, 10)) +
  ggtitle("Random Scatterplot") +
  xlab("X") +
  ylab("Y")


```


### Descriptive Tables

```{r}
#| echo: false
#| warning: false
#| label: tbl-fake-data
#| tbl-cap: "Fake data table"

library(gt)
library(dplyr)

# Calculate mean and standard deviation for each variable
means <- sapply(data, mean)
sds <- sapply(data, sd)

# Create a data frame for summary statistics
summary_data <- data.frame(
  Variable = names(data),
  Mean = means,
  SD = sds
)

# Create a gt table
summary_data %>%
  gt() %>%
  tab_header(
    title = "Descriptive Table of Random Data",
    subtitle = "Summary statistics: Mean and Standard Deviation"
  )

```

### Regression Results


```{r}
#| echo: false
#| warning: false

library(gt)
library(dplyr)
## Colliders
n = 1000 # sample size

# generating data according to a collider structure
x1 = runif(n) # random values for explenatory variable x1
y =  rnorm(n, sd = 0.1) # random values for target variable y
x2 = 2 * x1 + 2 * y + rnorm(n,sd = 0.1) # x2 is a collider, no influence on y, but influenced by y and x1

# including the colider in the regression prduces an estimate of -0.5 for x1
# so we see a collider can even flip the sign of another variable, as in Simpson's paradoxon
modelsummary::modelsummary(
  list(lm(y ~ x1), lm(y ~ x1 + x2)),
  estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
  )


```

### Inline Code

Use code within the text to describe your data (n = `r nrow(data)`) for a more reproducible workflow. If your data changes, the writing automatically updates.




# Review of Basic Programming Concepts


## Objects: where values are saved in R

"Object" is a generic term for anything that R stores in the environment. This can include anything from an individual number or word, to lists of values, to entire datasets.

Importantly, objects belong to different "classes" depending on the type of values that they store.

-   Numerics are numbers
-   Characters are text or strings like `"hello world"` and `"welcome to R"`.
-   Factors are a group of characters/strings with a fixed number of unique values
-   Logicals are either `TRUE` or `FALSE`

```{r }
#| echo: true
#| include: true
#| warning: true
#| message: true

# Create a numeric object
my_number = 5.6
# Check the class
class(my_number)

# Create a character object
my_character = "welcome to R"
# Check the class
class(my_character)

# Create a logical object
my_logical = TRUE
# Check the class
class(my_logical)

```

R can perform operations on objects.

```{r }
#| echo: true
#| include: true
#| warning: true
#| message: true

# Create a numeric object
my_number = 5.6
# Check the class
class(my_number)
# Perform a calculation
my_number = my_number + 5

```

The class of an object determines the type of operations you can perform on it. Some operations can only be run on numeric objects (numbers).

```{r , eval=FALSE}
#| echo: true
#| include: true
#| warning: true
#| message: true

# Create a character object
my_number = "5.6"
# Check the class
class(my_number)
# Perform a calculation
my_number + 5
round(my_number)

```

R contains functions that can convert some objects to different factors.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Convert character to numeric
my_number = as.numeric("5")
class(my_number)

my_number <- 5

# But R is only so smart
my_number = as.numeric("five")
print(my_number)
```

## Data Structures

The most simple objects are single values, but most data analysis involves more complicated data structures.

### Lists

Lists are a type of data structure that store multiple values together. Lists are created using `c()` and allow you to perform operations on a series of values.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Create a numeric list (also called a "vector")
numeric_vector = c(6, 11, 13, 31)
# Print the vector
print(numeric_vector)
# Check the class
class(numeric_vector)
# Calculate the mean
mean(numeric_vector)

```

An important part of working with more complex data structures is called "indexing." Indexing allows you to extract specific values from a data structure.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Extract the 2nd element from the list
numeric_vector[2]

# Extract elements 2-4
numeric_vector[2:4]

# Extract elements 1-2
numeric_vector[c(TRUE, TRUE, FALSE, FALSE)]


```

### Dataframes

Data frames are the most common type of data structure used in research. Data frames combine multiple lists of values into a single object.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Create a dataframe
my_data = data.frame(
  x1 = rnorm(100, mean = 1, sd = 1),
  x2 = rnorm(100, mean = 1, sd = 1)
)

class(my_data)

```

Anything that comes in a spreadsheet (for example, an excel file) can be loaded into an R environment as a dataframe. R works most easily when spreadsheets are saved as a `.csv` file.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Use `read.csv()` to load data from a website
dat = read.csv("https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv") 

# Use `read.csv()` to load data from your computer's Downloads folder
# dat = read.csv("/home/jeremy/Downloads/clean_endline_did.csv")

```

In most data frames, rows correspond to observations and the columns correspond to variables that describe the observations. Here, we are looking at survey data from an RCT involving university students in Addis Ababa. Each row correspondents to a different survey respondent, and each column represents their answers to a different question from the survey.

## Loading Packages

Packages are an extremely important part of data analysis with R.

-   R gives you access to thousands of "packages" that are created by users
-   Packages contain bundles of code called "functions" that can execute specific tasks
-   Use `install.packages()` to install a package and `library()` to load a package

In the next section, we'll use the package `dplyr` to perform some data cleaning. `dplyr` is part of a universe of packages called `tidyverse`. Since this is one of the most important packages in the R ecosystem, let's install and load it.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# install.packages("tidyverse")
library(tidyverse)

```

When you are searching online or asking ChatGPT how to perform a specific task in R, it often helps to specify that you are looking for a solution in `dplyr`.

# Cleaning Data

In the real-world, data never comes ready to be analyzed. Data cleaning is the process of manipulating data so that it can be analyzed. This is usually the most difficult and time-consuming part of any data analysis project. Let's walk through some examples.

**Question: In your work, what data cleaning have you had to do? How have you cleaned your data?**

## Creating Variables

Imagine we want to analyze the relationship between whether a respondent moved to to Addis Ababa to attend university and their level of political participation. However, there are two problems:

-   We don't have a specific variable that measures whether or not respondents moved
-   We have many measures of participation

How can we create a variable measuring whether the respondent moved to Addis Ababa? We have a multiple-choice question asking students about what region they come from.

Let's start by investigating this variable.

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# The name of the variable in our dataframe is `q8_baseline`
table(dat$q8_baseline)

dat = dat %>% # this is called a "pipe"
  # give our variable a better name
  rename(home_region = q8_baseline) 

dat = dat %>%
  # drop respondents who report "Prefer not to say"
  filter(!home_region == "Prefer not to say") 

dat = dat %>%
  # clean home region variable using `mutate()`
  mutate(
    # Shorten a long name to an abbreviation
    home_region = ifelse(home_region == "Southern Nations, Nationalities, and Peoples Region", "SNNPR", home_region),
    # remove the word "Region" from every observation of this column
    home_region = str_remove(home_region, " Region"),
    home_region = str_remove(home_region, " \\(city\\)")
    )

# Check if it worked
table(dat$home_region)

```

```{r }
#| echo: true
#| include: true
#| warning: false
#| message: false

# Chain these all together for more concise code
dat = read_csv("https://raw.githubusercontent.com/jrspringman/psci3200-globaldev/main/workshops/aau_survey/clean_endline_did.csv" ) %>%
  # give our variable a better name
  rename(home_region = q8_baseline) %>%
  # drop respondents who report "Prefer not to say"
  filter(!home_region == "Prefer not to say") %>%
  # clean home region variable
  mutate(
    # Shorten a long name to an abbreviation
    home_region = case_when(
      home_region == "Southern Nations, Nationalities, and Peoples Region" ~ "SNNPR",
      home_region == "South West Ethiopia Peoples Region" ~ "SWEPR",
      TRUE ~ home_region
    ),
    # remove the word "Region" from every observation of this column
    home_region = str_remove(home_region, " Region| \\(city\\)")
    )

# Check if it worked
table(dat$home_region)

```

Now that we've cleaned-up the names, we want to create a variable that tells us whether or not each respondent is originally from Addis Ababa. This will let us measure whether or not they moved to Addis in order to attend college.

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

# Creating a measure of whether a respondent moved to Addis Ababa in order to at independent variable
dat = dat %>% 
  mutate(
    moved = case_when(
      home_region == "Addis Ababa" ~ 0,
      TRUE ~ 1
    ) 
  )

table(dat$moved, dat$home_region)

```

Now, we need to create our second variable measuring levels of political participation. Remember, the challenge is that we have multiple measures of participation. Let's start with two measures:

-   Number of times you've contacted gov't official `q13_4_1`
-   Number of times you've signed a petition `q13_5_1`

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

# Check out the distribution of our variables
dat %>% select(q13_4_1, q13_5_1) %>% head(5)

mean(dat$q13_4_1)
hist(dat$q13_5_1)

# Create a single measuree
dat = dat %>%
  mutate(
      add_participation = q13_4_1 + q13_5_1

  )

hist(dat$add_participation)
# how do investigate NA values?

```

One thing we need to be careful with are NA values. We need to think carefully about why NA values are in our data and how to handle them appropriately.

**Question: Thinking about the data that you have worked with, what are the most common sources of `NA` values?**

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

## Find our two participation measures
add_ecols = grep("q13_4_1$|q13_5_1$", names(dat), value = T)

dat = dat %>%
  mutate(add_participation =  rowSums(across(add_ecols) ) )
  #mutate(add_participation_end =  rowSums(across(add_ecols), na.rm = T) )

# dat = dat %>%
#   mutate(home_region = na_if(home_region, "Prefer not to say")) %>%

```

## Merging Datasets

Often, data analysis projects will require you to use variables from more than one dataset. This will require you to combine separate datasets into a single dataset in R, which is called *merging*. A *merge* uses an identifier (administrative unit names, respondent IDs, etc.) that is present in both datasets to combine them into one.

**Question: Thinking about the data that you have worked with, have you ever had to merge multiple datasets? What was the data? How did you do it?**

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

## Here we are selecting the unit-identifier (response_id) and the two variables we have created and saving them to a separate dataframe
new_vars = dat %>%
  select(response_id, moved, add_participation)

## After you do some cleaning or create new variables, you may want to save that dataframe for future use
# write.csv(new_dat, here::here("workshops/aau_survey/new_vars.csv"))

## Drop the new variables from our dataframe so that we can merge them back in
dat = dat %>%
  select( -moved, -add_participation)

## Check to make sure the merge will work as expected
# nrow(dat)
# nrow(new_vars)
# dat$response_id[! dat$response_id %in% new_vars$response_id]

## Use dplyr's `full_join` function  to merge datasets
dat = dat %>%
  full_join(., new_vars)

  
```

# Data Visualization

One of the major benefits of using R is the ability to create beautiful tables and figures that can be incorporated into deliverables and other professional documents. Importantly, you can create a script that allows you to automatically recreate the same visualizations when you want to make slight changes or incorporate new data.

**Question: What methods do you currently use to create data visualizations for deliverables and other research products?**

## Tables

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false


#install.packages("gt")
library(gt)

desciptives = dat %>%
  group_by(moved) %>%
  summarise(observations = n(), 
            share = observations / nrow(dat),
            mean = mean(add_participation, na.rm = T)) %>%
  mutate(moved = case_when(moved == 0 ~ "No",
                           moved == 1 ~ "Yes"),
         share = share *100)

gt(desciptives) %>%
  tab_header(
    title = "Moving to University and Political Participation",
  ) %>%
  fmt_number(
    columns = 3:4,
    decimals = 2,
    use_seps = FALSE
  ) %>%
  cols_label(
    moved = md("Moved to<br>Addis"),
    observations = md("Respondents<br>(count)"),
    share = md("Respondents<br>(%)"),
    mean = md("Participation<br>(mean score)")
  ) %>%
  gtsave(filename = "tab_1.png")


```

## Figures

There are many ways to create figures in R, but `ggplot` is the most popular.

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false


ggplot(dat, aes(add_participation)) +
  geom_histogram(binwidth = 1)

dat %>%
  filter(add_participation < 15) %>%
  ggplot(aes(add_participation)) +
  geom_histogram(binwidth = 1)

dat %>%
  filter(add_participation < 15) %>%
  ggplot(aes(x = add_participation, fill = q3_baseline)) +
  geom_histogram(binwidth = 1, alpha = 0.5, position = "identity")

dat %>%
  filter(add_participation < 15) %>%
  mutate(moved = fct_rev(as.factor(moved))) %>%
  ggplot(aes(x = add_participation, fill = moved)) +
  geom_histogram(aes(y = (..count..) / sum(..count..) * 100),
                 binwidth = 1, alpha = 0.5, position = "identity") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  labs(y = "Percentage") +
  theme_bw() +
  theme(legend.title = element_blank(), legend.position = c(.8, .8))




```

You can do tremendous amounts of customization with `ggplot` to create extremely informative and professional plots.

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false


## demographics
demos = dat %>%
  drop_na(q3_baseline) %>% 
  select(`Respondent Gender` = q3_baseline, `Work as a student?` = q4_baseline, 
         `Rural or urban?` = q5_baseline, `Financial support from family?` = q6_baseline, 
         `Home region` = home_region,
         `Student Year` = class_year) %>% 
  pivot_longer(everything()) %>% 
  group_by(name, value) %>% 
  tally() %>% 
  mutate(pct = n/sum(n)) %>% 
  top_n(n = 5, wt = pct)

demos$name = factor(demos$name, levels = c('Home region', 'Rural or urban?', 'Student Year', 'Respondent Gender', 'Work as a student?', 'Financial support from family?'))

demos <- demos %>%
  group_by(name) %>%
  mutate(total_n = sum(n)) %>%
  ungroup()

ggplot(demos , aes(y = value, x = pct)) + 
  geom_col(fill = "grey") + 
  facet_wrap( ~name, scales = "free") + 
  scale_y_discrete(labels = scales::label_wrap(30)) + 
  hrbrthemes::scale_x_percent(limits = c(0, 1)) + 
  labs(x = "Percent of respondents to the survey", y = NULL, 
       title = "Demographic characteristics of baseline respondents", 
       subtitle = glue::glue("Number of respondents = {scales::comma(nrow(df))}"), 
       caption = "Note: home region only displays top five categories by size.") +
  geom_text(
    aes(
      label = glue::glue("n = {total_n}")
    ), 
    x = 0.92, y = -Inf, 
    vjust = -1, hjust = 1, 
    inherit.aes = FALSE
  )

#ggsave("/home/jeremy/Downloads/demographics.png")



```

# Regression

Now let's use simple linear regression to estimate the relationship between these two variables that we created.

**Question: Have you used regression in your research? If so, what software did you use to**

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

est = lm(add_participation ~ moved, dat)

summary(est)

```

What if we want to control for the influence of the number of years someone has been at university? Here, we can create a new measure of year and use multiple regression.

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

#install.packages("modelsummary")
library(modelsummary)

table(dat$class_year)

dat = dat %>% mutate(
  year_bin = as.numeric(case_when(class_year == "Year I" ~ 0,
                                  class_year == "Year II" ~ NA,
                                  class_year == "Year III" ~ 1 
                                  )
                        ) 
  )
table(dat$year_bin)

models <- list()
models[['Bivariate']] = lm(add_participation ~ moved, dat)
models[['Multivariate']] = lm(add_participation ~ moved + year_bin, dat)

modelsummary(
  models,
  estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')

```

Let's compare two different ways of defining the year measure.

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

dat = dat %>% mutate(year_cont = as.numeric(case_when(class_year == "Year I" ~ 0,
                                                 class_year == "Year II" ~ 1,
                                                 class_year == "Year III" ~ 2 )) )
table(dat$year_cont)

models <- list()
models[['Bivariate']] = lm(add_participation ~ moved + year_bin, dat)
models[['Multivariate']] = lm(add_participation ~ moved + year_cont, dat)

modelsummary(
  models,
  estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')

```



# Appendix

## Averaged Z-Scores

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false


## Find participation measures that are based on likert
# baseline
bcols = grep("^q13_.*_baseline$", names(dat), value = T)
dat[, paste0(bcols, "_st")] = dat[, bcols]
bcols = paste0(bcols,"_st")

# endline
ecols = grep("^q13_[1-7]_\\d$", names(dat), value = T)
dat[, paste0(ecols, "_st")] = dat[, ecols]
ecols = paste0(ecols,"_st")


# Create treatment variable
dat = dat %>% mutate(moved = case_when(home_region == "Addis Ababa" ~ 0, TRUE ~ 1) )

# clean q13_
levels = c("Never", "Once or Twice", "More than twice", "More than 5 times", 
           "More than 10 times")
dat = dat %>% 
  mutate(across(c(bcols), 
                .fns = ~ factor(.x, levels = levels)))

# Create z-score function from Kling, Liberman, and Katz (2007)
z_score = function(x, y){
  # calculate mean and sd of control group
  c_mean = mean( as.numeric( unlist(x[, y])) , na.rm = T)
  c_sd = sd( as.numeric( unlist(x[, y])) , na.rm = T)
  # subtract control group mean; divide by control group SD
  ( as.numeric(x[, y, drop = TRUE]) - c_mean) / c_sd
}

# calculate z-scores
for (i in c(bcols, ecols)) {
  dat[,i] = z_score(dat, i)
}

dat = dat %>% 
  rowwise() %>% 
  mutate( z_participation_end = mean(c_across(all_of(bcols)), na.rm = TRUE)) %>% 
  mutate( z_participation_base = mean(c_across(all_of(ecols)), na.rm = TRUE)) %>%
  ungroup()

```

```{r}
#| echo: true
#| include: true
#| warning: false
#| message: false

regd = dat %>% select(z_participation_end, z_participation_base, moved, response_id ) %>%
  pivot_longer(cols = c(z_participation_end, z_participation_base),
               names_to = "time",
               values_to = "z_participation") %>%
  mutate(time = case_when(time == "z_participation_end" ~ 1,
                          TRUE ~ 0))

models <- list()
models[['Bivariate']] = lm(z_participation ~ moved, regd)
models[['Multivariate']] = lm(z_participation ~ moved + time, regd)
models[['Interaction']] = lm(z_participation ~ moved + time + moved*time, regd)

modelsummary(
  models,
  estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
  gof_omit = 'IC|RMSE|Log|F|R2$|Std.')


```
